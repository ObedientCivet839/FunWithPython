-------------G------------AudioLM
https://google-research.github.io/seanet/audiolm/examples/
A Language Modeling Approach to Audio Generation ... Abstract. We introduce AudioLM, a framework for high-quality audio generation with long-term consistency.
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour
Google Research
Abstract. We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.
Continuations using 3 second prompts from LibriSpeech test-{clean, other}, for speakers and content not seen during training. AudioLM excels at generating continuations that:
For acoustic generation, we sample the acoustic tokens given the semantic tokens extracted from the original samples from LibriSpeech test-clean. The model generates samples with different speakers and recording conditions, while the semantic content is identical.
The unconditional generation performs sampling without using prompts. In that case, every sequence varies in speaker identity, linguistic content, and recording conditions.
To illustrate that the semantic tokens are crucial for generating coherent linguistic content, we train the language model on the acoustic tokens only. While the generated continuations of the 4-second prompts maintain speaker identity, the linguistic content is inconsistent, and often akin to babbling.
We compare SoundStream reconstructions of two models, one using 3-layer residual vector quantization (3-RVQ) and another with 12 layers (12-RVQ), the latter being the default. The equivalent bitrates are 1.5 kbps and 6 kbps.
AudioLM is not limited to modeling speech. It can also learn to generate coherent piano music continuations, despite being trained on piano music without any symbolic representation. We also show the continuations produced by a version of the model trained exclusively on the acoustic tokens. These continuations are much less coherent, stressing the importance of the semantic tokens in our framework. The 4-second prompts come from the test split of MAESTRO dataset.



AudioLM: a Language Modeling Approach to Audio Generation
https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html
Oct 6, 2022 · AudioLM is a pure audio model that is trained without any text or symbolic representation of music. AudioLM models an audio sequence ...
Generating realistic audio requires modeling information represented at different scales. For example, just as music builds complex musical phrases from individual notes, speech combines temporally local structures, such as phonemes or syllables, into words and sentences. Creating well-structured and coherent audio sequences at all these scales is a challenge that has been addressed by coupling audio with transcriptions that can guide the generative process, be it text transcripts for speech synthesis or MIDI representations for piano. However, this approach breaks when trying to model untranscribed aspects of audio, such as speaker characteristics necessary to help people with speech impairments recover their voice, or stylistic components of a piano performance.
In “AudioLM: a Language Modeling Approach to Audio Generation”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming previous systems and pushing the frontiers of audio generation with applications in speech synthesis or computer-assisted music. Following our AI Principles, we've also developed a model to identify synthetic audio generated by AudioLM.
From Text to Audio Language Models
In recent years, language models trained on very large text corpora have demonstrated their exceptional generative abilities, from open-ended dialogue to machine translation or even common-sense reasoning. They have further shown their capacity to model other signals than texts, such as natural images. The key intuition behind AudioLM is to leverage such advances in language modeling to generate audio without being trained on annotated data.
However, some challenges need to be addressed when moving from text language models to audio language models. First, one must cope with the fact that the data rate for audio is significantly higher, thus leading to much longer sequences — while a written sentence can be represented by a few dozen characters, its audio waveform typically contains hundreds of thousands of values. Second, there is a one-to-many relationship between text and audio. This means that the same sentence can be rendered by different speakers with different speaking styles, emotional content and recording conditions.
To overcome both challenges, AudioLM leverages two kinds of audio tokens. First, semantic tokens are extracted from w2v-BERT, a self-supervised audio model. These tokens capture both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure (e.g., language syntax and semantic content in speech, harmony and rhythm in piano music), while heavily downsampling the audio signal to allow for modeling long sequences.
However, audio reconstructed from these tokens demonstrates poor fidelity. To overcome this limitation, in addition to semantic tokens, we rely on acoustic tokens produced by a SoundStream neural codec, which capture the details of the audio waveform (such as speaker characteristics or recording conditions) and allow for high-quality synthesis. Training a system to generate both semantic and acoustic tokens leads simultaneously to high audio quality and long-term consistency.
Training an Audio-Only Language Model
AudioLM is a pure audio model that is trained without any text or symbolic representation of music. AudioLM models an audio sequence hierarchically, from semantic tokens up to fine acoustic tokens, by chaining several Transformer models, one for each stage. Each stage is trained for the next token prediction based on past tokens, as one would train a text language model. The first stage performs this task on semantic tokens to model the high-level structure of the audio sequence.
In the second stage, we concatenate the entire semantic token sequence, along with the past coarse acoustic tokens, and feed both as conditioning to the coarse acoustic model, which then predicts the future tokens. This step models acoustic properties such as speaker characteristics in speech or timbre in music.
In the third stage, we process the coarse acoustic tokens with the fine acoustic model, which adds even more detail to the final audio. Finally, we feed acoustic tokens to the SoundStream decoder to reconstruct a waveform.
After training, one can condition AudioLM on a few seconds of audio, which enables it to generate consistent continuation. In order to showcase the general applicability of the AudioLM framework, we consider two tasks from different audio domains:
- Speech continuation, where the model is expected to retain the speaker characteristics, prosody and recording conditions of the prompt while producing new content that is syntactically correct and semantically consistent.
- Piano continuation, where the model is expected to generate piano music that is coherent with the prompt in terms of melody, harmony and rhythm.
In the video below, you can listen to examples where the model is asked to continue either speech or music and generate new content that was not seen during training. As you listen, note that everything you hear after the gray vertical line was generated by AudioLM and that the model has never seen any text or musical transcription, but rather just learned from raw audio. We release more samples on this webpage.
To validate our results, we asked human raters to listen to short audio clips and decide whether it is an original recording of human speech or a synthetic continuation generated by AudioLM. Based on the ratings collected, we observed a 51.2% success rate, which is not statistically significantly different from the 50% success rate achieved when assigning labels at random. This means that speech generated by AudioLM is hard to distinguish from real speech for the average listener.
Our work on AudioLM is for research purposes and we have no plans to release it more broadly at this time. In alignment with our AI Principles, we sought to understand and mitigate the possibility that people could misinterpret the short speech samples synthesized by AudioLM as real speech. For this purpose, we trained a classifier that can detect synthetic speech generated by AudioLM with very high accuracy (98.6%). This shows that despite being (almost) indistinguishable to some listeners, continuations generated by AudioLM are very easy to detect with a simple audio classifier. This is a crucial first step to help protect against the potential misuse of AudioLM, with future efforts potentially exploring technologies such as audio “watermarking”.
Conclusion
We introduce AudioLM, a language modeling approach to audio generation that provides both long-term coherence and high audio quality. Experiments on speech generation show not only that AudioLM can generate syntactically and semantically coherent speech without any text, but also that continuations produced by the model are almost indistinguishable from real speech by humans. Moreover, AudioLM goes well beyond speech and can model arbitrary audio signals such as piano music. This encourages the future extensions to other types of audio (e.g., multilingual speech, polyphonic music, and audio events) as well as integrating AudioLM into an encoder-decoder framework for conditioned tasks such as text-to-speech or speech-to-speech translation.
Acknowledgments
The work described here was authored by Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi and Neil Zeghidour. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google.



AudioLM: a Language Modeling Approach to Audio Generation - arXiv
https://arxiv.org/abs/2209.03143
Sep 7, 2022 · Abstract: We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a ...
Computer Science > Sound
[Submitted on 7 Sep 2022]
Title:AudioLM: a Language Modeling Approach to Audio GenerationDownload PDF
Abstract: We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.
Submission historyFrom: Neil Zeghidour [view email]
[v1] Wed, 7 Sep 2022 13:40:08 UTC (1,183 KB)
Full-text links:
Download:
(license)
Current browse context:
cs.SD
References & Citations
a Loading...
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Replicate (What is Replicate?)
Hugging Face Spaces (What is Spaces?)
Recommenders and Search Tools
Connected Papers (What is Connected Papers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.



GitHub - lucidrains/audiolm-pytorch
https://github.com/lucidrains/audiolm-pytorch
Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch - GitHub - lucidrains/audiolm-pytorch: ...
AudioLM - Pytorch
Implementation of AudioLM, a Language Modeling Approach to Audio Generation out of Google Research, in Pytorch
It also extends the work for conditioning with classifier free guidance with T5. This allows for one to do text-to-audio or TTS, not offered in the paper. Yes, this means VALL-E can be trained from this repository. It is essentially the same.
Please join if you are interested in replicating this work in the open
This repository now also contains a MIT licensed version of SoundStream. Once EnCodec becomes MIT licensed, then I will consider adding a wrapper for that as well for use here.
Update: AudioLM was essentially used to 'solve' music generation in the new MusicLM
In the future, this movie clip would no longer make any sense. You would just prompt an AI instead.
Appreciation
-
Stability.ai for the generous sponsorship to work and open source cutting edge artificial intelligence research
-
🤗Huggingface for their amazing accelerate and transformers libraries
-
-
@eonglints and Joseph for offering their professional advice and expertise as well as pull requests!
-
@djqualia, @yigityu, @inspirit, and @BlackFox1197 for helping with the debugging of soundstream
-
Allen for catching and fixing some bugs!
-
Andrey for identifying a missing loss in soundstream and guiding me through the proper mel spectrogram hyperparameters
Install
$ pip install audiolm-pytorch
Usage
First,
SoundStream needs to be trained on a large corpus of audio data
from audiolm_pytorch import SoundStream, SoundStreamTrainer soundstream = SoundStream( codebook_size = 1024, rq_num_quantizers = 8, attn_window_size = 128, # local attention receptive field at bottleneck attn_depth = 2 # 2 local attention transformer blocks - the soundstream folks were not experts with attention, so i took the liberty to add some. encodec went with lstms, but attention should be better ) trainer = SoundStreamTrainer( soundstream, folder = '/path/to/audio/files', batch_size = 4, grad_accum_every = 8, # effective batch size of 32 data_max_length = 320 * 32, num_train_steps = 10000 ).cuda() trainer.train()
Then three separate transformers (
SemanticTransformer,
CoarseTransformer,
FineTransformer) need to be trained
ex.
SemanticTransformer
import torch from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer # hubert checkpoints can be downloaded at # https://github.com/facebookresearch/fairseq/tree/main/examples/hubert wav2vec = HubertWithKmeans( checkpoint_path = './hubert/hubert_base_ls960.pt', kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin' ) semantic_transformer = SemanticTransformer( num_semantic_tokens = wav2vec.codebook_size, dim = 1024, depth = 6 ).cuda() trainer = SemanticTransformerTrainer( transformer = semantic_transformer, wav2vec = wav2vec, folder ='/path/to/audio/files', batch_size = 1, data_max_length = 320 * 32, num_train_steps = 1 ) trainer.train()
ex.
CoarseTransformer
import torch from audiolm_pytorch import HubertWithKmeans, SoundStream, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer wav2vec = HubertWithKmeans( checkpoint_path = './hubert/hubert_base_ls960.pt', kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin' ) soundstream = SoundStream( codebook_size = 1024, rq_num_quantizers = 8, ) soundstream.load('/path/to/trained/soundstream.pt') coarse_transformer = CoarseTransformer( num_semantic_tokens = wav2vec.codebook_size, codebook_size = 1024, num_coarse_quantizers = 3, dim = 512, depth = 6 ) trainer = CoarseTransformerTrainer( transformer = coarse_transformer, soundstream = soundstream, wav2vec = wav2vec, folder = '/path/to/audio/files', batch_size = 1, data_max_length = 320 * 32, num_train_steps = 10000 ) trainer.train()
ex.
FineTransformer
import torch from audiolm_pytorch import SoundStream, FineTransformer, FineTransformerWrapper, FineTransformerTrainer soundstream = SoundStream( codebook_size = 1024, rq_num_quantizers = 8, ) soundstream.load('/path/to/trained/soundstream.pt') fine_transformer = FineTransformer( num_coarse_quantizers = 3, num_fine_quantizers = 5, codebook_size = 1024, dim = 512, depth = 6 ) trainer = FineTransformerTrainer( transformer = fine_transformer, soundstream = soundstream, folder = '/path/to/audio/files', batch_size = 1, data_max_length = 320 * 32, num_train_steps = 10000 ) trainer.train()
All together now
from audiolm_pytorch import AudioLM audiolm = AudioLM( wav2vec = wav2vec, soundstream = soundstream, semantic_transformer = semantic_transformer, coarse_transformer = coarse_transformer, fine_transformer = fine_transformer ) generated_wav = audiolm(batch_size = 1) # or with priming generated_wav_with_prime = audiolm(prime_wave = torch.randn(1, 320 * 8)) # or with text condition, if given generated_wav_with_text_condition = audiolm(text = ['chirping of birds and the distant echos of bells'])
Text Conditioned Audio Synthesis
Update: Looks like this will work, given 'VALL-E'
ex. Semantic Transformer
import torch from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer wav2vec = HubertWithKmeans( checkpoint_path = './hubert/hubert_base_ls960.pt', kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin' ) semantic_transformer = SemanticTransformer( num_semantic_tokens = 500, dim = 1024, depth = 6, has_condition = True, # this will have to be set to True cond_as_self_attn_prefix = True # whether to condition as prefix to self attention, instead of cross attention, as was done in 'VALL-E' paper ).cuda() # mock text video dataset (as an example) # you will have to extend your own from `Dataset`, and return an audio tensor as well as a string (the audio description) in any order (the framework will autodetect and route it into the transformer) from torch.utils.data import Dataset class MockTextAudioDataset(Dataset): def __init__(self, length = 100, audio_length = 320 * 32): super().__init__() self.audio_length = audio_length self.len = length def __len__(self): return self.len def __getitem__(self, idx): mock_audio = torch.randn(self.audio_length) mock_caption = 'audio caption' return mock_caption, mock_audio dataset = MockTextAudioDataset() # instantiate semantic transformer trainer and train trainer = SemanticTransformerTrainer( transformer = semantic_transformer, wav2vec = wav2vec, dataset = dataset, batch_size = 4, grad_accum_every = 8, data_max_length = 320 * 32, num_train_steps = 100000 ) trainer.train() # after much training above sample = trainer.generate(text = ['sound of rain drops on the rooftops'], batch_size = 1, max_length = 2) # (1, < 128) - may terminate early if it detects [eos]
Todo
-
complete CoarseTransformer
-
use fairseq vq-wav2vec for embeddings
-
add conditioning
-
add classifier free guidance
-
add unique consecutive for
-
incorporate ability to use hubert intermediate features as semantic tokens, recommended by eonglints
-
accommodate variable lengthed audio, bring in eos token
-
make sure unique consecutive works with coarse transformer
-
pretty printing all discriminator losses to log
-
handle when generating semantic tokens, that last logits may not be necessarily the last in the sequence given unique consecutive processing
-
complete sampling code for both Coarse and Fine Transformers, which will be tricky
-
make sure full inference with or without prompting works on the
AudioLMclass
-
complete full training code for soundstream, taking care of discriminator training
-
add efficient gradient penalty for discriminators for soundstream
-
wire up sample hz from sound dataset -> transformers, and have proper resampling within during training - think about whether to allow for dataset to have sound files of varying or enforce same sample hz
-
full transformer training code for all three transformers
-
refactor so semantic transformer has a wrapper to that handles unique consecutives as well as wav to hubert or vq-wav2vec
-
simply not self attend to eos token on the prompting side (semantic for coarse transformer, coarse for fine transformer)
-
add structured dropout from forgetful causal masking, far better than traditional dropouts
-
figure out how to suppress logging in fairseq
-
assert that all three transformers passed into audiolm is compatible
-
figure out how to do the normalization across each dimension mentioned in the paper, but ignore it for v1 of the framework
-
DRY a little at the end
-
test with speech synthesis for starters
-
add option to use flash attention
-
simplify training even more within AudioLM class
-
cli tool, something like
audiolm generate <wav.file | text>and save generated wav file to local directory
-
return a list of waves in the case of variable lengthed audio
-
just take care of the edge case in coarse transformer text conditioned training, where the raw wave is resampled at different frequencies. autodetermine how to route based on length
Citations
@inproceedings{Borsos2022AudioLMAL, title = {AudioLM: a Language Modeling Approach to Audio Generation}, author = {Zal{\'a}n Borsos and Rapha{\"e}l Marinier and Damien Vincent and Eugene Kharitonov and Olivier Pietquin and Matthew Sharifi and Olivier Teboul and David Grangier and Marco Tagliasacchi and Neil Zeghidour}, year = {2022} }
@misc{https://doi.org/10.48550/arxiv.2107.03312, title = {SoundStream: An End-to-End Neural Audio Codec}, author = {Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco}, publisher = {arXiv}, url = {https://arxiv.org/abs/2107.03312}, year = {2021} }
@misc{shazeer2020glu, title = {GLU Variants Improve Transformer}, author = {Noam Shazeer}, year = {2020}, url = {https://arxiv.org/abs/2002.05202} }
@article{Shazeer2019FastTD, title = {Fast Transformer Decoding: One Write-Head is All You Need}, author = {Noam M. Shazeer}, journal = {ArXiv}, year = {2019}, volume = {abs/1911.02150} }
@article{Ho2022ClassifierFreeDG, title = {Classifier-Free Diffusion Guidance}, author = {Jonathan Ho}, journal = {ArXiv}, year = {2022}, volume = {abs/2207.12598} }
@misc{crowson2022, author = {Katherine Crowson}, url = {https://twitter.com/rivershavewings} }
@misc{ding2021cogview, title = {CogView: Mastering Text-to-Image Generation via Transformers}, author = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang}, year = {2021}, eprint = {2105.13290}, archivePrefix = {arXiv}, primaryClass = {cs.CV} }
@article{Liu2022FCMFC, title = {FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners}, author = {Hao Liu and Xinyang Geng and Lisa Lee and Igor Mordatch and Sergey Levine and Sharan Narang and P. Abbeel}, journal = {ArXiv}, year = {2022}, volume = {abs/2210.13432} }
@inproceedings{anonymous2022normformer, title = {NormFormer: Improved Transformer Pretraining with Extra Normalization}, author = {Anonymous}, booktitle = {Submitted to The Tenth International Conference on Learning Representations }, year = {2022}, url = {https://openreview.net/forum?id=GMYWzWztDx5}, note = {under review} }
@article{Li2021LocalViTBL, title = {LocalViT: Bringing Locality to Vision Transformers}, author = {Yawei Li and K. Zhang and Jie Cao and Radu Timofte and Luc Van Gool}, journal = {ArXiv}, year = {2021}, volume = {abs/2104.05707} }
@misc{liu2021swin, title = {Swin Transformer V2: Scaling Up Capacity and Resolution}, author = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo}, year = {2021}, eprint = {2111.09883}, archivePrefix = {arXiv}, primaryClass = {cs.CV} }
@inproceedings{Ma2022MegaMA, title = {Mega: Moving Average Equipped Gated Attention}, author = {Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer}, year = {2022} }
@misc{gilmer2023intriguing title = {Intriguing Properties of Transformer Training Instabilities}, author = {Justin Gilmer, Andrea Schioppa, and Jeremy Cohen}, year = {2023}, status = {to be published - one attention stabilization technique is circulating within Google Brain, being used by multiple teams} }



Google's Audiolm: Generating Music by Hearing a Song's Snippet
https://towardsai.net/p/l/googles-audiolm-generating-music-by-hearing-a-songs-snippet
Oct 14, 2022 · AudioLM is Google's new model, capable of generating music in the same style as the prompt. The model is also capable of generating complex ...
Google’s Audiolm: Generating Music by Hearing a Song’s Snippet
Last Updated on January 6, 2023 by Editorial Team
Last Updated on October 14, 2022 by Editorial Team
Author(s): Salvatore Raieli
Originally published on Towards AI the World’s Leading AI and Technology News and Media Company. If you are building an AI-related product or service, we invite you to consider becoming an AI sponsor. At Towards AI, we help scale AI and technology startups. Let us help you unleash your technology to the masses.
Whether music or speech, Google's new model can continue playing what is hearing.
AudioLM is Google’s new model, capable of generating music in the same style as the prompt. The model is also capable of generating complex sounds such as piano music or people talking. the result is stunning. In fact, it seems to be indistinguishable from the original.
Why is generating music difficult?
Generating music is not an easy task. In fact, generating audio signals (music, ambient sounds, people's speech) requires multiple scales of abstraction. For example, music has a structure that has to be analyzed over a long period of time and is also composed of numerous interacting signals. Even personal speech itself can be analyzed at different levels, be it the simple acoustic signal or phonetics, but also in terms of prosody, syntax, grammar, or semantics.
Several attempts have been made previously. The first attempts to generate music focused on generating MIDI files (an interesting project where they generated MIDI music for piano was created in 2018 using a transformer). In addition, some studies focused on tasks such as text-to-speech, where speech is generated from a transcript. The problem is that everything that is not in the transcript is not translated into the audio file. Several studies explain how in human communication, pauses and inflections, and other signals are extremely important.
For example, those using Alexa or other speakers have noticed that the voice does not sound natural. Especially in the early days, no matter how correct the pronunciation was, it sounded unnatural and gave an uncanny effect.
AudioLM, the new Google model
A few days ago, Google announced the release of a new model: “AudioLM: a Language Modeling Approach to Audio Generation”. The new model is capable of generating audio (such as realistic music and speech) just by listening to audio.
Google AI on Twitter: "Learn about AudioLM, an audio generation framework that demonstrates long-term consistency (e.g., syntax in speech & melody in music) and high fidelity, with applications for speech synthesis and computer-assisted music. ↓ https://t.co/onTH6HdCcX / Twitter"
Learn about AudioLM, an audio generation framework that demonstrates long-term consistency (e.g., syntax in speech & melody in music) and high fidelity, with applications for speech synthesis and computer-assisted music. ↓ https://t.co/onTH6HdCcX
As they blogged, there has been a great improvement in the field of Natural Language Processing (NLP) in recent years. In fact, language models have proven to be extremely effective in a number of tasks. Many of these systems are based on the use of transformers, and those who have used them know that one of the initial pre-processing steps is to tokenize (break up the text into smaller units that are assigned a numerical value).
The key intuition behind AudioLM is to leverage such advances in language modeling to generate audio without being trained on annotated data. — Google AI blogpost
AudioLM does not need transcription or labeling. The authors collected a database of sounds and fed it directly to the model. The model compresses the sound files into a series of snippets (sort of tokens). These tokens are then used as if they were an NLP model (the model, in this way, uses the same approach to learn patterns and relationships between the various audio snippets). In the same way as a text-generating model, AudioLM generates sounds from a prompt.
The result is very interesting, the sound is much more natural. AudioLM seems to be able to find and recreate certain patterns present in human music (like subtle vibrations contained in each note when piano keys are struck). In the link below, Google has provided a number of examples if you are curious to listen:
AudioLM has been trained on a vast library of sounds that include not only music but also human voices. For this reason, the model can generate sentences produced by a human being. The model is able to pick up the accent of the speaker and add pauses and exclamations. Although many of the sentences generated by the model do not make sense, the result is impressive.
Indeed, treating sequences of sounds as if they were sequences of words may seem like a clever approach, nonetheless, some difficulties remain:
First, one must cope with the fact that the data rate for audio is significantly higher, thus leading to much longer sequences — while a written sentence can be represented by a few dozen characters, its audio waveform typically contains hundreds of thousands of values. Second, there is a one-to-many relationship between text and audio. This means that the same sentence can be rendered by different speakers with different speaking styles, emotional content and recording conditions. — Google AI blogpost
In more detail, the audio tokenization approach was already tried by OpenAI Jukebox, only that the model generated many more artifacts, and the sound did not sound as natural.
As described by the authors, the model consists of three parts:
- a tokenizer model, which maps a sequence of sounds into a discrete sequence of tokens. This step also reduces the size of the sequence (the sampling rate is reduced by about 300 times).
- a decoder-only transformer (a classical language model) that maximizes the likelihood of predicting the next tokens in the sequence. The model contains 12 layers with 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of 4096
- a detokenizer model that transforms predicted tokens into audio tokens.
The model was trained on 60,000 hours of English speech and 40,000 hours of music for the piano experiments.
For this, we retrain all components of AudioLM on an internal dataset of 40k hours of piano music that includes players from beginner to expert level, and exhibits a wide range of different acoustic conditions, with content ranging from piano scale exercises to famous pieces. — source the original article
You can also watch the results in this short video:
The authors report that people who listened to AudioLM results failed to notice the difference with the original recording of human speech. Because the model could be used against AI principles (malicious applications, deep fakes, and so on), the authors have built a classifier that can recognize audio made with AudioLM and are investigating technology for audio “watermarking”
Parting thoughts
In recent months we have seen how several models have been capable of generating images (DALL-E, stable diffusion) and there are models such as GPT3 capable of generating text sequences. Generating audio sequences presents some additional difficulty but it seems that we will soon see some more major advances on this front.
In fact, Google has just unveiled AudioLM, a model capable of using an audio prompt (voice or piano) and generating the continuation. On the other hand, the same group that presented stable diffusion has just presented Harmonai (which, in fact, uses a similar algorithm of stable diffusion).
These technologies in the future could be used as background music for videos and presentations, better applications for health care settings, or Internet accessibility. On the other hand, these technologies could be used for deep fakes, misinformation spreading, scams, and so on.
If you have found it interesting:
You can look for my other articles, you can also subscribe to get notified when I publish articles, and you can also connect or reach me on LinkedIn. Thanks for your support!
Here is the link to my GitHub repository, where I am planning to collect code and many resources related to machine learning, artificial intelligence, and more.
GitHub – SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence, data science with math explanation and reusable code (in python and R)
Or feel free to check out some of my other articles on Medium:
- How artificial intelligence could save the Amazon rainforest
- Nobel prize Cyberpunk
- AlphaFold2 Year 1: Did It Change the World?
- Blending the power of AI with the delicacy of poetry
Google’s Audiolm: Generating Music by Hearing a Song’s Snippet was originally published in Towards AI on Medium, where people are continuing the conversation by highlighting and responding to this story.
Join thousands of data leaders on the AI newsletter. It’s free, we don’t spam, and we never share your email address. Keep up to date with the latest work in AI. From research to projects and ideas. If you are building an AI startup, an AI-related product, or a service, we invite you to consider becoming a sponsor.
Published via Towards AI



This Google AI's New Audio Generation Framework, 'AudioLM ...
https://www.marktechpost.com/2022/10/09/this-google-ais-new-audio-generation-framework-audiolm-learns-to-generate-realistic-speech-and-piano-music-by-listening-to-audio-only/
Oct 9, 2022 · AudioLM is a music-only model that undergoes training using only audio. To represent an audio sequence hierarchically, from semantic tokens to ...
Audio signals, whether human speech, musical composition, or ambient noise, entail different levels of abstraction. Prosody, syntax, grammar, and semantics are a few ways speech can be dissected and examined.
The problem of generating well-organized and consistent audio sequences at all three levels has been addressed by combining audio with transcriptions that can direct the generative process, such as text transcripts for speech synthesis or MIDI representations for the piano. However, this method falls short when attempting to represent non-transcribed features of audio, such as speaker qualities necessary to assist people with speech difficulties.
Language models have shown they can model high-level, long-term structures for various content types. “Textless NLP” has recently been advanced concerning unconditioned speech production. In particular, without textual annotations, a Transformer trained on discretized speech units may generate meaningful speech. The model is only trained on clean speech, and the synthesis is only possible with a single speaker. Thus acoustic diversity and quality are still constrained.
A new Google research introduces a new framework called AudioLM for audio production that can learn to make realistic speech and piano music just by listening to audio. AudioLM outperforms earlier systems and pushes the boundaries of audio production with applications in speech synthesis and computer-assisted music because of its long-term consistency (e.g., syntax in speech) and high fidelity. Using the same AI principles that guided the development of our other models, we have created a system to detect AudioLM-generated synthetic sounds.
AudioLM uses two distinct types of audio tokens to solve these problems. In the first step, w2v-BERT, a self-supervised audio model, is used to extract semantic tokens. These tokens heavily downsample the audio signal to model lengthy audio sequences while capturing local dependencies and long-term global structure.
It is possible to achieve high audio quality and long-term consistency by training a system to generate semantic and acoustic tokens. A low level of fidelity is present in the reconstructed audio when using these tokens. To get over this restriction, the team employs a SoundStream neural codec to generate acoustic tokens that capture the nuances of the audio waveform and permit accurate synthesis.
AudioLM is a music-only model that undergoes training using only audio. To represent an audio sequence hierarchically, from semantic tokens to fine acoustic tokens, AudioLM links together various Transformer models. In the same way, a text language model is taught, each stage learns to predict the next token based on the tokens that came before them.
- In the first phase, semantic tokens are used to model the overall structure of the audio file.
- Second, the complete semantic token sequence is fed and the previous coarse acoustic tokens into the coarse acoustic model as conditioning, allowing the model to predict the next set of tokens. In this stage, acoustic qualities are modeled, such as those of the speaker in a speech or the sound of a musical instrument.
- The final audio is refined by applying the fine acoustic model to the coarse acoustic tokens. Afterward, they recreated an audio waveform by feeding a series of acoustic tokens into a SoundStream decoder. Once trained, AudioLM may be conditioned on short audio clips, allowing it to produce seamless loops.
To verify the findings, human raters listened to audio samples. They judged whether they were hearing a natural continuation of a recorded human voice or a synthetic one generated by AudioLM. Their findings show a success percentage of 51.2%. This means that the ordinary listener will have difficulty telling the difference between AudioLM-generated speech and real human speech.
The researchers investigated the possibility that people might mistake the brief speech samples generated by AudioLM for actual human speech and took measures to reduce this risk. To do this, they developed a classifier that can accurately identify AudioLM-generated synthetic speech (98.6% of the time). This demonstrates how simple audio classifiers can readily identify continuations produced by AudioLM, despite their (near) indistinguishability to some listeners. This is an essential first step in securing AudioLM from abuse, and future work may investigate technologies like audio “watermarking” to further strengthen security.
The study’s authors believe that their work will pave the way for future applications of AudioLM to a wider variety of audio and the incorporation of AudioLM into an encoder-decoder framework for conditioned tasks like text-to-speech and speech-to-speech translation.
This Article is written as a research summary article by Marktechpost Staff based on the research paper 'AudioLM: a Language Modeling Approach to Audio Generation'. All Credit For This Research Goes To Researchers on This Project. Check out the paper, project and reference article. Please Don't Forget To Join Our ML Subreddit
Tanushree Shenwai is a consulting intern at MarktechPost. She is currently pursuing her B.Tech from the Indian Institute of Technology(IIT), Bhubaneswar. She is a Data Science enthusiast and has a keen interest in the scope of application of artificial intelligence in various fields. She is passionate about exploring the new advancements in technologies and their real-life application.



Google AudioLM is already capable of making speeches with your ...
https://www.lenseup.com/en/google-audio-lm-is-already-capable-of-making-speeches-with-your-voice/
Oct 8, 2022 · Google's research division has presented AudioLM, a framework for generating high-quality audio that remains consistent over the long term.
Computers are already able to play chess games and they became unbeatable opponents; we let them read our texts and they started to write. They also learned to paint and retouch photographs. Did anyone doubt that artificial intelligence would be able to do the same with speeches and music?
Google’s research division has presented AudioLM, a framework for generating high-quality audio that remains consistent over the long term. To do this, it starts with a recording of just a few seconds in length, and is able to prolong it in a natural and coherent way. What is remarkable is that it achieves this without being trained with previous transcriptions or annotations even though the generated speech is syntactically and semantically correct Moreover, it maintains the identity and prosody of the speaker to such an extent that the listener is unable to discern which part of the audio is original and which has been generated by an artificial intelligence.
The examples of this artificial intelligence are striking. Not only is it able to replicate articulation, pitch, timbre and intensity, but it is able to input the sound of the speaker’s breathing and form meaningful sentences. If it does not start from a studio audio, but from one with background noise, AudioLM replicates it to give it continuity. More samples can be heard on the AudioLM website.
Google Brain
Google Audio LM is an artificial intelligence trained in semantics and acoustics. How does it do it? The generation of audio or music is nothing new. But the way Google researchers have devised to tackle the problem is. From each audio, semantic markers are extracted to encode a high-level structure (phonemes, lexicon, semantics…), and acoustic markers (speaker identity, recording quality, background noise…). With this data already processed and understandable to the artificial intelligence, AudioML begins its work by establishing a hierarchy in which it first predicts the semantic markers, which are then used as conditions for predicting the acoustic markers. The latter are then used again at the end to convert the bits into something humans can hear.
This semantic separation of acoustics, and its hierarchy, is not only a beneficial practice for training language models to generate speech. According to the researchers, it is also more effective for continuing piano compositions, as they show on their website. It is much better than models that are only trained using acoustic markers.
The most significant thing about AudioLM’s artificial intelligence is not that it is able to continue speeches and melodies, but that it can do everything at once. It is therefore a unique language model that can be used to convert text to speech – a robot could read entire books – or to make any device able to communicate with people using a familiar voice. This idea has already been explored by Amazon, which considered using the voice of loved ones in its Alexa speakers.
Innovation or danger?
Software such as Dalle-2 and Stable Diffusion are exceptional tools that allow ideas to be sketched out or creative resources to be generated in a few seconds. Audio can be even more important, and one can imagine the voice of an announcer being used on demand by various companies. Even films could be dubbed with the voices of deceased actors. The reader may be wondering whether this possibility, while exciting, is not dangerous. Any audio recording could be manipulated for political, legal or judicial purposes. Google says that, while humans may have difficulty detecting what comes from humans and what comes from artificial intelligence, a computer can detect whether the audio is organic or not. In other words, not only can the machine replace us, but another machine will be essential to assess its work.
For the moment AudioLM is not open to the public, it is only a language model that can be integrated into different projects. But this demonstration, together with OpenAI’s Jukebox music programme, shows how quickly we are entering a new world where nobody will know, or care, whether that photograph is taken by a person or whether there is a person or an artificially generated voice-over on the other end of the phone in real time.



Google AI on Twitter: "We introduced a new framework, AudioLM ...
https://twitter.com/GoogleAI/status/1587797064636715009
Nov 2, 2022 · We introduced a new framework, AudioLM, that learns to generate realistic speech and piano music by listening to audio only ↓ (3/5).
JavaScript is not available.
We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.
Help Center



Google's new AI can hear a snippet of song—and then keep on ...
https://www.technologyreview.com/2022/10/07/1060897/ai-audio-generation/
Oct 7, 2022 · AudioLM, developed by Google researchers, generates audio that fits the style of the prompt, including complex sounds like piano music, ...
Google’s new AI can hear a snippet of song—and then keep on playing
The technique, called AudioLM, generates naturalistic sounds without the need for human annotation.
A new AI system can create natural-sounding speech and music after being prompted with a few seconds of audio.
AudioLM, developed by Google researchers, generates audio that fits the style of the prompt, including complex sounds like piano music, or people speaking, in a way that is almost indistinguishable from the original recording. The technique shows promise for speeding up the process of training AI to generate audio, and it could eventually be used to auto-generate music to accompany videos.
(You can listen to all of the examples here.)
AI-generated audio is commonplace: voices on home assistants like Alexa use natural language processing. AI music systems like OpenAI’s Jukebox have already generated impressive results, but most existing techniques need people to prepare transcriptions and label text-based training data, which takes a lot of time and human labor. Jukebox, for example, uses text-based data to generate song lyrics.
AudioLM, described in a non-peer-reviewed paper last month, is different: it doesn’t require transcription or labeling. Instead, sound databases are fed into the program, and machine learning is used to compress the audio files into sound snippets, called “tokens,” without losing too much information. This tokenized training data is then fed into a machine-learning model that uses natural language processing to learn the sound’s patterns.
To generate the audio, a few seconds of sound are fed into AudioLM, which then predicts what comes next. The process is similar to the way language models like GPT-3 predict what sentences and words typically follow one another.
The audio clips released by the team sound pretty natural. In particular, piano music generated using AudioLM sounds more fluid than piano music generated using existing AI techniques, which tends to sound chaotic.
Roger Dannenberg, who researches computer-generated music at Carnegie Mellon University, says AudioLM already has much better sound quality than previous music generation programs. In particular, he says, AudioLM is surprisingly good at re-creating some of the repeating patterns inherent in human-made music. To generate realistic piano music, AudioLM has to capture a lot of the subtle vibrations contained in each note when piano keys are struck. The music also has to sustain its rhythms and harmonies over a period of time.
“That’s really impressive, partly because it indicates that they are learning some kinds of structure at multiple levels,” Dannenberg says.
AudioLM isn’t only confined to music. Because it was trained on a library of recordings of humans speaking sentences, the system can also generate speech that continues in the accent and cadence of the original speaker—although at this point those sentences can still seem like non sequiturs that don’t make any sense. AudioLM is trained to learn what types of sound snippets occur frequently together, and it uses the process in reverse to produce sentences. It also has the advantage of being able to learn the pauses and exclamations that are inherent in spoken languages but not easily translated into text.
Rupal Patel, who researches information and speech science at Northeastern University, says that previous work using AI to generate audio could capture those nuances only if they were explicitly annotated in training data. In contrast, AudioLM learns those characteristics from the input data automatically, which adds to the realistic effect.
“There is a lot of what we could call linguistic information that is not in the words that you pronounce, but it’s another way of communicating based on the way you say things to express a specific intention or specific emotion,” says Neil Zeghidour, a co-creator of AudioLM. For example, someone may laugh after saying something to indicate that it was a joke. “All that makes speech natural,” he says.
Eventually, AI-generated music could be used to provide more natural-sounding background soundtracks for videos and slideshows. Speech generation technology that sounds more natural could help improve internet accessibility tools and bots that work in health care settings, says Patel. The team also hopes to create more sophisticated sounds, like a band with different instruments or sounds that mimic a recording of a tropical rainforest.
However, the technology’s ethical implications need to be considered, Patel says. In particular, it’s important to determine whether the musicians who produce the clips used as training data will get attribution or royalties from the end product—an issue that has cropped up with text-to-image AIs. AI-generated speech that’s indistinguishable from the real thing could also become so convincing that it enables the spread of misinformation more easily.
In the paper, the researchers write that they are already considering and working to mitigate these issues—for example, by developing techniques to distinguish natural sounds from sounds produced using AudioLM. Patel also suggested including audio watermarks in AI-generated products to make them easier to distinguish from natural audio.
Deep Dive
Artificial intelligence
A Roomba recorded a woman on the toilet. How did screenshots end up on Facebook?
Robot vacuum companies say your images are safe, but a sprawling global supply chain for data from our devices creates risk.
Roomba testers feel misled after intimate images ended up on Facebook
An MIT Technology Review investigation recently revealed how images of a minor and a tester on the toilet ended up on social media. iRobot said it had consent to collect this kind of data from inside homes—but participants say otherwise.
How to spot AI-generated text
The internet is increasingly awash with text written by AI software. We need new tools to detect it.
The original startup behind Stable Diffusion has launched a generative AI for video
Runway’s new model, called Gen-1, can change the visual style of existing videos and movies.
Stay connected
Get the latest updates from
MIT Technology Review
Discover special offers, top stories, upcoming events, and more.



AudioLM: A Language Modeling Approach to Audio Generation
https://news.ycombinator.com/item?id=32808890
It is fascinating that it is now able to continue both piano and speech coherently. At least for a while, it is indistinguishable to me (in the examples) to ...
The paper is here: https://arxiv.org/pdf/2209.03143.pdf
When I read a GPT-3 generated text, I catch myself that I do the same thing. I'm very forgiving. Interestingly, with these speech continuation the illusion of somebody being there on the other side, is much stronger. It's like listening to an old Feynman lecture or the like. You don't quite get it, but surely it must make sense, right. It doesn't of course. The box is empty. Or is it?





-------------Y------------google-research.github.io › seanet › audiolmAudioLM - google-research.github.io
https://google-research.github.io/seanet/audiolm/examples/
AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives.
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour
Google Research
Abstract. We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.
Continuations using 3 second prompts from LibriSpeech test-{clean, other}, for speakers and content not seen during training. AudioLM excels at generating continuations that:
For acoustic generation, we sample the acoustic tokens given the semantic tokens extracted from the original samples from LibriSpeech test-clean. The model generates samples with different speakers and recording conditions, while the semantic content is identical.
The unconditional generation performs sampling without using prompts. In that case, every sequence varies in speaker identity, linguistic content, and recording conditions.
To illustrate that the semantic tokens are crucial for generating coherent linguistic content, we train the language model on the acoustic tokens only. While the generated continuations of the 4-second prompts maintain speaker identity, the linguistic content is inconsistent, and often akin to babbling.
We compare SoundStream reconstructions of two models, one using 3-layer residual vector quantization (3-RVQ) and another with 12 layers (12-RVQ), the latter being the default. The equivalent bitrates are 1.5 kbps and 6 kbps.
AudioLM is not limited to modeling speech. It can also learn to generate coherent piano music continuations, despite being trained on piano music without any symbolic representation. We also show the continuations produced by a version of the model trained exclusively on the acoustic tokens. These continuations are much less coherent, stressing the importance of the semantic tokens in our framework. The 4-second prompts come from the test split of MAESTRO dataset.



ai.googleblog.com › 2022 › 10AudioLM: a Language Modeling Approach to Audio Generation
https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html
In “AudioLM: a Language Modeling Approach to Audio Generation”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming previous systems and pushing the frontiers of audio generation with applications in speech synthesis or computer-assisted music.
Generating realistic audio requires modeling information represented at different scales. For example, just as music builds complex musical phrases from individual notes, speech combines temporally local structures, such as phonemes or syllables, into words and sentences. Creating well-structured and coherent audio sequences at all these scales is a challenge that has been addressed by coupling audio with transcriptions that can guide the generative process, be it text transcripts for speech synthesis or MIDI representations for piano. However, this approach breaks when trying to model untranscribed aspects of audio, such as speaker characteristics necessary to help people with speech impairments recover their voice, or stylistic components of a piano performance.
In “AudioLM: a Language Modeling Approach to Audio Generation”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming previous systems and pushing the frontiers of audio generation with applications in speech synthesis or computer-assisted music. Following our AI Principles, we've also developed a model to identify synthetic audio generated by AudioLM.
From Text to Audio Language Models
In recent years, language models trained on very large text corpora have demonstrated their exceptional generative abilities, from open-ended dialogue to machine translation or even common-sense reasoning. They have further shown their capacity to model other signals than texts, such as natural images. The key intuition behind AudioLM is to leverage such advances in language modeling to generate audio without being trained on annotated data.
However, some challenges need to be addressed when moving from text language models to audio language models. First, one must cope with the fact that the data rate for audio is significantly higher, thus leading to much longer sequences — while a written sentence can be represented by a few dozen characters, its audio waveform typically contains hundreds of thousands of values. Second, there is a one-to-many relationship between text and audio. This means that the same sentence can be rendered by different speakers with different speaking styles, emotional content and recording conditions.
To overcome both challenges, AudioLM leverages two kinds of audio tokens. First, semantic tokens are extracted from w2v-BERT, a self-supervised audio model. These tokens capture both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure (e.g., language syntax and semantic content in speech, harmony and rhythm in piano music), while heavily downsampling the audio signal to allow for modeling long sequences.
However, audio reconstructed from these tokens demonstrates poor fidelity. To overcome this limitation, in addition to semantic tokens, we rely on acoustic tokens produced by a SoundStream neural codec, which capture the details of the audio waveform (such as speaker characteristics or recording conditions) and allow for high-quality synthesis. Training a system to generate both semantic and acoustic tokens leads simultaneously to high audio quality and long-term consistency.
Training an Audio-Only Language Model
AudioLM is a pure audio model that is trained without any text or symbolic representation of music. AudioLM models an audio sequence hierarchically, from semantic tokens up to fine acoustic tokens, by chaining several Transformer models, one for each stage. Each stage is trained for the next token prediction based on past tokens, as one would train a text language model. The first stage performs this task on semantic tokens to model the high-level structure of the audio sequence.
In the second stage, we concatenate the entire semantic token sequence, along with the past coarse acoustic tokens, and feed both as conditioning to the coarse acoustic model, which then predicts the future tokens. This step models acoustic properties such as speaker characteristics in speech or timbre in music.
In the third stage, we process the coarse acoustic tokens with the fine acoustic model, which adds even more detail to the final audio. Finally, we feed acoustic tokens to the SoundStream decoder to reconstruct a waveform.
After training, one can condition AudioLM on a few seconds of audio, which enables it to generate consistent continuation. In order to showcase the general applicability of the AudioLM framework, we consider two tasks from different audio domains:
- Speech continuation, where the model is expected to retain the speaker characteristics, prosody and recording conditions of the prompt while producing new content that is syntactically correct and semantically consistent.
- Piano continuation, where the model is expected to generate piano music that is coherent with the prompt in terms of melody, harmony and rhythm.
In the video below, you can listen to examples where the model is asked to continue either speech or music and generate new content that was not seen during training. As you listen, note that everything you hear after the gray vertical line was generated by AudioLM and that the model has never seen any text or musical transcription, but rather just learned from raw audio. We release more samples on this webpage.
To validate our results, we asked human raters to listen to short audio clips and decide whether it is an original recording of human speech or a synthetic continuation generated by AudioLM. Based on the ratings collected, we observed a 51.2% success rate, which is not statistically significantly different from the 50% success rate achieved when assigning labels at random. This means that speech generated by AudioLM is hard to distinguish from real speech for the average listener.
Our work on AudioLM is for research purposes and we have no plans to release it more broadly at this time. In alignment with our AI Principles, we sought to understand and mitigate the possibility that people could misinterpret the short speech samples synthesized by AudioLM as real speech. For this purpose, we trained a classifier that can detect synthetic speech generated by AudioLM with very high accuracy (98.6%). This shows that despite being (almost) indistinguishable to some listeners, continuations generated by AudioLM are very easy to detect with a simple audio classifier. This is a crucial first step to help protect against the potential misuse of AudioLM, with future efforts potentially exploring technologies such as audio “watermarking”.
Conclusion
We introduce AudioLM, a language modeling approach to audio generation that provides both long-term coherence and high audio quality. Experiments on speech generation show not only that AudioLM can generate syntactically and semantically coherent speech without any text, but also that continuations produced by the model are almost indistinguishable from real speech by humans. Moreover, AudioLM goes well beyond speech and can model arbitrary audio signals such as piano music. This encourages the future extensions to other types of audio (e.g., multilingual speech, polyphonic music, and audio events) as well as integrating AudioLM into an encoder-decoder framework for conditioned tasks such as text-to-speech or speech-to-speech translation.
Acknowledgments
The work described here was authored by Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi and Neil Zeghidour. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google.



ai.googleblog.com › 2022 › 10AudioLM: a Language Modeling Approach to Audio Generation
https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html%3fhl%3dno
AudioLM is a pure audio model that is trained without any text or symbolic representation of music. AudioLM models an audio sequence hierarchically, from semantic tokens up to fine acoustic tokens, by chaining several Transformer models, one for each stage.

dataconomy.com › 2022 › 09AudioLM Indiscernibly Mimics Speech And Music - Dataconomy
https://dataconomy.com/2022/09/ai-audiolm-can-perfectly-mimic-speech/
Google’s AudioLM performs miracles both with speech and music Google’s research group has launched AudioLM, a framework for producing high-quality audio that maintains consistency across time. To do this, it begins with a recording that is just a few seconds long and is capable of extending it naturally and logically.
- Google’s research division has launched AudioLM, a framework for creating high-quality audio that retains consistency across time.
- The most amazing part is that it does so without prior transcripts or annotations, despite the fact that the generated speech is syntactically and semantically acceptable.
- Furthermore, it keeps the speaker’s identity and prosody to the point that the listener cannot discern which portion of the audio is genuine and which was generated by artificial intelligence.
- The most crucial feature of AudioLM’s artificial intelligence is its ability to accomplish several tasks at once, not only repeat talks and tunes.
- AudioLM is not yet publicly accessible; it is only a language model that may be applied in a variety of applications.
We showed them chess games, and they quickly became unbeatable opponents; we let them read our texts, and they soon began to write. They also learned to paint and do photo edits. Was there anyone who doubted that artificial intelligence could do the same with speeches and music?
Google’s AudioLM performs miracles both with speech and music
Google’s research group has launched AudioLM, a framework for producing high-quality audio that maintains consistency across time. To do this, it begins with a recording that is just a few seconds long and is capable of extending it naturally and logically.
The most impressive aspect is that it does so without being taught with previous transcripts or annotations, despite the fact that the created speech is syntactically and semantically reasonable. Furthermore, it preserves the speaker’s identity and prosody to the point that the listener is unable to determine which piece of the audio is genuine and which was created by artificial intelligence.
The applications of artificial intelligence are astounding. It can not only mimic articulation, pitch, timbre, and intensity, but it can also introduce the sound of the speaker’s breath and make understandable phrases. If it’s not from a studio but rather from a recording with background noise, AudioLM mimics it to ensure continuity. More examples are available on the AudioLM website.
AudioLM was trained in semantics and acoustics
The creation of audio or music is not a new phenomenon. However, it is the approach taken by Google researchers to solve the issue. Semantic indicators (phonemes, lexicon, semantics…) and acoustic markers (speaker identity, recording quality, background noise…) are collected from each audio to encode a high-level structure (phonemes, lexicon, semantics…).
With this data already processed and intelligible for AI, AudioML starts its job by constructing a hierarchy in which it predicts semantic markers first, which are subsequently utilized as constraints to forecast acoustic markers. The latter is employed once more at the end to turn the bits into something we can hear.
This semantic separation and hierarchy of acoustics are not just useful for training language models that create speech. It is also more successful for continuing piano compositions, according to the researchers, as demonstrated on their website. It outperforms models that are exclusively trained using auditory markers.
France starts using artificial intelligence to discover taxable swimming pools
The most important aspect of AudioLM’s artificial intelligence is that it can perform everything at once, not only repeat speeches and melodies. It is, therefore, a single language model that can be used for text-to-speech — a robot might read entire novels and replace professional voice actors — or to enable any gadget to speak with humans using a familiar voice. Amazon has already investigated the possibility of utilizing the voice of loved ones in its Alexa devices.
Is AI becoming more dangerous by the day?
Programs like DALL-E 2 and Stable Diffusion are excellent tools for quickly sketching ideas or generating creative materials. Audio may be much more significant, and one may see firms using an announcer’s voice on demand. The voices of departed actors might even be used in dubbing films.
You may be thinking this idea, while thrilling, is also risky. Any audio recording can be tampered with for political, legal, or judicial objectives. According to Google, while people have difficulties distinguishing between what comes from man and what comes from artificial intelligence, a computer can discern whether the audio is organic or not. Not only that machines might replace us, but another machine will be required to appraise their job.
Artificial intelligence jobs are in high demand: Here are the career paths
AudioLM is not yet available to the public; it is only a language model that may be implemented into various applications. However, this example, along with OpenAI’s Jukebox music software, highlights how swiftly we’re entering a new world where no one will ever know, or care, if that photo was shot by a person or if there’s someone on the other end of the phone.





-------------B------------AudioLM - google-research.github.io
https://google-research.github.io/seanet/audiolm/examples/
WebAudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives.
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour
Google Research
Abstract. We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.
Continuations using 3 second prompts from LibriSpeech test-{clean, other}, for speakers and content not seen during training. AudioLM excels at generating continuations that:
For acoustic generation, we sample the acoustic tokens given the semantic tokens extracted from the original samples from LibriSpeech test-clean. The model generates samples with different speakers and recording conditions, while the semantic content is identical.
The unconditional generation performs sampling without using prompts. In that case, every sequence varies in speaker identity, linguistic content, and recording conditions.
To illustrate that the semantic tokens are crucial for generating coherent linguistic content, we train the language model on the acoustic tokens only. While the generated continuations of the 4-second prompts maintain speaker identity, the linguistic content is inconsistent, and often akin to babbling.
We compare SoundStream reconstructions of two models, one using 3-layer residual vector quantization (3-RVQ) and another with 12 layers (12-RVQ), the latter being the default. The equivalent bitrates are 1.5 kbps and 6 kbps.
AudioLM is not limited to modeling speech. It can also learn to generate coherent piano music continuations, despite being trained on piano music without any symbolic representation. We also show the continuations produced by a version of the model trained exclusively on the acoustic tokens. These continuations are much less coherent, stressing the importance of the semantic tokens in our framework. The 4-second prompts come from the test split of MAESTRO dataset.



AudioLM: a Language Modeling Approach to Audio Generation
https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html
WebOct 6, 2022 · In “AudioLM: a Language Modeling Approach to Audio Generation”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming …
Generating realistic audio requires modeling information represented at different scales. For example, just as music builds complex musical phrases from individual notes, speech combines temporally local structures, such as phonemes or syllables, into words and sentences. Creating well-structured and coherent audio sequences at all these scales is a challenge that has been addressed by coupling audio with transcriptions that can guide the generative process, be it text transcripts for speech synthesis or MIDI representations for piano. However, this approach breaks when trying to model untranscribed aspects of audio, such as speaker characteristics necessary to help people with speech impairments recover their voice, or stylistic components of a piano performance.
In “AudioLM: a Language Modeling Approach to Audio Generation”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming previous systems and pushing the frontiers of audio generation with applications in speech synthesis or computer-assisted music. Following our AI Principles, we've also developed a model to identify synthetic audio generated by AudioLM.
From Text to Audio Language Models
In recent years, language models trained on very large text corpora have demonstrated their exceptional generative abilities, from open-ended dialogue to machine translation or even common-sense reasoning. They have further shown their capacity to model other signals than texts, such as natural images. The key intuition behind AudioLM is to leverage such advances in language modeling to generate audio without being trained on annotated data.
However, some challenges need to be addressed when moving from text language models to audio language models. First, one must cope with the fact that the data rate for audio is significantly higher, thus leading to much longer sequences — while a written sentence can be represented by a few dozen characters, its audio waveform typically contains hundreds of thousands of values. Second, there is a one-to-many relationship between text and audio. This means that the same sentence can be rendered by different speakers with different speaking styles, emotional content and recording conditions.
To overcome both challenges, AudioLM leverages two kinds of audio tokens. First, semantic tokens are extracted from w2v-BERT, a self-supervised audio model. These tokens capture both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure (e.g., language syntax and semantic content in speech, harmony and rhythm in piano music), while heavily downsampling the audio signal to allow for modeling long sequences.
However, audio reconstructed from these tokens demonstrates poor fidelity. To overcome this limitation, in addition to semantic tokens, we rely on acoustic tokens produced by a SoundStream neural codec, which capture the details of the audio waveform (such as speaker characteristics or recording conditions) and allow for high-quality synthesis. Training a system to generate both semantic and acoustic tokens leads simultaneously to high audio quality and long-term consistency.
Training an Audio-Only Language Model
AudioLM is a pure audio model that is trained without any text or symbolic representation of music. AudioLM models an audio sequence hierarchically, from semantic tokens up to fine acoustic tokens, by chaining several Transformer models, one for each stage. Each stage is trained for the next token prediction based on past tokens, as one would train a text language model. The first stage performs this task on semantic tokens to model the high-level structure of the audio sequence.
In the second stage, we concatenate the entire semantic token sequence, along with the past coarse acoustic tokens, and feed both as conditioning to the coarse acoustic model, which then predicts the future tokens. This step models acoustic properties such as speaker characteristics in speech or timbre in music.
In the third stage, we process the coarse acoustic tokens with the fine acoustic model, which adds even more detail to the final audio. Finally, we feed acoustic tokens to the SoundStream decoder to reconstruct a waveform.
After training, one can condition AudioLM on a few seconds of audio, which enables it to generate consistent continuation. In order to showcase the general applicability of the AudioLM framework, we consider two tasks from different audio domains:
- Speech continuation, where the model is expected to retain the speaker characteristics, prosody and recording conditions of the prompt while producing new content that is syntactically correct and semantically consistent.
- Piano continuation, where the model is expected to generate piano music that is coherent with the prompt in terms of melody, harmony and rhythm.
In the video below, you can listen to examples where the model is asked to continue either speech or music and generate new content that was not seen during training. As you listen, note that everything you hear after the gray vertical line was generated by AudioLM and that the model has never seen any text or musical transcription, but rather just learned from raw audio. We release more samples on this webpage.
To validate our results, we asked human raters to listen to short audio clips and decide whether it is an original recording of human speech or a synthetic continuation generated by AudioLM. Based on the ratings collected, we observed a 51.2% success rate, which is not statistically significantly different from the 50% success rate achieved when assigning labels at random. This means that speech generated by AudioLM is hard to distinguish from real speech for the average listener.
Our work on AudioLM is for research purposes and we have no plans to release it more broadly at this time. In alignment with our AI Principles, we sought to understand and mitigate the possibility that people could misinterpret the short speech samples synthesized by AudioLM as real speech. For this purpose, we trained a classifier that can detect synthetic speech generated by AudioLM with very high accuracy (98.6%). This shows that despite being (almost) indistinguishable to some listeners, continuations generated by AudioLM are very easy to detect with a simple audio classifier. This is a crucial first step to help protect against the potential misuse of AudioLM, with future efforts potentially exploring technologies such as audio “watermarking”.
Conclusion
We introduce AudioLM, a language modeling approach to audio generation that provides both long-term coherence and high audio quality. Experiments on speech generation show not only that AudioLM can generate syntactically and semantically coherent speech without any text, but also that continuations produced by the model are almost indistinguishable from real speech by humans. Moreover, AudioLM goes well beyond speech and can model arbitrary audio signals such as piano music. This encourages the future extensions to other types of audio (e.g., multilingual speech, polyphonic music, and audio events) as well as integrating AudioLM into an encoder-decoder framework for conditioned tasks such as text-to-speech or speech-to-speech translation.
Acknowledgments
The work described here was authored by Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi and Neil Zeghidour. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google.



GitHub - lucidrains/audiolm-pytorch: Implementation of AudioLM…
https://github.com/lucidrains/audiolm-pytorch
WebSep 9, 2022 · AudioLM - Pytorch. Implementation of AudioLM, a Language Modeling Approach to Audio Generation out of Google Research, in Pytorch. It also extends the work for conditioning with classifier free guidance with T5. This allows for one to do text-to-audio or TTS, not offered in the paper. Yes, this means VALL-E can be trained from this …
AudioLM - Pytorch
Implementation of AudioLM, a Language Modeling Approach to Audio Generation out of Google Research, in Pytorch
It also extends the work for conditioning with classifier free guidance with T5. This allows for one to do text-to-audio or TTS, not offered in the paper. Yes, this means VALL-E can be trained from this repository. It is essentially the same.
Please join if you are interested in replicating this work in the open
This repository now also contains a MIT licensed version of SoundStream. Once EnCodec becomes MIT licensed, then I will consider adding a wrapper for that as well for use here.
Update: AudioLM was essentially used to 'solve' music generation in the new MusicLM
In the future, this movie clip would no longer make any sense. You would just prompt an AI instead.
Appreciation
-
Stability.ai for the generous sponsorship to work and open source cutting edge artificial intelligence research
-
🤗Huggingface for their amazing accelerate and transformers libraries
-
-
@eonglints and Joseph for offering their professional advice and expertise as well as pull requests!
-
@djqualia, @yigityu, @inspirit, and @BlackFox1197 for helping with the debugging of soundstream
-
Allen for catching and fixing some bugs!
-
Andrey for identifying a missing loss in soundstream and guiding me through the proper mel spectrogram hyperparameters
Install
$ pip install audiolm-pytorch
Usage
First,
SoundStream needs to be trained on a large corpus of audio data
from audiolm_pytorch import SoundStream, SoundStreamTrainer soundstream = SoundStream( codebook_size = 1024, rq_num_quantizers = 8, attn_window_size = 128, # local attention receptive field at bottleneck attn_depth = 2 # 2 local attention transformer blocks - the soundstream folks were not experts with attention, so i took the liberty to add some. encodec went with lstms, but attention should be better ) trainer = SoundStreamTrainer( soundstream, folder = '/path/to/audio/files', batch_size = 4, grad_accum_every = 8, # effective batch size of 32 data_max_length = 320 * 32, num_train_steps = 10000 ).cuda() trainer.train()
Then three separate transformers (
SemanticTransformer,
CoarseTransformer,
FineTransformer) need to be trained
ex.
SemanticTransformer
import torch from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer # hubert checkpoints can be downloaded at # https://github.com/facebookresearch/fairseq/tree/main/examples/hubert wav2vec = HubertWithKmeans( checkpoint_path = './hubert/hubert_base_ls960.pt', kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin' ) semantic_transformer = SemanticTransformer( num_semantic_tokens = wav2vec.codebook_size, dim = 1024, depth = 6 ).cuda() trainer = SemanticTransformerTrainer( transformer = semantic_transformer, wav2vec = wav2vec, folder ='/path/to/audio/files', batch_size = 1, data_max_length = 320 * 32, num_train_steps = 1 ) trainer.train()
ex.
CoarseTransformer
import torch from audiolm_pytorch import HubertWithKmeans, SoundStream, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer wav2vec = HubertWithKmeans( checkpoint_path = './hubert/hubert_base_ls960.pt', kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin' ) soundstream = SoundStream( codebook_size = 1024, rq_num_quantizers = 8, ) soundstream.load('/path/to/trained/soundstream.pt') coarse_transformer = CoarseTransformer( num_semantic_tokens = wav2vec.codebook_size, codebook_size = 1024, num_coarse_quantizers = 3, dim = 512, depth = 6 ) trainer = CoarseTransformerTrainer( transformer = coarse_transformer, soundstream = soundstream, wav2vec = wav2vec, folder = '/path/to/audio/files', batch_size = 1, data_max_length = 320 * 32, num_train_steps = 10000 ) trainer.train()
ex.
FineTransformer
import torch from audiolm_pytorch import SoundStream, FineTransformer, FineTransformerWrapper, FineTransformerTrainer soundstream = SoundStream( codebook_size = 1024, rq_num_quantizers = 8, ) soundstream.load('/path/to/trained/soundstream.pt') fine_transformer = FineTransformer( num_coarse_quantizers = 3, num_fine_quantizers = 5, codebook_size = 1024, dim = 512, depth = 6 ) trainer = FineTransformerTrainer( transformer = fine_transformer, soundstream = soundstream, folder = '/path/to/audio/files', batch_size = 1, data_max_length = 320 * 32, num_train_steps = 10000 ) trainer.train()
All together now
from audiolm_pytorch import AudioLM audiolm = AudioLM( wav2vec = wav2vec, soundstream = soundstream, semantic_transformer = semantic_transformer, coarse_transformer = coarse_transformer, fine_transformer = fine_transformer ) generated_wav = audiolm(batch_size = 1) # or with priming generated_wav_with_prime = audiolm(prime_wave = torch.randn(1, 320 * 8)) # or with text condition, if given generated_wav_with_text_condition = audiolm(text = ['chirping of birds and the distant echos of bells'])
Text Conditioned Audio Synthesis
Update: Looks like this will work, given 'VALL-E'
ex. Semantic Transformer
import torch from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer wav2vec = HubertWithKmeans( checkpoint_path = './hubert/hubert_base_ls960.pt', kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin' ) semantic_transformer = SemanticTransformer( num_semantic_tokens = 500, dim = 1024, depth = 6, has_condition = True, # this will have to be set to True cond_as_self_attn_prefix = True # whether to condition as prefix to self attention, instead of cross attention, as was done in 'VALL-E' paper ).cuda() # mock text video dataset (as an example) # you will have to extend your own from `Dataset`, and return an audio tensor as well as a string (the audio description) in any order (the framework will autodetect and route it into the transformer) from torch.utils.data import Dataset class MockTextAudioDataset(Dataset): def __init__(self, length = 100, audio_length = 320 * 32): super().__init__() self.audio_length = audio_length self.len = length def __len__(self): return self.len def __getitem__(self, idx): mock_audio = torch.randn(self.audio_length) mock_caption = 'audio caption' return mock_caption, mock_audio dataset = MockTextAudioDataset() # instantiate semantic transformer trainer and train trainer = SemanticTransformerTrainer( transformer = semantic_transformer, wav2vec = wav2vec, dataset = dataset, batch_size = 4, grad_accum_every = 8, data_max_length = 320 * 32, num_train_steps = 100000 ) trainer.train() # after much training above sample = trainer.generate(text = ['sound of rain drops on the rooftops'], batch_size = 1, max_length = 2) # (1, < 128) - may terminate early if it detects [eos]
Todo
-
complete CoarseTransformer
-
use fairseq vq-wav2vec for embeddings
-
add conditioning
-
add classifier free guidance
-
add unique consecutive for
-
incorporate ability to use hubert intermediate features as semantic tokens, recommended by eonglints
-
accommodate variable lengthed audio, bring in eos token
-
make sure unique consecutive works with coarse transformer
-
pretty printing all discriminator losses to log
-
handle when generating semantic tokens, that last logits may not be necessarily the last in the sequence given unique consecutive processing
-
complete sampling code for both Coarse and Fine Transformers, which will be tricky
-
make sure full inference with or without prompting works on the
AudioLMclass
-
complete full training code for soundstream, taking care of discriminator training
-
add efficient gradient penalty for discriminators for soundstream
-
wire up sample hz from sound dataset -> transformers, and have proper resampling within during training - think about whether to allow for dataset to have sound files of varying or enforce same sample hz
-
full transformer training code for all three transformers
-
refactor so semantic transformer has a wrapper to that handles unique consecutives as well as wav to hubert or vq-wav2vec
-
simply not self attend to eos token on the prompting side (semantic for coarse transformer, coarse for fine transformer)
-
add structured dropout from forgetful causal masking, far better than traditional dropouts
-
figure out how to suppress logging in fairseq
-
assert that all three transformers passed into audiolm is compatible
-
figure out how to do the normalization across each dimension mentioned in the paper, but ignore it for v1 of the framework
-
DRY a little at the end
-
test with speech synthesis for starters
-
add option to use flash attention
-
simplify training even more within AudioLM class
-
cli tool, something like
audiolm generate <wav.file | text>and save generated wav file to local directory
-
return a list of waves in the case of variable lengthed audio
-
just take care of the edge case in coarse transformer text conditioned training, where the raw wave is resampled at different frequencies. autodetermine how to route based on length
Citations
@inproceedings{Borsos2022AudioLMAL, title = {AudioLM: a Language Modeling Approach to Audio Generation}, author = {Zal{\'a}n Borsos and Rapha{\"e}l Marinier and Damien Vincent and Eugene Kharitonov and Olivier Pietquin and Matthew Sharifi and Olivier Teboul and David Grangier and Marco Tagliasacchi and Neil Zeghidour}, year = {2022} }
@misc{https://doi.org/10.48550/arxiv.2107.03312, title = {SoundStream: An End-to-End Neural Audio Codec}, author = {Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco}, publisher = {arXiv}, url = {https://arxiv.org/abs/2107.03312}, year = {2021} }
@misc{shazeer2020glu, title = {GLU Variants Improve Transformer}, author = {Noam Shazeer}, year = {2020}, url = {https://arxiv.org/abs/2002.05202} }
@article{Shazeer2019FastTD, title = {Fast Transformer Decoding: One Write-Head is All You Need}, author = {Noam M. Shazeer}, journal = {ArXiv}, year = {2019}, volume = {abs/1911.02150} }
@article{Ho2022ClassifierFreeDG, title = {Classifier-Free Diffusion Guidance}, author = {Jonathan Ho}, journal = {ArXiv}, year = {2022}, volume = {abs/2207.12598} }
@misc{crowson2022, author = {Katherine Crowson}, url = {https://twitter.com/rivershavewings} }
@misc{ding2021cogview, title = {CogView: Mastering Text-to-Image Generation via Transformers}, author = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang}, year = {2021}, eprint = {2105.13290}, archivePrefix = {arXiv}, primaryClass = {cs.CV} }
@article{Liu2022FCMFC, title = {FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners}, author = {Hao Liu and Xinyang Geng and Lisa Lee and Igor Mordatch and Sergey Levine and Sharan Narang and P. Abbeel}, journal = {ArXiv}, year = {2022}, volume = {abs/2210.13432} }
@inproceedings{anonymous2022normformer, title = {NormFormer: Improved Transformer Pretraining with Extra Normalization}, author = {Anonymous}, booktitle = {Submitted to The Tenth International Conference on Learning Representations }, year = {2022}, url = {https://openreview.net/forum?id=GMYWzWztDx5}, note = {under review} }
@article{Li2021LocalViTBL, title = {LocalViT: Bringing Locality to Vision Transformers}, author = {Yawei Li and K. Zhang and Jie Cao and Radu Timofte and Luc Van Gool}, journal = {ArXiv}, year = {2021}, volume = {abs/2104.05707} }
@misc{liu2021swin, title = {Swin Transformer V2: Scaling Up Capacity and Resolution}, author = {Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo}, year = {2021}, eprint = {2111.09883}, archivePrefix = {arXiv}, primaryClass = {cs.CV} }
@inproceedings{Ma2022MegaMA, title = {Mega: Moving Average Equipped Gated Attention}, author = {Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer}, year = {2022} }
@misc{gilmer2023intriguing title = {Intriguing Properties of Transformer Training Instabilities}, author = {Justin Gilmer, Andrea Schioppa, and Jeremy Cohen}, year = {2023}, status = {to be published - one attention stabilization technique is circulating within Google Brain, being used by multiple teams} }



AudioLM: a Language Modeling Approach to Audio Generation
https://arxiv.org/abs/2209.03143
WebSep 7, 2022 · AudioLM: a Language Modeling Approach to Audio Generation. We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio …
Computer Science > Sound
[Submitted on 7 Sep 2022]
Title:AudioLM: a Language Modeling Approach to Audio GenerationDownload PDF
Abstract: We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.
Submission historyFrom: Neil Zeghidour [view email]
[v1] Wed, 7 Sep 2022 13:40:08 UTC (1,183 KB)
Full-text links:
Download:
(license)
Current browse context:
cs.SD
References & Citations
a Loading...
Bibliographic and Citation Tools
Bibliographic Explorer (What is the Explorer?)
Litmaps (What is Litmaps?)
scite Smart Citations (What are Smart Citations?)
Code, Data and Media Associated with this Article
Papers with Code (What is Papers with Code?)
ScienceCast (What is ScienceCast?)
Demos
Replicate (What is Replicate?)
Hugging Face Spaces (What is Spaces?)
Recommenders and Search Tools
Connected Papers (What is Connected Papers?)
CORE Recommender (What is CORE?)
arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.



AudioLM: a Language Modeling Approach to Audio Generation
https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html?hl=no
WebIn “AudioLM: a Language Modeling Approach to Audio Generation”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming previous …
Generating realistic audio requires modeling information represented at different scales. For example, just as music builds complex musical phrases from individual notes, speech combines temporally local structures, such as phonemes or syllables, into words and sentences. Creating well-structured and coherent audio sequences at all these scales is a challenge that has been addressed by coupling audio with transcriptions that can guide the generative process, be it text transcripts for speech synthesis or MIDI representations for piano. However, this approach breaks when trying to model untranscribed aspects of audio, such as speaker characteristics necessary to help people with speech impairments recover their voice, or stylistic components of a piano performance.
In “AudioLM: a Language Modeling Approach to Audio Generation”, we propose a new framework for audio generation that learns to generate realistic speech and piano music by listening to audio only. Audio generated by AudioLM demonstrates long-term consistency (e.g., syntax in speech, melody in music) and high fidelity, outperforming previous systems and pushing the frontiers of audio generation with applications in speech synthesis or computer-assisted music. Following our AI Principles, we've also developed a model to identify synthetic audio generated by AudioLM.
From Text to Audio Language Models
In recent years, language models trained on very large text corpora have demonstrated their exceptional generative abilities, from open-ended dialogue to machine translation or even common-sense reasoning. They have further shown their capacity to model other signals than texts, such as natural images. The key intuition behind AudioLM is to leverage such advances in language modeling to generate audio without being trained on annotated data.
However, some challenges need to be addressed when moving from text language models to audio language models. First, one must cope with the fact that the data rate for audio is significantly higher, thus leading to much longer sequences — while a written sentence can be represented by a few dozen characters, its audio waveform typically contains hundreds of thousands of values. Second, there is a one-to-many relationship between text and audio. This means that the same sentence can be rendered by different speakers with different speaking styles, emotional content and recording conditions.
To overcome both challenges, AudioLM leverages two kinds of audio tokens. First, semantic tokens are extracted from w2v-BERT, a self-supervised audio model. These tokens capture both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure (e.g., language syntax and semantic content in speech, harmony and rhythm in piano music), while heavily downsampling the audio signal to allow for modeling long sequences.
However, audio reconstructed from these tokens demonstrates poor fidelity. To overcome this limitation, in addition to semantic tokens, we rely on acoustic tokens produced by a SoundStream neural codec, which capture the details of the audio waveform (such as speaker characteristics or recording conditions) and allow for high-quality synthesis. Training a system to generate both semantic and acoustic tokens leads simultaneously to high audio quality and long-term consistency.
Training an Audio-Only Language Model
AudioLM is a pure audio model that is trained without any text or symbolic representation of music. AudioLM models an audio sequence hierarchically, from semantic tokens up to fine acoustic tokens, by chaining several Transformer models, one for each stage. Each stage is trained for the next token prediction based on past tokens, as one would train a text language model. The first stage performs this task on semantic tokens to model the high-level structure of the audio sequence.
In the second stage, we concatenate the entire semantic token sequence, along with the past coarse acoustic tokens, and feed both as conditioning to the coarse acoustic model, which then predicts the future tokens. This step models acoustic properties such as speaker characteristics in speech or timbre in music.
In the third stage, we process the coarse acoustic tokens with the fine acoustic model, which adds even more detail to the final audio. Finally, we feed acoustic tokens to the SoundStream decoder to reconstruct a waveform.
After training, one can condition AudioLM on a few seconds of audio, which enables it to generate consistent continuation. In order to showcase the general applicability of the AudioLM framework, we consider two tasks from different audio domains:
- Speech continuation, where the model is expected to retain the speaker characteristics, prosody and recording conditions of the prompt while producing new content that is syntactically correct and semantically consistent.
- Piano continuation, where the model is expected to generate piano music that is coherent with the prompt in terms of melody, harmony and rhythm.
In the video below, you can listen to examples where the model is asked to continue either speech or music and generate new content that was not seen during training. As you listen, note that everything you hear after the gray vertical line was generated by AudioLM and that the model has never seen any text or musical transcription, but rather just learned from raw audio. We release more samples on this webpage.
To validate our results, we asked human raters to listen to short audio clips and decide whether it is an original recording of human speech or a synthetic continuation generated by AudioLM. Based on the ratings collected, we observed a 51.2% success rate, which is not statistically significantly different from the 50% success rate achieved when assigning labels at random. This means that speech generated by AudioLM is hard to distinguish from real speech for the average listener.
Our work on AudioLM is for research purposes and we have no plans to release it more broadly at this time. In alignment with our AI Principles, we sought to understand and mitigate the possibility that people could misinterpret the short speech samples synthesized by AudioLM as real speech. For this purpose, we trained a classifier that can detect synthetic speech generated by AudioLM with very high accuracy (98.6%). This shows that despite being (almost) indistinguishable to some listeners, continuations generated by AudioLM are very easy to detect with a simple audio classifier. This is a crucial first step to help protect against the potential misuse of AudioLM, with future efforts potentially exploring technologies such as audio “watermarking”.
Conclusion
We introduce AudioLM, a language modeling approach to audio generation that provides both long-term coherence and high audio quality. Experiments on speech generation show not only that AudioLM can generate syntactically and semantically coherent speech without any text, but also that continuations produced by the model are almost indistinguishable from real speech by humans. Moreover, AudioLM goes well beyond speech and can model arbitrary audio signals such as piano music. This encourages the future extensions to other types of audio (e.g., multilingual speech, polyphonic music, and audio events) as well as integrating AudioLM into an encoder-decoder framework for conditioned tasks such as text-to-speech or speech-to-speech translation.
Acknowledgments
The work described here was authored by Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi and Neil Zeghidour. We are grateful for all discussions and feedback on this work that we received from our colleagues at Google.



GitHub - RoganInglis/AudioLM
https://github.com/RoganInglis/AudioLM
WebCheck for existing implementations of soundstream. This repo contains a tflite soundstream model which it might be possible to inspect. This repo contains a vector quantization implementation which might be useful. This repo contains a soundstream implementation which is missing some features from the original paper but will likely still be useful. Have …
Description
A PyTorch implementation of AudioLM. Still in early stages and not at the point of running anything yet.
TODO
- Check for existing implementations of w2v-BERT
- Don't see anything complete but lucidrains is working on an implementation of AudioLM here which might contain some inspiration later
- Check for existing implementations of soundstream
- Have a look at audio-diffusion-pytorch and see if there is anything useful there
- There is some good dataset info here. Particularly YoutubeDataset sounds interesting and potentially useful
- Implement w2v-BERT
- Implement w2v-BERT network
- Check experimental setup in this paper, which matches w2v-BERT
- Implement feature encoder
- Implement contrastive module
- Implement conformer block
- should be able to just use torchaudio.models.Conformer
- Implement conformer block
- Implement masked prediction module
- Implement masked prediction loss
- Implement contrastive loss
- Implement w2v-BERT data module
- Implement w2v-BERT training
- Implement w2v-BERT network
- Implement soundstream
- Implement soundstream network
- Implement soundstream data module
- Implement soundstream training
- Implement AudioLM
- Implement AudioLM network
- Implement AudioLM data module
- Implement AudioLM training
- Train on LibriSpeech (version available in torchaudio)
- Train on music dataset
How to run
Install dependencies
# clone project git clone https://github.com/YourGithubName/your-repo-name cd your-repo-name # [OPTIONAL] create conda environment conda create -n myenv python=3.9 conda activate myenv # install pytorch according to instructions # https://pytorch.org/get-started/ # install requirements pip install -r requirements.txt
Train model with default configuration
# train on CPU python src/train.py trainer=cpu # train on GPU python src/train.py trainer=gpu
Train model with chosen experiment configuration from configs/experiment/
python src/train.py experiment=experiment_name.yaml
You can override any parameter from command line like this
python src/train.py trainer.max_epochs=20 datamodule.batch_size=64



Google AI Announced the First-ever Text-to-Music Generator AudioLM …
https://mpost.io/google-ai-announced-the-first-ever-text-to-music-generator-audiolm/
WebOct 21, 2022 · According to recent Google research, a new framework for audio production called AudioLM may be taught to create realistic speech and piano music simply by listening to sounds. Due to its long-term consistency and excellent fidelity, AudioLM surpasses earlier systems and advances audio creation with applications in voice synthesis and computer …
In Brief
AudioLM can produce music just by listening to sounds
Mubert AI to continue human speech and piano music
The Trust Project is a worldwide group of news organizations working to establish transparency standards.
With GPT-3 and others, the idea of generative AI has a good chance of moving forward. We also discovered the concepts of inpainting and outpainting; AI skillfully completes the images while keeping the theme and the style. What about music?
And yet again! Since all of this is based on AI language models that retain meaning, it was just a matter of time before this technology would be applied to music. And now the time has come.
According to recent Google research, a new framework for audio production called AudioLM may be taught to create realistic speech and piano music simply by listening to sounds. Due to its long-term consistency and excellent fidelity, AudioLM surpasses earlier systems and advances audio creation with applications in voice synthesis and computer-assisted music.
AudioLM from Google AI can extend an acoustic passage while keeping “intent.” As of now, it has been trained to continue human speech and piano music, based on a limited sample of input data. Check the sample below.
The criteria for speech were straightforward: Listeners were asked to assess whether the continuation sounded like human speech. With the music, it was discovered that the “continuation” of the section supplied for input is far superior in quality than all current music generators from scratch, such as JukeBox. With a suggestion at the input, the AI continues the music considerably better.
Human raters listened to audio samples to confirm the results. They determined whether they were hearing a real continuation of a human voice that had been recorded or an artificial voice produced by AudioLM. Their data indicate a 51.2% success rate. As a result, it will be challenging for the average listener to distinguish between speech produced by AudioLM and actual human speech.
Does text-to-music technology alter the music business?
A text-to-music generator based on the Mubert API was recently announced by another AI model, Mubert. Mubert creates a different set of sounds for each request that you send. The likelihood of a repeat is really slim. Music is created when a request is made; it is not pulled from a database of finished tunes. How truly generative this music is is a common question.
Sounds are chosen before being created. Both the input prompt and the Mubert API tags are encoded to a transformer neural network’s latent space vector. The closest tags vector for each query is then chosen, and the accompanying tags are transmitted to our API to create music. No neural network was used to construct any of the sounds (separate loops for bass, leads, etc.); all of the sounds were produced by musicians and sound designers.
Mubert’s next significant step is to take items from the current world, such as photos, movies, scenarios, and presentations, and create the music of the world around you.
Here’s what you can get by recklessly putting text prompts into the mouth of the musical Mubert AI:
This is the initial stage in the process of building a more sophisticated and precise generating algorithm, but this will take time and money.
However, text-to-music technology is already available, so you can generate albums in bulk by switching out “input prompt” for “write a random prompt script.” Seems artists are no longer required.
Read more related news:
Disclaimer
Any data, text, or other content on this page is provided as general market information and not as investment advice. Past performance is not necessarily an indicator of future results.



What is Google's MusicLM and How You Can Generate Music
https://www.tasq.ai/blog/what-is-googles-musiclm/
WebFeb 6, 2023 · Google MusicLM is a language model capable of generating music when given a text description. For example, “a calming guitar melody in 6/8 time signature riff”. MusicLM is similar to other language models but it is totally dedicated to music. It is created by the folks at Google. It is built on top of AudioLM.

Google AudioLM is already capable of making speeches with your …
https://www.lenseup.com/en/google-audio-lm-is-already-capable-of-making-speeches-with-your-voice/
WebOct 7, 2022 · Google AudioLM is already capable of making speeches with your voice Language models, Speech recognition and speech-to-text Computers are already able to play chess games and they became unbeatable opponents; we let them read our texts and they started to write. They also learned to paint and retouch photographs.
Computers are already able to play chess games and they became unbeatable opponents; we let them read our texts and they started to write. They also learned to paint and retouch photographs. Did anyone doubt that artificial intelligence would be able to do the same with speeches and music?
Google’s research division has presented AudioLM, a framework for generating high-quality audio that remains consistent over the long term. To do this, it starts with a recording of just a few seconds in length, and is able to prolong it in a natural and coherent way. What is remarkable is that it achieves this without being trained with previous transcriptions or annotations even though the generated speech is syntactically and semantically correct Moreover, it maintains the identity and prosody of the speaker to such an extent that the listener is unable to discern which part of the audio is original and which has been generated by an artificial intelligence.
The examples of this artificial intelligence are striking. Not only is it able to replicate articulation, pitch, timbre and intensity, but it is able to input the sound of the speaker’s breathing and form meaningful sentences. If it does not start from a studio audio, but from one with background noise, AudioLM replicates it to give it continuity. More samples can be heard on the AudioLM website.
Google Brain
Google Audio LM is an artificial intelligence trained in semantics and acoustics. How does it do it? The generation of audio or music is nothing new. But the way Google researchers have devised to tackle the problem is. From each audio, semantic markers are extracted to encode a high-level structure (phonemes, lexicon, semantics…), and acoustic markers (speaker identity, recording quality, background noise…). With this data already processed and understandable to the artificial intelligence, AudioML begins its work by establishing a hierarchy in which it first predicts the semantic markers, which are then used as conditions for predicting the acoustic markers. The latter are then used again at the end to convert the bits into something humans can hear.
This semantic separation of acoustics, and its hierarchy, is not only a beneficial practice for training language models to generate speech. According to the researchers, it is also more effective for continuing piano compositions, as they show on their website. It is much better than models that are only trained using acoustic markers.
The most significant thing about AudioLM’s artificial intelligence is not that it is able to continue speeches and melodies, but that it can do everything at once. It is therefore a unique language model that can be used to convert text to speech – a robot could read entire books – or to make any device able to communicate with people using a familiar voice. This idea has already been explored by Amazon, which considered using the voice of loved ones in its Alexa speakers.
Innovation or danger?
Software such as Dalle-2 and Stable Diffusion are exceptional tools that allow ideas to be sketched out or creative resources to be generated in a few seconds. Audio can be even more important, and one can imagine the voice of an announcer being used on demand by various companies. Even films could be dubbed with the voices of deceased actors. The reader may be wondering whether this possibility, while exciting, is not dangerous. Any audio recording could be manipulated for political, legal or judicial purposes. Google says that, while humans may have difficulty detecting what comes from humans and what comes from artificial intelligence, a computer can detect whether the audio is organic or not. In other words, not only can the machine replace us, but another machine will be essential to assess its work.
For the moment AudioLM is not open to the public, it is only a language model that can be integrated into different projects. But this demonstration, together with OpenAI’s Jukebox music programme, shows how quickly we are entering a new world where nobody will know, or care, whether that photograph is taken by a person or whether there is a person or an artificially generated voice-over on the other end of the phone in real time.



typical range of `num_train_steps`? · Issue #80 · lucidrains/audiolm ...
https://github.com/lucidrains/audiolm-pytorch/issues/80
WebWas able to replicate @djqualia's result with 0.11.1 :). Here's a rough sketch of the total loss over time (unfortunately, had some weird issues with print statement issues and whatever so this doesn't cover the full training run but you get the general idea. 20k training steps, batch_size=4 grad_accum_every = 8, OpenSLR dev-clean dataset ) BTW, the end result …
Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.
By clicking “Sign up for GitHub”, you agree to our terms of service and
privacy statement. We’ll occasionally send you account related emails.
Already on GitHub?
Sign in
to your account
num_train_steps
Thanks for sharing this great repo!
I'm wondering what is the typical range of num_train_steps for a SoundStream model and others.
I tested with 10000 and saw the loss went down somewhat smoothly, but it did not generate any meaningful results (very noisy).
The text was updated successfully, but these errors were encountered:
what version are you using? what is your batch size? i suspect you'll need to train more than 10k steps, however, there might be a problem with the recent versions...
@lucidrains i'm still testing and was going to wait until i had more data before posting, but i'm worried the complex stft discriminator might need to come back. i've moved onto a larger training set (so not quite the same as before), and i was 2 days into training on 0.10.3 and was still getting mostly noise, so i'm now testing at 0.7.8 (the last version before you removed it) with use_complex_stft_discriminator = True. i'm only half a day in, but the loss curves are looking better (trending down vs. up) and samples seem less noise (but more time is needed to tell there). i'll report back in time...
Sorry, something went wrong.
what version are you using? what is your batch size? i suspect you'll need to train more than 10k steps, however, there might be a problem with the recent versions...
@lucidrains i'm still testing and was going to wait until i had more data before posting, but i'm worried the complex stft discriminator might need to come back. i've moved onto a larger training set (so not quite the same as before), and i was 2 days into training on 0.10.3 and was still getting mostly noise, so i'm now testing at 0.7.8 (the last version before you removed it) with use_complex_stft_discriminator = True. i'm only half a day in, but the loss curves are looking better (trending down vs. up) and samples seem less noise (but more time is needed to tell there). i'll report back in time...
what version are you using? what is your batch size? i suspect you'll need to train more than 10k steps, however, there might be a problem with the recent versions...
@lucidrains i'm still testing and was going to wait until i had more data before posting, but i'm worried the complex stft discriminator might need to come back. i've moved onto a larger training set (so not quite the same as before), and i was 2 days into training on 0.10.3 and was still getting mostly noise, so i'm now testing at 0.7.8 (the last version before you removed it) with use_complex_stft_discriminator = True. i'm only half a day in, but the loss curves are looking better (trending down vs. up) and samples seem less noise (but more time is needed to tell there). i'll report back in time...
oh bummer, i really need help on this one 😢
ok, i've reverted it for now
@djqualia @lucidrains thanks for your comments! I tried with batch_size = 4 and 8. I'll try again with the reverted version and see how it goes!
batch_size = 4
8
Update, I tested with 0.11.1 and can confirm the samples are MUCH better (I can already hear the source in some samples, <1 day in to training). The total loss was much higher than previous versions, but is generally trending down.
@djqualia that's great! thank you! btw I like your name 🤣
@djqualia definitely has the best name
Was able to replicate @djqualia's result with 0.11.1 :)
Here's a rough sketch of the total loss over time (unfortunately, had some weird issues with print statement issues and whatever so this doesn't cover the full training run but you get the general idea. 20k training steps, batch_size=4 grad_accum_every = 8, OpenSLR dev-clean dataset )
batch_size=4
grad_accum_every = 8
BTW, the end result generally contains things that I recognize as sounding like human speech but if you ran it on 0.25x (sort of like a robot):
(I hear "rarely succeed")
Is this to be expected and hopefully would get better with more training, or is it more likely I did something wrong with running SoundStream e2e?
my first thought https://www.youtube.com/watch?v=uc6f_2nPSX8 🤖🎵
I think the "robot sound" is probably expected this early on in training. It's akin to bit-reduction effects in music production (as soundstream builds up the RVQ network). My early samples trained on music have a similar effect, which has gone away with more training. Example:
I'm having trouble with understanding what I'm doing wrong... I left the data_max_length to 320 * 32... I don't understand with these numbers mean after looking through the code base. The data I'm using is all 30s clips, but for some reason I'm only getting less than a second for the samples that gets spit out at the end of each save results... Is there documentation anywhere on what this data_max_length means as I feel it's my problem.
Edit:
I think this should maybe be added to the Readme somewhere. I guess this data_max_length corresponds to the size torchaudio gives you.... so 30 seconds of 24000 hz would be data_max_length 24000 * 30. This seems right?
@djqualia I trained for about 60k steps with batch size 4 and the robotic sound did not seem to go away or get better after 20k steps. However, the results were pretty good after a day for me too (~15k steps).
I started a new run with version 12.1 and will keep you posted on the results. :)
Sounds like some decent progress thus far 👍
@djqualia - may I ask what dataset you trained upon to get that ten-second clip you posted above? And, in addition, how many steps had you trained over when that clip was emitted, and what 'data_max_length' value did you use when training upon that dataset?
@aTylerRice Did you ever reach a conclusion on your question above Re: data_max_length? I'm also training on 30s clips, and my save results are also <1s. Thanks.
re: data_max_length, yes you will need to set this to your desired sample length based on your sample_rate. the default value is low i suspect for testing and also because most GPUs can't handle a large value
for my sample above, i'm being a bit ambitious and attempting to train a general music soundstream model at 44100 sample rate. to achieve this, i've adjusted the stride factor from 320 -> 882 (using 3,6,7,7), based on my limited understanding of the research papers. with my gpu, i can only support a puny batch size of 2. data_max_length is 10 x 44100 for 10s. this is with 12 RVQ layers and codebook 1024 (as musiclm did, but they used 24000 sample rate)
i'm now 2 days into training on 0.12.1 (at step 41500), and there's still robot/bit-reduction effect on most samples (some more than others), but it's getting better. TBD on whether this goes away with time (in my past data set, it was only bass-instrument sounds, which perhaps enabled it to converge faster?)
initially, my data set was my very large collection of music+samples (i have DJed in the past and still produce music). i've been augmenting it with every free music database i can find (fma, jamendo, audioset). i've probably spent more time writing code to gather/organize datasets than anything else so far :-)
p.s. i would recommend reading and trying to understand the associated research papers (soundstream, audiolm, hubert, musiclm). this all state of the art so it's not an easy thing to get working (yet)
@djqualia I still had these robotic sounds after training on 15s clips on about 80k samples from fma dataset.. I'm guessing google either had many more samples to train on and maybe the model is even slightly different. It's a sad state that research is so hard to reproduce. I'm going to try training with a sample size of 10s next but using all the audio from fma large so essentially 300k examples.. This is going to wait though as my machine learning computer is currently in pieces due to an upgrade
No branches or pull requests





