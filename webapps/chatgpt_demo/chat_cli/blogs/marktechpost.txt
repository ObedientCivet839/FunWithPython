Title: https://www.marktechpost.com/2022/11/30/this-artificial-intelligence-paper-presents-an-advanced-method-for-differential-privacy-in-image-recognition-with-better-accuracy/
Link: https://www.marktechpost.com/2022/11/30/this-artificial-intelligence-paper-presents-an-advanced-method-for-differential-privacy-in-image-recognition-with-better-accuracy/
Description: 
Full text:
Machine learning has increased considerably in several areas due to its performance in recent years. Thanks to modern computers’ computing capacity and graphics cards, deep learning has made it possible to achieve results that sometimes exceed those experts give. However, its use in sensitive areas such as medicine or finance causes confidentiality issues. A formal privacy guarantee called differential privacy (DP) prohibits adversaries with access to machine learning models from obtaining data on specific training points. The most common training approach for differential privacy in image recognition is differential private stochastic gradient descent (DPSGD). However, the deployment of differential privacy is limited by the performance deterioration caused by current DPSGD systems.
The existing methods for differentially private deep learning still need to operate better since that, in the stochastic gradient descent process, these techniques allow all model updates regardless of whether the corresponding objective function values get better. In some model updates, adding noise to the gradients might worsen the objective function values, especially when convergence is imminent. The resulting models get worse as a result of these effects. The optimization target degrades, and the privacy budget is wasted. To address this problem, a research team from Shanghai University in China suggests a simulated annealing-based differentially private stochastic gradient descent (SA-DPSGD) approach that accepts a candidate update with a probability that depends on the quality of the update and the number of iterations.
Concretely, the model update is accepted if it gives a better objective function value. Otherwise, the update is rejected with a certain probability. To prevent settling into a local optimum, the authors suggest using probabilistic rejections rather than deterministic ones and limiting the number of continuous rejections. Therefore, the simulated annealing algorithm is used to select model updates with probability during the stochastic gradient descent process.
The following gives a high-level explanation of the proposed approach.
1- DPSGD generates the updates iteratively, and the objective function value is computed following that. The energy shift from the previous iteration to the current one and the overall number of approved solutions are then used to calculate the acceptance probability of the current solution.
2- The acceptance probability is always kept to 1, when the energy change is negative. That means updates that step in the right direction are accepted. It is nevertheless guaranteed that the training moves mostly in the direction of convergence even while the model updates are noisy, meaning that the actual energy may be positive with a very small probability.
3- When the energy change is positive, the acceptance probability falls exponentially as the number of approved solutions rises. In this situation, accepting a solution would make the energy worse. Deterministic rejections, however, can lead to the ultimate solution falling inside a local optimum. Therefore, the authors proposed to accept updates of positive energy changes with a small, decreasing probability.
4- If there have been too many consecutive rejections, an update will still be allowed since the number of continuous rejections is limited. The acceptance probability may drop so low that it almost rejects all solutions with positive energy changes as the training approaches convergence, and it may even reach a local maximum. Limiting the number of rejections prevents this issue by accepting a solution when it is essential.
To evaluate the performance of the proposed method, SA-DPSGD is evaluated on three datasets: MNIST, FashionMNIST, and CIFAR10. Experiments demonstrated that SA-DPSGD significantly outperforms the state-of-the-art schemes, DPSGD, DPSGD(tanh), and DPSGD(AUTO-S), regarding privacy cost or test accuracy.
According to the authors, SA-DPSGD significantly bridges the classification accuracy gap between private and non-private images. Using the random update screening, the differentially private gradient descent proceeds in the right direction in each iteration, making the obtained result more accurate. In the experiments under the same hyperparameters, SA-DPSGD achieves high accuracies on datasets MNIST, FashionMNIST, and CI-FAR10, compared to the state-of-the-art result. Under the freely adjusted hyperparameters, the proposed approach achieves even higher accuracies.
Check out the Paper. All Credit For This Research Goes To Researchers on This Project. Also, don’t forget to join our Reddit page and discord channel, where we share the latest AI research news, cool AI projects, and more.
Mahmoud is a PhD researcher in machine learning. He also holds a
bachelor's degree in physical science and a master's degree in
telecommunications and networking systems. His current areas of
research concern computer vision, stock market prediction and deep
learning. He produced several scientific articles about person re-
identification and the study of the robustness and stability of deep
networks.


Title: https://www.marktechpost.com/2022/12/11/iom-releases-its-second-synthetic-dataset-from-trafficking-victim-case-records-generated-with-differential-privacy-and-ai-from-microsoft/
Link: https://www.marktechpost.com/2022/12/11/iom-releases-its-second-synthetic-dataset-from-trafficking-victim-case-records-generated-with-differential-privacy-and-ai-from-microsoft/
Description: 
Full text:
Researchers at Microsoft are committed to researching ways technology may help the world’s most marginalized peoples improve their human rights situations. Their expertise spans human-computer interaction, data science, and the social sciences. The research team collaborates with community, governmental, and nongovernmental groups to develop available technologies that allow scalable answers to such issues.
International Organization for Migration (IOM) is a United Nations agency that helps migrants and survivors of human trafficking. By offering assistance to governments and migrants in its 175 member nations, IOM strives to promote humanitarian and orderly migration.
IOM has released its second synthetic dataset, derived from case records of victims of trafficking, using software built by Microsoft researchers. This dataset is the first public dataset to depict victim-perpetrator interactions. To further facilitate data sharing and rigorous research while respecting privacy and civil liberties, the synthetic dataset is the first to be developed with differential privacy, offering an extra security assurance for repeated data releases. The new data release results from years of cooperation between Microsoft and the IOM. It promotes the secure sharing of victim case information in ways that may influence collaborative action within the anti-trafficking community. The CTDC data hub (Counter-Trafficking Data Collaborative) is the first worldwide gateway for human trafficking case data. Its creation was motivated by a shared commitment to improving that hub’s security and usefulness. Since then, IOM and Microsoft have worked together to enhance the use of information on victims and survivors, including their descriptions of traffickers, in the fight against human trafficking.
This work has resulted in a new user interface offered as a public utility web application, allowing users to aggregate and synthesize private data without sending any of it outside the user’s local web browser.
Importance of data privacy while working with vulnerable populations
All precautions must be taken to prevent traffickers from identifying victims of trafficking in published databases. People’s personal information must be kept confidential to avoid further traumatization or social exclusion. The over- or under-reporting of a certain trend in victim instances by a privacy approach might mislead decision-makers into improperly allocating limited resources, preventing them from solving the underlying problem.
IOM and Microsoft’s collaboration was founded on rather than redacting sensitive data to achieve privacy. It could be possible to produce synthetic datasets that properly capture the structure and statistics of underlying sensitive information while staying private by design. In light of this guiding principle and the necessity of providing case count breakdowns by various attribute combinations (e.g., age range, gender, nationality), a method was developed whereby synthetic data matching all short combinations of case attributes would be released alongside privacy-preserving counts of cases. Therefore, the compiled information is useful for assessing the quality of synthetic data and recovering precise numbers for official reporting.
Datasets aggregated in this way maintain the same level of privacy since differentially private data has the feature that additional processing cannot exacerbate privacy loss which allowed the team to adapt their preexisting method of data synthesis, which involves synthesizing records by sampling sets of qualities until all attributes were covered, to extrapolate these noisy reported attribute combinations into complete, differentially-private synthetic records. This yields accurate aggregate data for official reporting, synthetic data for engaging exploration and machine learning, and differential privacy assurances that provide protection even over multiple overlapping data releases, all of which are essential for IOM and similar organizations to establish a strong data ecosystem against human trafficking and other human rights violations.
Stakeholders may improve their understanding of susceptibility risk factors and implement efficient counter-trafficking actions when they have access to precise yet anonymous patterns of attributes describing victim-perpetrator connections.
What’s next?
To make the solution available to other businesses and government entities, Microsoft and IOM have made it open to the public. It may be used by any interested party to collect and share personal information safely.
Together with the UN Office on Drugs and Crime (UNODC), IOM has been developing guidelines and recommendations to assist countries in generating high-quality administrative data. They have also been working with the International Labor Organization (ILO) of the United Nations to compile a bibliography of studies focusing on the effects of trafficking on public policy. To encourage governments and frontline anti-trafficking organizations to share data securely, IOM is developing an online course that will include a session with instructions on synthetic data.
Check out the Reference Article. All Credit For This Research Goes To Researchers on This Project. Also, don’t forget to join our Reddit page and discord channel, where we share the latest AI research news, cool AI projects, and more.
Dhanshree Shenwai is a Computer Science Engineer and has a good experience in FinTech companies covering Financial, Cards & Payments and Banking domain with keen interest in applications of AI. She is enthusiastic about exploring new technologies and advancements in today’s evolving world making everyone's life easy.


Title: https://www.marktechpost.com/2022/12/13/in-a-new-ai-research-federated-learning-enables-big-data-for-rare-cancer-boundary-detection/
Link: https://www.marktechpost.com/2022/12/13/in-a-new-ai-research-federated-learning-enables-big-data-for-rare-cancer-boundary-detection/
Description: 
Full text:
The number of primary observations produced by healthcare systems has dramatically increased due to recent technological developments and a shift in patient culture from reactive to proactive. Clinical professionals may become burned out since such observations need careful evaluation. There have been several attempts to develop, assess, and ultimately translate machine learning (ML) technologies into clinical settings to address this issue and lessen the load on clinical professionals by identifying pertinent links among these observations. In particular, deep learning (DL) has made strides in ML and has shown promise in tackling these challenging healthcare issues.
According to the literature, robust and accurate models must be trained on huge quantities of data, the variety of which impacts how well the model generalizes to “out-of-sample” situations. However, there are issues with their generalizability on “out-of-sample” data or data from sources that did not take part in model training. To overcome these issues, models must be trained on data from different sites representing various demographic samples. “Centralized learning” (CL), in which data from several locations are exchanged in a single place after inter-site agreements, is the current paradigm for such multi-site cooperation.
Due to privacy, data ownership, intellectual property, technological difficulties (such as network and storage restrictions), and compliance with various governmental laws, data centralization is difficult to scale (and may not even be practicable), particularly at a worldwide level. When opposed to models trained using the centralized paradigm, “federated learning” (FL) refers to a paradigm where models are taught by simply exchanging model parameter updates from decentralized data (i.e., each site stores its data locally) (CL).
Thus, FL can provide an alternative to CL, possibly leading to a paradigm change that reduces the requirement for data sharing, increases access to geographically dispersed collaborators, and subsequently expands the volume and variety of data used to train ML models. Health inequities and underserved communities are some of the issues that FL can help with by allowing ML models to learn from a wealth of data that would otherwise be unavailable. In light of this, they concentrate on the “rare” disease of glioblastoma in this article, emphasizing how multi-parametric magnetic resonance imaging (mpMRI) scans may be used to determine the extent of the disease.
Although glioblastoma is the most prevalent malignant primary brain tumor, its incidence rate (i.e., 3/100,000 individuals) is far lower than the rate required to meet the criteria of a rare illness (i.e., 10/ 100,000 people). Hence it is still categorized as a “rare” disease. Collaboration between geographically disparate sites is required because a single site cannot amass big and varied datasets to train reliable and generalizable ML models. The median overall survival of glioblastoma patients following standard-of-care treatment is only 14.6 months, and their median survival without treatment is only four months, despite significant attempts to improve the prognosis of these patients with rigorous multimodal therapy. Despite advancements in glioblastoma subtyping and the expansion of standard-of-care treatment choices over the past 20 years, overall survival has not significantly increased.
This reflects the necessity for analysis of bigger and more diverse data to understand better the illness and the main challenge in treating these tumors, which is their inherent heterogeneity. Glioblastomas have three main sub-compartments in terms of their radiologic appearance:
- The “enhancing tumor” (ET) represents the breakdown of the blood-brain barrier within the tumor.
- The “tumor core” (TC), which combines the ET and the necrotic (NCR) part and represents the surgically relevant part of the tumor
- The “whole tumor” (WT).
To better quantify and evaluate these various uncommon diseases and eventually have an impact on clinical decision-making, it is crucial to identify these sub-compartment borders. The results of these investigations confirmed the advantages of the FL process, which was based on an aggregate server and had a performance nearly equal to CL for this use case. This definition of the task as a multi-parametric multi-class learning problem is vital.
As opposed to merely transcribing a categorical entry from medical records, this study dealt with a multi-parametric multi-class challenge with reference standards that demand professional doctors to follow a careful manual annotation methodology. Additionally, due to differences in scanner technology and acquisition techniques, consistent preprocessing pipelines were created at each participating location to manage the different aspects of the mpMRI data. These elements, together with the study’s extensive global scope and job difficulty, set it apart.
The main scientific contributions of this manuscript are I demonstrating the effectiveness of FL at such scale and task complexity as a paradigm-shifting approach; (ii) making a potential impact for the treatment of the rare disease of glioblastoma by publicly releasing clinically deployable trained consensus models; and, most importantly, (iii) paving the way for more successful FL studies of increased scale and task complexity. Data and code are available on GitHub.
Check out the Paper and Github. All Credit For This Research Goes To Researchers on This Project. Also, don’t forget to join our Reddit page and discord channel, where we share the latest AI research news, cool AI projects, and more.
Aneesh Tickoo is a consulting intern at MarktechPost. He is currently pursuing his undergraduate degree in Data Science and Artificial Intelligence from the Indian Institute of Technology(IIT), Bhilai. He spends most of his time working on projects aimed at harnessing the power of machine learning. His research interest is image processing and is passionate about building solutions around it. He loves to connect with people and collaborate on interesting projects.


Title: https://www.marktechpost.com/2023/01/31/vision-transformers-have-taken-the-field-of-computer-vision-by-storm-ut-what-do-vision-transformers-learn/
Link: https://www.marktechpost.com/2023/01/31/vision-transformers-have-taken-the-field-of-computer-vision-by-storm-ut-what-do-vision-transformers-learn/
Description: 
Full text:
Vision transformers (ViTs) are a type of neural network architecture that has reached tremendous popularity for vision tasks such as image classification, semantic segmentation, and object detection. The main difference between the vision and original transformers was the replacement of the discrete tokens of text with continuous pixel values extracted from image patches. ViTs extracts features from the image by attending to different regions of it and combining them to make a prediction. However, despite the recent widespread use, little is known about the inductive biases or features that ViTs tend to learn. While feature visualizations and image reconstructions have been successful in understanding the workings of convolutional neural networks (CNNs), these methods have not been as successful in understanding ViTs, which are difficult to visualize.
The latest work from a group of researchers from the University of Maryland-College Park and New York University enlarges the ViTs literature with an in-depth study concerning their behavior and their inner-processing mechanisms. The authors established a visualization framework to synthesize images that maximally activate neurons in the ViT model. In particular, the method involved taking gradient steps to maximize feature activations by starting from random noise and applying various regularization techniques, such as penalizing total variation and using augmentation ensembling, to improve the quality of the generated images.
The analysis found that patch tokens in ViTs preserve spatial information throughout all layers except the last attention block, which learns a token-mixing operation similar to the average pooling operation widely used in CNNs. The authors observed that the representations remain local, even for individual channels in deep layers of the network.
To this end, the CLS token seems to play a relatively minor role throughout the network and is not used for globalization until the last layer. The authors demonstrated this hypothesis by performing inference on images without using the CLS token in layers 1-11 and then inserting a value for the CLS token at layer 12. The resulting ViT could still successfully classify 78.61% of the ImageNet validation set instead of the original 84.20%.
Hence, both CNNs and ViTs exhibit a progressive specialization of features, where early layers recognize basic image features such as color and edges, while deeper layers recognize more complex structures. However, an important difference found by the authors concerns the reliance of ViTs and CNNs on background and foreground image features. The study observed that ViTs are significantly better than CNNs at using the background information in an image to identify the correct class and suffer less from the removal of the background. Additionally, ViT predictions are more resilient to the removal of high-frequency texture information compared to ResNet models (results visible in Table 2 of the paper).
Finally, the study also briefly analyzes the representations learned by ViT models trained in the Contrastive Language Image Pretraining (CLIP) framework which connects images and text. Interestingly, they found that CLIP-trained ViTs produce features in deeper layers activated by objects in clearly discernible conceptual categories, unlike ViTs trained as classifiers. This is reasonable yet surprising because text available on the internet provides targets for abstract and semantic concepts like “morbidity” (examples are visible in Figure 11).
Check out the Paper and Github. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 13k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Lorenzo Brigato is a Postdoctoral Researcher at the ARTORG center, a research institution affiliated with the University of Bern, and is currently involved in the application of AI to health and nutrition. He holds a Ph.D. degree in Computer Science from the Sapienza University of Rome, Italy. His Ph.D. thesis focused on image classification problems with sample- and label-deficient data distributions.


Title: https://www.marktechpost.com/2023/02/04/a-new-ai-research-from-mit-reduces-variance-in-denoising-score-matching-improving-image-quality-stability-and-training-speed-in-diffusion-models/
Link: https://www.marktechpost.com/2023/02/04/a-new-ai-research-from-mit-reduces-variance-in-denoising-score-matching-improving-image-quality-stability-and-training-speed-in-diffusion-models/
Description: 
Full text:
Diffusion models have recently produced outstanding results on various generating tasks, including the creation of images, 3D point clouds, and molecular conformers. Ito stochastic differential equations (SDE) are a unified framework that can incorporate these models. The models acquire knowledge of time-dependent score fields through score-matching, which later directs the reverse SDE during generative sampling. Variance-exploding (VE) and variance-preserving (VP) SDE are common diffusion models. EDM offers the finest performance to date by expanding on these compositions. The existing training method for diffusion models can still be enhanced, despite achieving outstanding empirical results.
The Stable Target Field (STF) objective is a generalized variation of the denoising score-matching objective. Particularly, the high volatility of the denoising score matching (DSM) objective’s training targets can result in subpar performance. They divide the score field into three regimes to comprehend the cause of this volatility better. According to their investigation, the phenomenon mostly occurs in the intermediate regime, defined by various modes or data points having a similar impact on the scores. In other words, under this regime, it is still being determined where the noisy samples produced throughout the forward process originated. Figure 1(a) illustrates the differences between the DSM and their proposed STF objectives.
Figure 1: Examples of the DSM objective’s and our suggested STF objective’s contrasts.
While their sources (in red box) are separated from one another, the “destroyed” photos (in blue box) are close together. Despite the fact that the true score in expectation is the weighted average of vi, the DSM objective’s individual training updates have a high variation, which our STF objective considerably lowers by using a sizable reference batch (yellow box)
The plan is to add a second reference batch of examples to be utilized as targets when calculating weighted conditional scores. They aggregate the contribution of each example in the reference batch using self-normalized importance sampling. Although this method, particularly in the intermediate regime, can significantly reduce the variation of training objectives (Figure 1(b)), it does introduce some bias. However, they demonstrate that as the size of the reference batch increases, the bias and trace-of-covariance of the STF training targets decrease to zero. Through experiments, they show how their STF objective, when added into EDM, yields new state-of-the-art performance on CIFAR10 unconditional generation. The final FID score after 35 network evaluations is 1.90.
In most instances, STF also raises the FID/Inception scores for other score-based model variations, such as VE and VP SDEs. Additionally, it enhances the stability of convergent score-based models on CIFAR-10 and CelebA 642 across random seeds and aids in preventing the development of noisy pictures in VE. STF quickens the training of score-based models while achieving the same or higher FID scores (3.6 speed-up for VE on CIFAR-10). As far as they know, STF is the first method for accelerating the training of diffusion models. They also illustrate the detrimental impact of excessive variance while demonstrating the performance benefit with increasing reference batch size.
The following is a summary of their contributions:
(1) They characterize the part of the forward process known as the intermediate phase, where the score-learning targets are most changeable
(2) They propose a generalized score-matching goal-stable target field to provide more consistent training targets
(3) They examine the behavior of the new objective and demonstrate that it is asymptotically unbiased and reduces the trace-of-covariance of the training targets in the intermediate phase under benign conditions by a factor related to the reference batch size
(4) They use empirical evidence to support the theoretical arguments and demonstrate how the proposed STF objective enhances score-based approaches’ functionality, stability, and training efficiency.
In particular, when paired with EDM, it gets the most recent state-of-the-art FID score on the CIFAR-10 benchmark.
Check out the Paper and GitHub. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 13k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Aneesh Tickoo is a consulting intern at MarktechPost. He is currently pursuing his undergraduate degree in Data Science and Artificial Intelligence from the Indian Institute of Technology(IIT), Bhilai. He spends most of his time working on projects aimed at harnessing the power of machine learning. His research interest is image processing and is passionate about building solutions around it. He loves to connect with people and collaborate on interesting projects.


Title: https://www.marktechpost.com/2023/02/04/a-new-artificial-intelligence-ai-benchmark-called-deepprivacy2-provides-realistic-anonymization-of-human-faces-and-whole-body/
Link: https://www.marktechpost.com/2023/02/04/a-new-artificial-intelligence-ai-benchmark-called-deepprivacy2-provides-realistic-anonymization-of-human-faces-and-whole-body/
Description: 
Full text:
Many applications require collecting personally identifiable information, making image collection and storage commonplace. Recently enacted legislation in many jurisdictions makes it difficult to acquire such data without anonymization or individual authorization.
Blurring images is a common method of traditional image anonymization. But it badly distorts the data, rendering it useless for other purposes. Generative models can now generate realistic faces suitable for a specific situation, which has led to the introduction of realistic anonymization. Although present approaches aim to hide a person’s identity, they only succeed in making their faces unrecognizable to primary and secondary identifiers.
Using dense pixel-to-surface correspondences derived from Continuous Surface Embeddings (CSE), Surface Guided GANs (SG-GAN) offer a full-body anonymization GAN. However, this approach is prone to visual aberrations that degrade image quality. According to researchers, the dataset is a modification of COCO comprising 40K human figures, which is the reason behind the poor visual quality. The CSE segmentation used for anonymization also does not account for hair or other body accessories; thus, the anonymized person frequently “wears” them nevertheless. Furthermore, SG-GAN fails to anonymize many people since the CSE detector typically misses people who are off-camera.
A new study by the Norwegian University of Science and Technology extends upon Surface Guided GANs to deal with the low visual quality and insufficient anonymization caused by inadequate segmentation. They introduce the Flickr Diverse Humans (FDH) dataset, a subset of the YFCC100M dataset, containing 1.5M photos of human beings in various settings. They demonstrate that the higher visual quality of created human figures directly results from the larger dataset. As a second step, they offer a unique anonymization framework that uses a combination of detections across modalities to boost human figure segmentation and detection.
The researchers have used separate anonymizers in their framework for:
- Human figures detected by dense pose estimation
- Human figures that CSE does not detect
- All other faces
The proposed approach uses a basic inpainting GAN for each class, trained using conventional methods for GANs. The study’s results show that the proposed GAN can produce high-quality, diversified identities with minimal modeling adjustments tailored to the job. They applied their GAN for face anonymization on a revised Flickr Diverse Faces (FDF) dataset. Because the GAN doesn’t rely on position guidance, it can anonymize people even when pose information is hard to detect, significantly improving over earlier face anonymization methods.
The team also demonstrates that the style-based generator can use techniques from unconditional GANs to locate globally semantically relevant directions in the GAN latent space. Therefore, the suggested anonymization pipeline can now accept edits to attributes based on textual guidance.
DeepPrivacy2 outperforms all prior state-of-the-art realistic anonymization approaches in terms of image quality and anonymization assurances. The accuracy of the DeepPrivacy2 synthesis has been verified by using both qualitative and quantitative analysis. Since there is no accepted benchmark against anonymization methods, the team compares their results to the widely used face anonymization method DeepPrivacy and those of Surface Guided GANs for whole-body anonymization (SG-GANs). The FDH dataset is used for training the whole-body anonymization generator, while the FDF256 dataset is used for training the face anonymization generator; the FDF256 dataset is an updated version of the FDF. In addition, they also incorporate evaluation data from Market1501, Cityscapes, and COCO.
For a wide range of scenes, poses, and overlaps, the results show that DeepPrivacy2 produces high-quality figures. The Unconditional Full-Body Generator, which does not employ CSE, reveals that it is also necessary for high-quality anonymization with its somewhat unnatural arms and legs.
The team hopes that their open-source framework will serve as a valuable resource for organizations and individuals in need of anonymization while maintaining image quality, particularly those working in the field of computer vision.
Check out the Paper and GitHub. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 13k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Tanushree Shenwai is a consulting intern at MarktechPost. She is currently pursuing her B.Tech from the Indian Institute of Technology(IIT), Bhubaneswar. She is a Data Science enthusiast and has a keen interest in the scope of application of artificial intelligence in various fields. She is passionate about exploring the new advancements in technologies and their real-life application.


Title: https://www.marktechpost.com/2023/02/07/a-recent-ai-research-proposes-ide-3d-an-interactive-disentangled-editing-framework-for-high-resolution-3d-aware-portrait-synthesis/
Link: https://www.marktechpost.com/2023/02/07/a-recent-ai-research-proposes-ide-3d-an-interactive-disentangled-editing-framework-for-high-resolution-3d-aware-portrait-synthesis/
Description: 
Full text:
Portrait synthesis has become a rapidly growing field of computer graphics in recent years. If you are wondering what portrait synthesis means, it is an Artificial Intelligence (AI) task involving an image generator. This generator is trained to produce photorealistic facial images that can be manipulated in several ways, such as haircut, clothing, poses, and pupil color. With the advancements in deep learning and computer vision, it is now possible to generate photorealistic 3D faces that can be used in various applications such as virtual reality, video games, and movies. Despite these advancements, existing methods still face challenges in balancing the trade-off between the quality and editability of the generated portraits. Some methods produce low-resolution but editable faces, while others generate high-quality but uneditable faces.
Existing methods using StyleGAN aim to provide editing capabilities by either learning attribute-specific directions in the latent space or by incorporating various priors to create a more controlled and separated latent space. These techniques are successful in generating 2D images, but they struggle to maintain consistency in different views when applied to 3D face editing.
Other methods focus on neural representations to construct 3D-aware Generative Adversarial Networks (GANs). Initially, NeRF-based generators were developed to generate portraits with consistency across different views by utilizing volumetric representation. However, this approach is memory-inefficient and has limitations in the resolution and authenticity of the synthesized images. The 3D-aware generative model presented in this article has been developed to overcome these issues.
The framework is termed IDE-3D and comprises a multi-head StyleGAN2 feature generator, a neural volume renderer, and a 2D CNN-based up-sampler. An overview of the architecture is presented below.
The shape and texture codes are independently fed to both shallow and deep layers of the StyleGAN feature generator to separate different facial attributes. The resulting features are used to construct 3D volumes of shape and texture, which are encoded in facial semantics and represented in an efficient tri-plane representation. These volumes can then be rendered into photorealistic, view-consistent portraits with free-view capability through the volume renderer and the 2D CNN-based up-sampler.
The authors propose a hybrid GAN inversion approach for face editing applications, which involves mapping the input image and semantic mask to the latent space and editing the encoded face. The method uses a combination of optimization-based GAN inversion and texture and semantic encoders to obtain latent codes, which are used for high-fidelity reconstruction. However, the latent output code of the encoders cannot accurately reconstruct the input images and semantic masks. To address this limitation, the authors introduce a “canonical editor” that normalizes the input image to a standard view and maps it into the latent space for real-time editing without sacrificing faithfulness.
According to the authors, the proposed approach results in a locally disentangled, semantics-aware 3D face generator, which supports interactive 3D face synthesis and editing with state-of-the-art performance (in photorealism and efficiency). The figure below offers a comparison between the proposed framework and state-of-the-art approaches.
This was the summary of IDE-3D, a novel and efficient framework for photorealistic and high-resolution 3D portrait synthesis.
If you are interested or want to learn more about this framework, you can find a link to the paper and the project page.
Check out the Paper, Code, and Project Page. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 13k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Daniele Lorenzi received his M.Sc. in ICT for Internet and Multimedia Engineering in 2021 from the University of Padua, Italy. He is a Ph.D. candidate at the Institute of Information Technology (ITEC) at the Alpen-Adria-Universität (AAU) Klagenfurt. He is currently working in the Christian Doppler Laboratory ATHENA and his research interests include adaptive video streaming, immersive media, machine learning, and QoS/QoE evaluation.


Title: https://www.marktechpost.com/2023/02/07/meet-steps-a-new-computer-vision-method-that-jointly-learns-a-nighttime-image-enhancer-and-a-depth-estimator-without-using-ground-truth/
Link: https://www.marktechpost.com/2023/02/07/meet-steps-a-new-computer-vision-method-that-jointly-learns-a-nighttime-image-enhancer-and-a-depth-estimator-without-using-ground-truth/
Description: 
Full text:
In recent times, researchers have gained considerable interest in self-supervised depth estimation techniques because of their low hardware cost and ability to promote the 3D sensing capabilities of self-driving vehicles. By employing the underlying geometry in image sequences as supervision, self-supervised learning for depth estimation produces encouraging results. Their performance on several datasets, including KITTI, Cityscapes, etc., is equivalent to that of other supervised learning approaches, which supports their outstanding performance.
However, existing research on image-based depth estimation mostly focuses on daylight image sequences where the photometric consistency assumption typically holds, and the inputs are well-lit. But at night, things are different. Researchers frequently use a variety of nighttime picture enhancement techniques to address this issue of photometric consistency and boost the quality of input photographs. However, because existing paired day/night datasets concentrate on indoor settings, supervised nighttime image enhancers are frequently constrained by dataset bias. On the other hand, creating these kinds of paired datasets for dynamic road scenarios becomes quite difficult when it comes to the use case of self-driving vehicles.
To address this issue, researchers from Tsinghua University’s Institute for AI Industry Research (AIR) and the Chinese Academy of Sciences introduced STEPS (Joint Self-supervised Nighttime Image Enhancement and Depth Estimation). This first-of-its-kind framework jointly learns a nighttime image enhancer and a single-view depth estimator without relying on ground truth for either task. This technique uses a recently developed pixel masking scheme to tightly entangle two self-supervised tasks. On public benchmarks, the strategy greatly outperforms current state-of-the-art methods.
The pixel masking strategy developed by the team, which is the primary foundation behind their framework, is based on the team’s discovery while developing the framework that nighttime images suffer not only from underexposed regions but also from overexposed regions (also referred to as unexpected regions). Underexposed and overexposed areas result in the loss of fine-grained information and hinder the model’s ability to calculate precise depth using local contextual cues. In order to identify such unexpected regions, the researchers used the illumination component in the self-supervised nighttime image enhancer. They also proposed a bridge-shape model for soft auto-masking wherein both regions are suppressed naturally by fitting a bridge-shaped curve to the illumination map distribution.
In order to address the issue of the sparse ground truth of existing datasets, the researchers first turned to CARLA (the simulator for autonomous driving research), intending to translate the knowledge learned in the simulation environment to the actual world. However, it is difficult to use the simulated data directly due to the significant domain gap between the simulated and real-world images. As a result, the researchers suggested CARLA-EPE, a new photo-realistically improved nighttime dataset based on CARLA. According to many experimental evaluations, the tasks in this newly created synthetic dataset are more difficult than others, which poses significant new challenges to the area.
The researchers evaluated their method on two established datasets, namely nuScenes and RobotCar. RobotCar is an autonomous driving dataset that includes videos taken along a consistent route in a variety of weather, traffic, and time of day and night conditions. In contrast, nuScenes is a large autonomous driving dataset of over 1000 video clips collected in various road scenes and weather conditions. On both benchmarks, the approach shows state-of-the-art performance. The researchers successfully developed a self-supervised system that can simultaneously learn image enhancement and depth estimation. Additionally, this research resulted in the development of a brand-new photo-realistically enhanced nighttime dataset with substantial depth ground truth. The team has also publicly released all their code which can be accessed here.
Check out the Paper and Github. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 13k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Khushboo Gupta is a consulting intern at MarktechPost. She is currently pursuing her B.Tech from the Indian Institute of Technology(IIT), Goa. She is passionate about the fields of Machine Learning, Natural Language Processing and Web Development. She enjoys learning more about the technical field by participating in several challenges.


Title: https://www.marktechpost.com/2023/02/09/meet-cutler-cut-and-learn-a-simple-ai-approach-for-training-object-detection-and-instance-segmentation-models-without-human-annotations/
Link: https://www.marktechpost.com/2023/02/09/meet-cutler-cut-and-learn-a-simple-ai-approach-for-training-object-detection-and-instance-segmentation-models-without-human-annotations/
Description: 
Full text:
Object detection and image segmentation are crucial tasks in computer vision and artificial intelligence. They are critical in numerous applications, such as autonomous vehicles, medical imaging, and security systems.
Object detection involves detecting instances of objects within an image or a video stream. It consists of identifying the class of the object and its location within the image. The goal is to produce a bounding box around the object, which can then be used for further analysis or to track the object over time in a video stream. Object detection algorithms can be divided into two categories: one-stage and two-stage. One-stage methods are faster but less accurate, while two-stage methods are slower but more accurate.
On the other hand, image segmentation involves partitioning an image into multiple segments or regions, where each segment corresponds to a different object or part of an object. The goal is to label each pixel in the image with a semantic class, such as “person,” “car,” “sky,” etc. Image segmentation algorithms can be divided into two categories: semantic segmentation and instance segmentation. Semantic segmentation involves labeling each pixel with a class label, while instance segmentation concerns detecting and segmenting individual objects within an image.
Both object detection and image segmentation algorithms have advanced significantly in recent years, mainly due to deep learning approaches. Because of their capacity to learn hierarchical representations of picture input, Convolutional Neural Networks (CNNs) have become the go-to option for these problems. However, training these models necessitates specialized annotations such as object boxes, masks, and localized points, which are both challenging and time-consuming. Without accounting for overhead, manually annotating 164K pictures in the COCO dataset with masks for only 80 classes required more than 28K hours.
With a novel architecture termed Cut-and-LEaRn (CutLER), the authors try to address these issues by studying unsupervised object detection and instance segmentation models that can be trained without human labels. The method consists of three simple architecture- and data-agnostic mechanisms. The pipeline for the proposed architecture is depicted below.
The authors of CutLER first introduce MaskCut, a tool capable of automatically generating several initial rough masks for each image based on features computed by a self-supervised pre-trained vision transformer ViT. MaskCut has been developed to address the limitations of current masking tools, such as Normalized Cuts (NCut). Indeed, NCut’s applications are restricted to single object detection in an image, which can be heavily limiting. For this reason, MaskCut extends it to discover multiple objects per image by iteratively applying NCut to a masked similarity matrix.
Second, the authors implement a straightforward loss-dropping strategy to train the detectors using these coarse masks, which are robust to objects that MaskCut missed. Despite being trained with these rough masks, the detectors can refine the ground truth and produce masks (and boxes) that are more accurate. Therefore, multiple rounds of self-training on the models’ predictions can allow the model to evolve from focusing on local pixel similarities to considering the overall object geometry, resulting in more precise segmentation masks.
The figure below offers a comparison between the proposed framework and state-of-the-art approaches.
This was the summary of CutLER, a novel AI tool for accurate and consistent object detection and image segmentation.
If you are interested or want to learn more about this framework, you can find a link to the paper and the project page.
Check out the Paper, Github, and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 13k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Daniele Lorenzi received his M.Sc. in ICT for Internet and Multimedia Engineering in 2021 from the University of Padua, Italy. He is a Ph.D. candidate at the Institute of Information Technology (ITEC) at the Alpen-Adria-Universität (AAU) Klagenfurt. He is currently working in the Christian Doppler Laboratory ATHENA and his research interests include adaptive video streaming, immersive media, machine learning, and QoS/QoE evaluation.


Title: https://www.marktechpost.com/2023/02/10/meet-mose-a-new-dataset-for-video-object-segmentation-in-complex-scenes/
Link: https://www.marktechpost.com/2023/02/10/meet-mose-a-new-dataset-for-video-object-segmentation-in-complex-scenes/
Description: 
Full text:
One of the most fundamental and difficult computer vision problems is essential for many illegal applications, including video analysis and comprehension, including self-driving cars, augmented reality, video editing, etc. The goal of video object segmentation (VOS) is to separate a certain object from the rest of the video sequence, such as the dominating objects or the things that users have pointed out. There are various VOS options, such as semi-supervised VOS, which provides the target object’s first-frame mask; unsupervised VOS, which detects main objects automatically; and interactive VOS, which depends on the user interacting with the target item. The study of video object segmentation has been addressed in-depth using both conventional and deep learning methodologies.
Deep-learning-based algorithms have significantly outperformed conventional methods in terms of video object segmentation performance. On two of the most popular VOS datasets, DAVIS and YouTube-VOS, state-of-the-art techniques have produced extremely high performance. For instance, XMem scores 92.0% on the DAVIS 2016 test, 87.7% on the DAVIS 2017 test, and 86.1% on the YouTubeVOS test. The video object segmentation has been successfully resolved, given the great performance. But do they see things in actual situations? They review video object segmentation in increasingly sophisticated and realistic settings to find a solution to this query.
In current datasets, the target items are often dominating and conspicuous. Real-world situations often contain complicated and obscured sceneries rather than separate and prominent things. They gather 2,149 films with complex situations to create a new, extremely difficult video object segmentation benchmark they call coMplex video Object SEgmentation to assess state-of-the-art VOS algorithms under more complex circumstances (MOSE). In particular, MOSE has 5,200 items from 36 categories and 431,725 excellent segmentation masks. The most remarkable aspect of MOSE, as seen in Figure 1, is its ability to handle complicated situations with items that disappear and reappear, are small or obvious, are heavily occluded, are in crowded settings, etc.
For instance, the bus obscures the white automobile in the first row of Figure 1, and the third picture’s third image has the most severe occlusion, which eliminates the sedan. The target player in the crowd in Figure 1’s second row is unnoticeable and obscured by the throng in the third frame. It is incredibly challenging to monitor the target player since each time he emerges, he spins around and assumes a different appearance from the previous two frames. Segmenting video objects is made more difficult by the significant occlusion and disappearance of objects in complicated sequences. They want to further studies on video object segmentation in challenging settings and make VOS practical.
To study the dataset, they retrain and assess some of the current VOS techniques on the planned MOSE dataset. They specifically retrain six cutting-edge methods in semisupervised settings using a mask as the first-frame reference, two in semisupervised settings using bounding boxes as the first-frame reference, three in multiobject zero-shot video object segmentation settings, and seven in interactive environments. The experimental findings demonstrate that movies of complex situations reduce the prominence of the state-of-the-art VOS approaches, particularly in tracking objects that temporarily vanish owing to occlusions.
For instance, the performance of XMem on DAVIS 2016 is 92.0% but drops to 57.6% on MOSE, and the implementation of DeAOT on DAVIS 2016 is 92.9% but drops to 59.4% on MOSE, which repeatedly shows the challenges posed by complicated scenarios. Occlusions, crowds, small-scale pictures, object disappearance and reappearance, and flickering across the temporal domain contribute to MOSE’s poor performance. While crowds, small objects, and strong occlusion make it difficult to segment things in pictures, the disappearance and reappearance of objects make it even harder to track an obscured object, adding to the difficulty of association.
Their primary contributions may be summarized as follows:
• They create the MOSE benchmark dataset for video object segmentation (coMplex Video Object Segmentation). MOSE focuses on comprehending video objects in challenging circumstances.
• Taking a close look at MOSE, they analyze the challenges and potential directions for future video understanding research in complex scenes.
• They conduct a thorough comparison and evaluation of state-of-the-art VOS methods on the MOSE dataset under four different settings, including mask-initialization semi-supervised, box-initialization semi-supervised, unsupervised, and interactive settings.
The dataset is accessible on OneDrive, Google Drive, and Baidu Pan. Additional information for accessing it can be found on their project website.
Check out the Paper, Github, and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 13k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Aneesh Tickoo is a consulting intern at MarktechPost. He is currently pursuing his undergraduate degree in Data Science and Artificial Intelligence from the Indian Institute of Technology(IIT), Bhilai. He spends most of his time working on projects aimed at harnessing the power of machine learning. His research interest is image processing and is passionate about building solutions around it. He loves to connect with people and collaborate on interesting projects.


Title: https://www.marktechpost.com/2023/02/11/cmu-researchers-develop-frida-an-artificial-intelligence-ai-powered-robotics-framework-that-can-collaborate-with-humans-to-paint-and-create-beautiful-art/
Link: https://www.marktechpost.com/2023/02/11/cmu-researchers-develop-frida-an-artificial-intelligence-ai-powered-robotics-framework-that-can-collaborate-with-humans-to-paint-and-create-beautiful-art/
Description: 
Full text:
Artificial Intelligence has been the talk of the town ever since it was introduced. It has contributed to almost every single domain with its incredible performance. From automating tasks to quick decision-making, AI is everywhere. It is transforming industries and making the lives of people easier. One such advancement is ChatGPT, which is taking the world by storm. With its ability to answer questions just like a human, it already has more than a million users and is simplifying lives. With all the AI innovations simulating human knowledge and intelligence, now comes the artistic side of AI. A robot named FRIDA has been developed, efficiently associating with humans to generate some amazing art.
Creating art or painting is basically expressing ideas and thoughts with the help of a paintbrush and multiple colors. The elements like colors, tone, and texture are all combined to portray patterns and represent an idea. AI art is produced based on different algorithms and the assistance of Artificial Intelligence. FRIDA is capable of generating such an art. It is a machine in the shape of a robotic arm with a paintbrush attached to it. Based on human commands and assistance, FRIDA paints beautiful pictures on the canvas.
FRIDA, short for Frameworks and Robotics Initiative for Developing Arts, computationally models innovative painting tasks and provides visually appealing and thoughtful art. FRIDA has been developed by students of Carnegie Mellon University’s Robotics Institute, namely Peter Schaldenbrand, James McCann, and Jean Oh, along with the help of RI faculty members. Named after the famous Mexican artist Frida Kahlo, FRIDA works on the concept of real-to-simulation-to-real and keeps on improvising and optimizing the art based on the set goals.
FRIDA works by the user input, a simple text description of the painting that the user wants FRIDA to draw. The user can also input some other related arts to help FRIDA get an idea of what the results should be inspired by. Apart from that, this robotic system can also create a representation based on the uploaded photograph. FRIDA uses Artificial Intelligence models like the currently emerging tools like ChatGPT and DALL-E, which generate text and images. It uses Machine Learning to observe and analyze its performance after simulating how it would use the brush to paint a drawing.
The team behind the research has mentioned that FRIDA is the new way to intersect a robot and creativity, unlike the older robots that only created art using AI but not with any human collaboration. The current methods of creating art with the help of Artificial Intelligence work in a sequential manner without the concept of re-analysis and optimization. Unlike those, FRIDA keeps accessing itself and continues to plan and re-analyze its steps with great optimization.
This new artist in the town is a great innovation and seems promising for future creative collaborations. It has several capabilities, such as helping people engage in arts or even automating monotonous tasks. With its constant evolution feature, the FRIDA robotic arm can greatly help humans by providing them with different ways to express their creative side.
Check out the Paper, Github and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 13k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Tanya Malhotra is a final year undergrad from the University of Petroleum & Energy Studies, Dehradun, pursuing BTech in Computer Science Engineering with a specialization in Artificial Intelligence and Machine Learning.
She is a Data Science enthusiast with good analytical and critical thinking, along with an ardent interest in acquiring new skills, leading groups, and managing work in an organized manner.


Title: https://www.marktechpost.com/2023/02/17/google-ai-and-tel-aviv-researchers-introduce-friendlycore-a-machine-learning-framework-for-computing-differentially-private-aggregations/
Link: https://www.marktechpost.com/2023/02/17/google-ai-and-tel-aviv-researchers-introduce-friendlycore-a-machine-learning-framework-for-computing-differentially-private-aggregations/
Description: 
Full text:
Data analysis revolves around the central goal of aggregating metrics. The aggregation should be conducted in secret when the data points match personally identifiable information, such as the records or activities of specific users. Differential privacy (DP) is a method that restricts each data point’s impact on the conclusion of the computation. Hence it has become the most frequently acknowledged approach to individual privacy.
Although differentially private algorithms are theoretically possible, they are typically less efficient and accurate in practice than their non-private counterparts. In particular, the requirement of differential privacy is a worst-case kind of requirement. It mandates that the privacy requirement holds for any two neighboring datasets, regardless of how they were constructed, even if they are not sampled from any distribution, which leads to a significant loss of accuracy. Meaning that “unlikely points” that have a major impact on the aggregation must be considered in the privacy analysis.
Recent research by Google and Tel Aviv University provides a generic framework for the preliminary processing of the data to ensure its friendliness. When it is known that the data is “friendly,” the private aggregation stage can be carried out without considering potentially influential “unfriendly” elements. Because the aggregation stage is no longer constrained to perform in the original “worst-case” setting, the proposed method has the potential to significantly reduce the amount of noise introduced at this stage.
Initially, the researchers formally define the conditions under which a dataset can be considered friendly. These conditions will vary depending on the type of aggregation required, but they will always include datasets for which the sensitivity of the aggregate is low. For instance, if the sum is average, “friendly” should include compact datasets.
The team developed the FriendlyCore filter that reliably extracts a sizable friendly subset (the core) from the input. The algorithm is designed to meet a pair of criteria:
- It must eliminate outliers to retain only elements close to many others in the core.
- For nearby datasets that differ by a single element, the filter outputs all elements except y with almost the same probability. Cores derived from these nearby databases can be joined together cooperatively.
Then the team created the Friendly DP algorithm, which, by introducing less noise into the total, meets a less stringent definition of privacy. By applying a benevolent DP aggregation method to the core generated by a filter satisfying the aforementioned conditions, the team proved that the resulting composition is differentially private in the conventional sense. Clustering and discovering the covariance matrix of a Gaussian distribution are further uses for this aggregation approach.
The researchers used the zero-Concentrated Differential Privacy (zCDP) model to test the efficacy of the FriendlyCore-based algorithms. 800 samples were taken from a Gaussian distribution with an unknown mean through their paces. As a benchmark, the researchers looked at how it stacked against the CoinPress algorithm. CoinPress, in contrast to FriendlyCore, necessitates a norm of the mean upper bound of R. The proposed method is independent of the upper bound and dimension parameters and hence outperforms CoinPress.
The team also evaluated the efficacy of their proprietary k-means clustering technology by comparing it to another recursive locality-sensitive hashing technique, LSH clustering. Each experiment was repeated 30 times. FriendlyCore frequently fails and produces inaccurate results for tiny values of n (the number of samples from the mixture). Yet as n grows, the proposed technique becomes more likely to succeed (as the created tuples get closer to each other), producing very accurate results, while LSH-clustering falls behind. Even without a distinct division into clusters, FriendlyCore performs well on huge datasets.
Check out the Paper and Reference Article. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 14k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Tanushree Shenwai is a consulting intern at MarktechPost. She is currently pursuing her B.Tech from the Indian Institute of Technology(IIT), Bhubaneswar. She is a Data Science enthusiast and has a keen interest in the scope of application of artificial intelligence in various fields. She is passionate about exploring the new advancements in technologies and their real-life application.


Title: https://www.marktechpost.com/2023/03/01/unlocking-the-secrets-of-deep-learning-with-tensorleaps-explainability-platform/
Link: https://www.marktechpost.com/2023/03/01/unlocking-the-secrets-of-deep-learning-with-tensorleaps-explainability-platform/
Description: 
Full text:
Deep Learning (DL) advances have cleared the way for intriguing new applications and are influencing the future of Artificial Intelligence (AI) technology. However, a typical concern for DL models is their explainability, as experts commonly agree that Neural Networks (NNs) function as black boxes. We do not precisely know what happens inside, but we know that the given input is somehow processed, and as a result, we obtain something as output. For this reason, DL models can often be difficult to understand or interpret. Understanding why a model makes certain predictions or how to improve it can be challenging.
This article will introduce and emphasize the importance of NN explainability, provide insights into how to achieve it, and suggest tools that could improve your DL model’s performance.
The importance of explainability
The explainability of NN models is essential for several reasons. First, in the scientific domain, it is crucial to have as much control as possible over these “black boxes.”
The ability to quickly identify issues with the model helps developers save time and resources. If developers catch problems early in the process, they can avoid training models with fundamental issues.
In addition, it is crucial to investigate the presence or impact of underrepresented samples or categories in the dataset that can affect the model. Indeed, if a biased dataset is used for training a DL model, the model may learn and reinforce the biases present in the data, leading to inaccurate or discriminatory outcomes. This can have serious consequences, particularly in critical fields such as healthcare, finance, and justice, where decisions based on biased or inaccurate algorithms can have life-altering impacts.
As discussed above, datasets play a critical role in DL models and applications. They help the NNs understand how to react to various input stimuli, giving information on both input and the expected outcome.
When dealing with datasets, it is important to understand how the samples contribute to the DL model. This investigation is sometimes harder than it seems for several reasons.
Firstly, when handling labeled data, we must be careful that the labels are correct. In the case of ambiguous data, it can be difficult to determine the correct label. Furthermore, since labeling is a time-consuming and labor-intensive task, it is error-prone and can lead to mislabelling. Particularly with high-dimensional data, it can be challenging to label all features for all samples in a timely and accurate manner.
The problems and challenges mentioned above should motivate us to clean and fix the dataset after training the model. You might be wondering, “How can I do that?” Well, usually, this procedure requires manually looking into the dataset’s samples to check their validity, acknowledging factors leading to poor performance of the model, and re-evaluating the model.
Given the importance of dataset validation and the limitations of current approaches that are extremely time-consuming and error-prone, we began exploring alternative methods to see if there were quick and accurate alternatives.
Explainability Techniques and Tools
For model explainability, many strategies are employed. Some concentrate on global explainability, while others concentrate on local explainability.
Global explainability provides a comprehensive picture of the model’s interpretability. It assimilates how it creates predictions and how the network’s different properties impact the final prediction.
On the other hand, local explainability examines a single sample without offering a comprehensive knowledge of the model and is helpful for model debugging. In certain circumstances, using local explainability, the reasons for particular prediction mistakes can be found based on a single sample.
There are a few available tools that provide model explainability. The first one is DeepLIFT (Deep Learning Important FeaTures), which enables the explainability of a neural network’s predictions. However, it can be computationally expensive, as it requires multiple forward and backward passes through the NN to compute the reference activations used to compute the importance of each input feature for a particular prediction. Furthermore, it provides only global explainability. The second one is called Alibi, an open-source python library aimed at Machine Learning (ML) model inspection and interpretation. It supports the analysis of multiple data types, such as tabular, text, and image. However, Alibi mainly focuses on classical ML algorithms and provides only local explainability.
While most applications fail to provide sufficiently robust insights on global and local explainability, one new all-in-one tool, Tensorleap, can provide you with the complete package. It is a new must-have DL platform for every data science or ML expert. With this out-of-the-box platform, you can unleash the full potential of your neural networks, as it is the best way to troubleshoot, debug and visualize NNs and datasets. You can gain insights into your model’s local and global features with just a few clicks. The advanced management tool visualizes all findings, making it easy to improve datasets, test procedures, and the model. Furthermore, this platform can help you understand how the model perceives the data since it parses the computational graph, tracks every sample, and highlights the most informative features.
No other tool on the market gives you the same advanced features Tensorleap offers, such as Population Visualization, Deep Unit Testing, and Guided Error Analysis.
Dataset Visualization and Clean-up
It would be wrong to assert that the success of a NN model lies all in its architecture. A massive role is played by the dataset exploited for training, which means that the model will not see the sunlight if the dataset is faulty. The three most important steps to assess the bounty of a dataset are scoring, labeling prioritization, and dataset clean-up. Scoring refers to the dataset’s quality, measured in terms of variance, density, entropy, and balance. Labeling prioritization helps assess which data are most important to gather and how to label them correctly. Cleaning up the dataset from all redundant, ambiguous, and inadequate data in most cases improves the NN’s performance.
Tensorleap simplifies the detection and removal of erroneous data with Population Analysis. The platform tracks the response of each feature in each layer to each instance in the dataset, creating a similarity map based on the model’s interpretation of similar samples. This tool clusters and visually represents the samples, allowing you to identify which ones are causing problems in the dataset. Each dot is associated with a sample, and its size represents the impact of the error on the overall error of the NN. The influence of each dot on the overall error can be smartly analyzed using any custom metric or loss. With Tensorleap, you can quickly target and fix problems in your dataset or gain valuable insights for better results.
Deep Unit Testing
Software developers generally agree that unit testing is an essential phase in the software development process because it simplifies finding bugs and weak spots in the code. Unit testing the network’s components is equally crucial in DL models not only to find network logic errors, but also to continuously improve the performance of our model. Indeed, unit tests provide a safety net when applying changes to the model. Therefore, when you make changes to refine your model on specific categories, you can rerun the tests to ensure that the changes have not broken any existing functionality or impacted any other category. For instance, the model can be very good at detecting specific classes but very bad at detecting others. With this knowledge, the attention can be shifted where the network needs it most.
Deep Unit Testing in Tensorleap allows developers to test their DL models and inspect unwanted behavior. The tool enables the creation of multiple unit tests based on the model’s features or some properties of the samples, which can then be all validated simultaneously. Using a visual representation dashboard, you can focus on specific sample groups to identify and monitor specific unit tests. In the same way, Tensorleap allows you to conduct unsupervised analysis by utilizing the features of a model to identify potential clusters and abnormalities, providing an understanding of how the model performs in particular scenarios.
This process helps data scientists identify where improvements need to be made. Tensorleap automatizes and makes this process easier since manual testing is sometimes tedious or even impossible.
Error Analysis and Troubleshooting
Error analysis is important in DL as it helps reduce the cost of training a model. One hour of training a large model can cost hundreds or thousands of dollars, and if errors are found after training, it results in wasted money. Tensorleap provides an effective way to identify and fix errors in a model, saving time and money and avoiding potential risks in deployment. It visualizes network errors and automatically detects failures, allowing users to track issues and errors and improve model performance with precision. In addition, the platform provides insight into the model’s success or failures and reports when these failures occur more frequently. This solution eliminates the need for random testing and allows users to see where the model excels or needs improvement.
Conclusion
DL explainability is a vital element that allows our DL models to be developed faster and more robustly. Tensorleap gives data scientists the tools to construct reliable models and balanced datasets and enhance results while lowering development expenses. The platform is an excellent resource for data scientists who want to understand their NNs, lessen their associated failure risks, and improve their performance towards real issues that these models encounter.
If you want to try it, Tensorleap can be found following this link, along with well-written documentation.
Note: Thanks to the Tensorleap team for the thought leadership/ Educational article above. Tensorleap has supported and sponsored this Content.
Daniele Lorenzi received his M.Sc. in ICT for Internet and Multimedia Engineering in 2021 from the University of Padua, Italy. He is a Ph.D. candidate at the Institute of Information Technology (ITEC) at the Alpen-Adria-Universität (AAU) Klagenfurt. He is currently working in the Christian Doppler Laboratory ATHENA and his research interests include adaptive video streaming, immersive media, machine learning, and QoS/QoE evaluation.


Title: https://www.marktechpost.com/2023/03/04/a-new-ai-research-proposes-voxformer-a-transformer-based-3d-semantic-scene-completion-framework/
Link: https://www.marktechpost.com/2023/03/04/a-new-ai-research-proposes-voxformer-a-transformer-based-3d-semantic-scene-completion-framework/
Description: 
Full text:
Understanding a holistic 3D picture is a significant challenge for autonomous vehicles (AV) to perceive. It directly influences later activities like planning and map creation. The lack of sensor resolution and the partial observation caused by the small field of vision and occlusions make it challenging to get precise and comprehensive 3D information about the actual environment. Semantic scene completion (SSC), a method for jointly inferring the whole scene geometry and semantics from sparse observations, was offered to solve the problems. Scene reconstruction for viewable areas and scene hallucination for obstructed sections are two subtasks an SSC solution must handle concurrently. Humans readily reason about scene geometry and semantics based on imperfect observations, which supports this endeavor.
Nevertheless, modern SSC techniques still lag below human perception in driving scenarios in terms of performance. LiDAR is regarded as a main modality by most current SSC systems to provide precise 3D geometric measurements. Yet, cameras are more affordable and offer better visual indications of the driving environment, but LiDAR sensors are more costly and less portable. This inspired the investigation of camera-based SSC solutions, which were initially put forth in the ground-breaking work of MonoScene. MonoScene uses dense feature projection to convert 2D picture inputs to 3D. Yet, such a projection gives empty or occluded voxels 2D characteristics from the viewable areas. An empty voxel covered by a car, for instance, will nevertheless receive the visual characteristic of the automobile.
As a result, the 3D features created have poor performance regarding geometric completeness and semantic segmentation—their involvement. VoxFormer, in contrast to MonoScene, views 3D-to-2D cross-attention as a representation of sparse queries. The suggested design is inspired by two realizations: (1) sparsity in 3-D space: Since a significant portion of 3-D space is typically empty, a sparse representation rather than a dense one is undoubtedly more effective and scalable. (2) reconstruction-before-hallucination: The 3D information of the non-visible region can be better completed using the reconstructed visible areas as starting points.
In brief, they made the following contributions to this effort:
• A cutting-edge two-stage system that transforms photos into a whole 3D voxelized semantic scene.
• An innovative 2D convolution-based query proposal network that produces trustworthy inquiries from picture depth.
• A unique Transformer that produces a full 3D scene representation and is akin to the masked autoencoder (MAE).
• As seen in Fig. 1(b), VoxFormer advances the state-of-the-art camera-based SSC .
VoxFormer comprises two stages: stage 1 suggests a sparse set of occupied voxels, and stage 2 completes the scene representations beginning from stage 1’s recommendations. Stage 1 is class-agnostic, while stage 2 is class-specific. As illustrated in Fig. 1(a), Stage-2 is built on a unique sparse-to-dense MAE-like design. In particular, stage-1 contains a lightweight 2D CNN-based query proposal network that reconstructs the scene geometry using picture depth. Then, throughout the whole field of vision, it suggests a sparse collection of voxels using preset learnable voxel queries.
They first strengthen their featurization by enabling the suggested voxels to pay attention to the picture observations. The remaining voxels will then be processed by self-attention to finish the scene representations for per-voxel semantic segmentation after the non-proposed voxels are connected to a learnable mask token. VoxFormer provides state-of-the-art geometric completion and semantic segmentation performance, according to extensive experiments on the large-scale SemanticKITTI dataset. More critically, as demonstrated in Fig. 1, the benefits are large in safety-critical short-range locations.
Check out the Paper and Github. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Aneesh Tickoo is a consulting intern at MarktechPost. He is currently pursuing his undergraduate degree in Data Science and Artificial Intelligence from the Indian Institute of Technology(IIT), Bhilai. He spends most of his time working on projects aimed at harnessing the power of machine learning. His research interest is image processing and is passionate about building solutions around it. He loves to connect with people and collaborate on interesting projects.


Title: https://www.marktechpost.com/2023/03/05/a-new-artificial-intelligence-ai-study-proposes-a-3d-aware-blending-technique-with-generative-nerfs/
Link: https://www.marktechpost.com/2023/03/05/a-new-artificial-intelligence-ai-study-proposes-a-3d-aware-blending-technique-with-generative-nerfs/
Description: 
Full text:
Image blending is a primary method in computer vision, one of the most known branches in the artificial intelligence component. The goal is to blend two or more images to produce a unique combination that incorporates the finest aspects of each input image. This method is extensively used in various application fields, including picture editing, computer images, and medical imaging.
Image blending is frequently used in artificial intelligence activities such as picture segmentation, object identification, and image super-resolution. It is critical in improving image clarity, which is essential for many uses, such as robotics, automated driving, and surveillance.
Over the years, several image blending techniques have been created, primarily relying on warping an image via 2D affine transformation. However, these approaches do not account for the discrepancy in 3D geometric features like pose or shape. 3D alignment is much more challenging to achieve, as it requires inferring the 3D structure from a single view.
To address this issue, a 3D-aware image blending method based on generative Neural Radiance Fields (NeRFs) has been proposed.
The purpose of generative NeRFs is to learn a strategy to synthesize images in 3D using only collections of 2D single-view images. Therefore, the authors project the input images to the volume density representation of generative NeRFs. To reduce the dimensionality and complexity of data and operations, the 3D-aware blending is then performed on these NeRFs’ latent representation spaces.
Concretely, the formulated optimization problem considers the latent code’s impact in synthesizing the blended image. The goal is to edit the foreground based on the reference images while preserving the background of the original image. For instance, if the two considered images were faces, the framework must replace the facial characteristics and features of the original image with the ones from the reference image while keeping the rest unchanged (hair, neck, years, surroundings, etc.).
An overview of the architecture compared to previous strategies is proposed in the picture below.
The first method consists of the sole 2D blending of two 2D images without alignment. An improvement can be found by supporting this 2D blending method with the 3D-aware alignment with generative NeRFs. To further exploit 3D information, the final architecture infers on two images in NeRFs’ latent representation spaces instead of 2D pixel space.
3D alignment is achieved via a CNN encoder, which infers the camera pose of each input image, and via the latent code of the image itself. Once the reference image is correctly rotated to reflect the original image, the NeRF representations of both images are computed. Lastly, the 3D transformation matrix (scale, translation) is estimated from the original image and applied to the reference image to obtain a semantically-accurate blend.
The results on unaligned images with different poses and scales are reported below.
According to the authors and their experiments, this method outperforms both classic and learning-based methods regarding both photorealism and faithfulness to the input images. Additionally, exploiting latent-space representations, this method can disentangle color and geometric changes during blending and create view-consistent results.
This was the summary of a novel AI framework for 3D-aware Blending with Generative Neural Radiance Fields (NeRFs).
If you are interested or want to learn more about this framework, you can find below a link to the paper and the project page.
Check out the Paper, Github, and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Daniele Lorenzi received his M.Sc. in ICT for Internet and Multimedia Engineering in 2021 from the University of Padua, Italy. He is a Ph.D. candidate at the Institute of Information Technology (ITEC) at the Alpen-Adria-Universität (AAU) Klagenfurt. He is currently working in the Christian Doppler Laboratory ATHENA and his research interests include adaptive video streaming, immersive media, machine learning, and QoS/QoE evaluation.


Title: https://www.marktechpost.com/2023/03/06/mimicking-is-the-way-innovative-ai-model-lets-robots-learn-tasks-by-watching-human-videos/
Link: https://www.marktechpost.com/2023/03/06/mimicking-is-the-way-innovative-ai-model-lets-robots-learn-tasks-by-watching-human-videos/
Description: 
Full text:
Robots are incredible. They have already revolutionized the way we live and work, and they still have the potential to do it again. They changed the way we live by doing mundane tasks for us, like vacuuming. Moreover, and more importantly, they changed the way we produce. Robots can perform complex tasks with speed, precision, and efficiency that far exceeds what humans are capable of.
Robots helped us to significantly increase productivity and output in industries such as manufacturing, logistics, and agriculture. As they continue to advance, we can expect them to become even more sophisticated and versatile. We will be able to use them to perform tasks that were previously thought impossible. For example, robots equipped with artificial intelligence and machine learning algorithms can now learn from their environment and adapt to new situations, making them even more useful in a wide range of applications.
However, robots are still expensive and fancy toys. Building them is one story, but teaching them to do something is often extremely time-consuming and requires extensive programming skills. Teaching robots how to perform manipulation tasks that are generally applicable with high efficiency has been a persistent challenge for a long time.
One approach to teaching robots efficiently is to use imitation learning. Imitation learning is a method of teaching robots how to perform tasks by imitating human demonstrations. Robots can observe and mimic human movements and then use that data to improve their own abilities. While recent advancements in imitation learning have shown promise, there are still significant obstacles to overcome.
Imitation learning is really useful to train robots to perform simple tasks such as opening a door or picking up a specific object, as these actions have a single goal, require short-horizon memory, and conditions usually do not change during the action. However, the issue arises when we change the task to a more complex one with varied initial and goal conditions.
The biggest challenge here is the time and labor required to collect long-horizon demonstrations. There are two main research directions to scale up imitation learning for more complex tasks; hierarchical imitation learning and learning from play data. Hierarchical imitation learning breaks down the learning process into high-level planners and low-level visuomotor controllers to increase sample efficiency and make it easier for robots to learn complex tasks.
On the other hand, learning from play data is about training robots using data collected from human-teleoperated robots interacting with the environment without specific task goals or guidance. This type of data is usually more diverse than task-oriented ones as they cover a wide range of behaviors and situations. However, collecting such play data can be costly.
These two approaches solve different problems, but we need something to combine them both. A way to utilize the efficiency of hierarchical imitation and effectiveness of learning from play data. Let us meet with MimicPlay.
MimicPlay aims to enable robots to learn long-horizon manipulation tasks using a combination of human play data and demonstration data. A goal-conditioned latent planner is trained using human play data that predicts future human hand trajectories based on goal images. This plan provides coarse guidance at each time step, making it easier for the robot to generate guided motions and perform complex tasks. Once the plan is ready, the low-level controller incorporates state information to generate final actions.
MimicPlay is evaluated on 14 long-horizon manipulation tasks in six different environments, and it managed to significantly improve the performance over state-of-the-art imitation learning methods, especially in sample efficiency and generalization abilities. This means MimicPlay was able to teach the robot how to perform complex tasks more quickly and accurately while also managing to generalize this knowledge to new environments.
Check out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Ekrem Çetinkaya received his B.Sc. in 2018 and M.Sc. in 2019 from Ozyegin University, Istanbul, Türkiye. He wrote his M.Sc. thesis about image denoising using deep convolutional networks. He is currently pursuing a Ph.D. degree at the University of Klagenfurt, Austria, and working as a researcher on the ATHENA project. His research interests include deep learning, computer vision, and multimedia networking.


Title: https://www.marktechpost.com/2023/03/08/leveraging-tensorleap-for-effective-transfer-learning-overcoming-domain-gaps/
Link: https://www.marktechpost.com/2023/03/08/leveraging-tensorleap-for-effective-transfer-learning-overcoming-domain-gaps/
Description: 
Full text:
Nowadays, constructing a large-scale dataset is the prerequisite to achieving the task in our hands. Sometimes the task is a niche, and it would be too expensive or even not possible to construct a large-scale dataset for it to train an entire model from scratch. Do we need to train a model from scratch in all cases?
Imagine we would like to detect a certain animal, let’s say an otter, in images. We first need to collect many otter images and construct a training dataset. Then, we need to train a model with those images. Now, imagine we want our model to learn how to detect koalas. What do we do now? Again, we collect many koala images and construct our dataset with them. Do we need to train our model from scratch again with the combined dataset of otter and koala images? We already had a model trained on otter images. Why are we wasting it? It learned some features to detect animals which could also come in handy for detecting koalas. Can we utilize this pre-trained model to make things faster and simpler?
Yes, we can, and that is called transfer learning. It is a machine learning technique that enables a model trained on one task to be used as a starting point for another related task. Instead of starting from scratch, this leads to faster and more efficient training and improved performance on the new task in most cases.
So all we need to do is find an existing model and use it as a starting point for our new training. Is it that simple, though? What if we change the problem to a more complicated one? Like image segmentizing the objects on the road for autonomous driving. We cannot just take a pre-trained model and use them as it is. If the model was pre-trained on city roads, it might not perform well when applied to rural roads. Just look at the difference!
One of the biggest, if not the biggest, challenges in transfer learning is adapting the model to the difference between the source and the target dataset. We use the term domain gap to refer to the significant difference between the distribution of features in the source and target datasets. This difference can cause problems for the pre-trained model as it would be difficult for the model to transfer the knowledge from the source to the target domain. Therefore, identifying and reducing the domain gaps is crucial when we plan to do transfer learning. These gaps can happen in any field, but they are particularly important for the safety-critical fields where the error cost is too high.
However, identifying domain gaps is not a straightforward task. We need to do certain evaluations to identify the domain gap between datasets:
- Analyze the statistical properties, like class feature distributions, to identify any significant differences.
- Visualize the data in a low-dimensional space, preferably in the latent space, to see if they form distinct clusters and compare their distribution.
- Evaluate the pre-trained model on the target dataset to assess its initial performance. If the model performs poorly, it might indicate a domain gap.
- Hold some ablation studies by removing certain components of the pre-trained model. This way, we can learn which components are transferable and which are not.
- Apply domain adaptation techniques like domain adversarial training or fine-tuning.
They all sound nice and fine, but all these operations require intense manual labor and consume a lot of time. Let us discuss this using a solid example which should make things clear.
Assume we have an image segmentation model, DeepLabV3Plus, which is trained on the Cityscapes dataset that contains data from more than fifty European cities. For simplicity, let’s say we work with a subset of the Cityscapes dataset using two cities, Aschen and Zurich. To train our model, we want to use the KITTI dataset that is constructed using data captured during driving in a mid-size city, rural area, and highway. We must identify the domain gap between those datasets to adapt our model properly and eliminate potential errors. How can we do it?
First, we need to find out if we have a domain gap. To do that, we can take the pre-trained model and run it on both datasets. Of course, first, we need to prepare both datasets for evaluation, find their error, and then compare the results. If the average error between the source and the target dataset is too high, that indicates we have a domain gap to fix.
Now we know we have a domain gap, how can we identify the root cause of it? We can start by finding the samples with the highest loss and compare them to find their common characteristics. It could be the color variation, roadside object variation, car variation, area that the sky covers, etc. We must first try fixing each of these differences, normalizing them properly to ensure they fit the source dataset’s characteristics, and reevaluate our model to see if the “root” cause we found was actually the root cause of the domain gap.
What if we had a tool that could do all these for us automatically so we could focus on the real aspect, solving the problem we have in hand? Thankfully, somebody thought about it and came up with the TensorLeap.
TensorLeap is a platform to enhance the development of deep neural network-based solutions. TensorLeap offers an advanced suite of tools to aid data scientists in refining and explaining their models. It provides valuable insights into the models and identifies their strengths and weaknesses. On top of that, the included tools for error analysis, unit testing, and dataset architecture are extremely helpful in finding the root cause of the problem and making the final model effective and reliable.
You can read this blog post to learn how it can be used to solve the domain gap problem in Cityscapes and KITTI datasets. In this example, TensorLeap’s automatic preparation of optimal latent space and various analytic tools, dashboards, and insights helped quickly spot and reduce three domain gaps, significantly improving the model’s performance. Identifying and fixing those domain gaps would have taken months of manual work, but with TensorLeap, it can be done in a matter of hours.
Note: Thanks to the Tensorleap team for the thought leadership/ Educational article above. Tensorleap has supported this Content.
Ekrem Çetinkaya received his B.Sc. in 2018 and M.Sc. in 2019 from Ozyegin University, Istanbul, Türkiye. He wrote his M.Sc. thesis about image denoising using deep convolutional networks. He is currently pursuing a Ph.D. degree at the University of Klagenfurt, Austria, and working as a researcher on the ATHENA project. His research interests include deep learning, computer vision, and multimedia networking.


Title: https://www.marktechpost.com/2023/03/09/divide-and-track-this-ai-model-can-track-3d-human-motion-in-videos-by-decoupling/
Link: https://www.marktechpost.com/2023/03/09/divide-and-track-this-ai-model-can-track-3d-human-motion-in-videos-by-decoupling/
Description: 
Full text:
Deep learning has been a game-changer in the field of computer vision, enabling unprecedented advances in numerous applications. One of these applications is tracking human movement in videos. The goal here is to accurately locate and follow people as they move through a video sequence. This is useful in applications like sports analytics and surveillance.
Tracking human motion in videos has always been a challenging problem in computer vision. We have seen remarkable progress in tracking human movement from videos captured in controlled environments, where the camera and human motion are well-defined, and the background is static. We have deep neural networks that can detect and track humans robustly, even in challenging conditions such as occlusion and partial visibility.
However, tracking the movement from videos captured in uncontrolled and dynamic environments is still an open problem. In those cases, we have several issues that make the human tracking algorithm fail. Camera motion is unpredictable, and the scene is cluttered with moving objects, which makes it challenging to construct global human trajectories accurately.
Existing approaches either rely on additional sensors like multiple cameras or require dense 3D modeling of the environment. We cannot obtain this information unless we have a controlled environment which is obviously the case for the videos captured in the wild.
So, do we need to set up the game field with expensive sensors and cameras whenever we want to track the players in the game to analyze their performance? Can we have an alternative solution that does not rely on controlling the environment and can actually provide an accurate motion trajectory for us using a single camera? The answer is yes, and it is called SLAHMR.
SLAHMR can acquire global trajectories from videos in the wild with no constraints on the capture setup, camera motion, or prior knowledge of the environment.
This is achieved by applying the Simultaneous Localization and Mapping (SLAM) system to estimate the relative camera motion between frames using the pixel information. While that’s happening, a 3D human tracking component estimates the body poses of all detected people. Once these estimates are obtained, SLAHMR uses them to initialize the trajectories of the humans and cameras in the shared world frame. Then, these trajectories are optimized over multiple stages to be consistent with both 2D observations in the video and learned priors about how humans move in real life.
What makes SLAHMR unique is its ability to optimize human and camera trajectories without requiring 3D reconstruction of the static scene. This enables executing SLAHMR on videos captured in the wild that do not contain any prior information about the 3D structure of the environment.
SLAHMR is a product of two valuable insights. The first insight is that even if the apparent displacement of objects in the scene is not sufficient for accurate scene reconstruction, it still allows for reasonable estimates of camera motion. Therefore, by analyzing the relative motion of the camera between frames, SLAHMR can accurately estimate the overall camera motion.
The second insight is that human motion is limited. We move in certain patterns, and those patterns are not subject to significant changes. Therefore, training a model to estimate human movement using large datasets results in an accurate approximation.
Overall, SLAHMR can accurately capture 3D human motion in videos without constraints on the capture setup, camera motion, or prior knowledge of the environment. Moreover, it can handle multiple people and reconstruct their motion in the same world coordinate frame.
Check out the Paper and Code. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Ekrem Çetinkaya received his B.Sc. in 2018 and M.Sc. in 2019 from Ozyegin University, Istanbul, Türkiye. He wrote his M.Sc. thesis about image denoising using deep convolutional networks. He is currently pursuing a Ph.D. degree at the University of Klagenfurt, Austria, and working as a researcher on the ATHENA project. His research interests include deep learning, computer vision, and multimedia networking.


Title: https://www.marktechpost.com/2023/03/11/meet-prismer-an-open-source-vision-language-model-with-an-ensemble-of-experts/
Link: https://www.marktechpost.com/2023/03/11/meet-prismer-an-open-source-vision-language-model-with-an-ensemble-of-experts/
Description: 
Full text:
Several recent vision-language models have demonstrated remarkable multi-modal generation abilities. But typically, they call for training enormous models on enormous datasets. Researchers introduce Prismer, a data- and parameter-efficient vision-language model that uses an ensemble of domain experts, as a scalable alternative. By inheriting most of the network weights from publicly available, pre-trained domain experts and freezing them during training, Prismer only requires training a few components.
The generalization abilities of large pre-trained models are exceptional across many different tasks. However, these features come at a high price, necessitating a lot of training data and computational resources for training and inference. Models with hundreds of billions of trainable parameters are common in the language domain, and they typically necessitate a computing budget on the yottaFLOP scale.
Issues related to visual language learning are more difficult to solve. Even though this field is a superset of language processing, it also necessitates visual and multi-modal thinking expertise. Using its projected multi-modal signals, Prismer is a data-efficient vision-language model that uses a wide range of pre-trained experts. It can handle visual question answering and picture captioning, two examples of vision-language reasoning tasks. Using a prism as an example, Prismer divides a general reasoning job into several smaller, more manageable chunks.
Researchers developed a visually conditioned autoregressive text generation model toTwo of Prismer’s most important design features are I vision-only. Language-only models for web-scale knowledge to construct our core network backbones, and (ii) modalities-specific vision experts encoding multiple types of visual information, from low-level vision signals like depth to high-level vision signals like instance and semantic labels, as auxiliary knowledge, directly from their corresponding network outputs. Researchers developed a visually conditioned autoregressive text generation model to better use various pre-trained domain experts for exploratory vision-language reasoning tasks.
Even though Prismer was only trained on 13M examples of publicly available image/alt-text data, it shows strong multi-modal reasoning performance in tasks like image captioning, image classification, and visual question answering, which is competitive with many state-of-the-art vision language models. Researchers conclude with a thorough investigation of Prismer’s learning habits, where researchers find several good features.
Model Design:
The Prismer model, shown in its encoder-decoder transformer version, draws on a large pool of already-trained subject matter experts to speed up the training process. A visual encoder plus an autoregressive language decoder make up this system. The vision encoder receives a sequence of RGB and multi-modal labels (depth, surface normal, and segmentation labels anticipated from the frozen pre-trained experts) as input. It produces a sequence of RGB and multi-modal features as output. As a result of this cross-attention training, the language decoder is conditioned to generate a string of text tokens.
Advantages:
- The Prismer model has several benefits, but one of the most notable is that it uses data extremely efficiently while being trained. Prismer is constructed on top of pre-trained vision-only and language-only backbone models to achieve this goal with a considerable decrease in GPU hours necessary to attain equivalent performance to other state-of-the-art vision-language models. One may use these pre-trained parameters to use the massive amounts of available web-scale knowledge.
- Researchers also developed a multi-modal signal input for the vision encoder. The created multi-modal auxiliary knowledge can better capture semantics and information about the input image. Prismer’s architecture is optimized for maximizing the use of trained experts with few trainable parameters.
Researchers have included two varieties of pre-trained specialists in Prismer:
- Specialists in the Backbone The pre-trained models responsible for translating text and pictures into a meaningful sequence of tokens are called “vision-only” and “language-only” models, respectively.
- Depending on the data used in their training, moderators of Discourse Models may label tasks in various ways.
Properties
- The more knowledgeable people there are, the better the results. As the number of modality specialists in Prismer grows, its performance enhances.
- More Skilled Professionals, Higher Results researchers replace some fraction of the predicted depth labels with random noise taken from a Uniform Distribution to create a corrupted depth expert and assess the effect of expert quality on Prismer’s performance.
- Resistance to Unhelpful Opinions the findings further demonstrate that Prismer’s performance is steady when noise-predicting experts are incorporated.
Check out the Paper and Github. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Dhanshree Shenwai is a Computer Science Engineer and has a good experience in FinTech companies covering Financial, Cards & Payments and Banking domain with keen interest in applications of AI. She is enthusiastic about exploring new technologies and advancements in today’s evolving world making everyone's life easy.


Title: https://www.marktechpost.com/2023/03/11/mit-researchers-uncover-the-structural-properties-and-dynamics-of-deep-classifiers-offering-novel-explanations-for-optimization-generalization-and-approximation-in-deep-networks/
Link: https://www.marktechpost.com/2023/03/11/mit-researchers-uncover-the-structural-properties-and-dynamics-of-deep-classifiers-offering-novel-explanations-for-optimization-generalization-and-approximation-in-deep-networks/
Description: 
Full text:
Researchers from MIT and Brown University have conducted a groundbreaking study on the dynamics of training deep classifiers, a widespread neural network used for tasks like image classification, speech recognition, and natural language processing. The study, published in the journal Research, is the first to analyze the properties that emerge during the training of deep classifiers with square loss.
The study primarily focuses on two types of deep classifiers : convolutional neural networks and fully connected deep networks. The researchers discovered that deep networks using stochastic gradient descent , weight decay regularization , and weight normalization (WN) are prone to neural collapse if they are trained to fit their training data. Neural collapse refers to when the network maps multiple examples of a particular class to a single template, making it challenging to accurately classify new examples. The researchers proved that neural collapse arises from minimizing the square loss using SGD, WD, and WN.
The researchers found that weight decay regularization helps prevent the network from over-fitting the training data by reducing the magnitude of the weights, while weight normalization scales the weight matrices of a network to have a similar scale. The study also validates the classical theory of generalization, indicating that its bounds are meaningful and that sparse networks such as CNNs perform better than dense networks. The authors proved new norm-based generalization bounds for CNNs with localized kernels, which are networks with sparse connectivity in their weight matrices.
Moreover, the study found that a low-rank bias predicts the existence of intrinsic SGD noise in the weight matrices and output of the network, providing an intrinsic source of noise comparable to chaotic systems. The researchers’ findings provide new insights into the properties that arise during deep classifier training and can advance our understanding of why deep learning works so well.
In conclusion, the MIT and Brown University researchers’ study provides crucial insights into the properties that emerge during deep classifier training. The study validates the classical theory of generalization, introduces new norm-based generalization bounds for CNNs with localized kernels, and explains how weight decay regularization and weight normalization help prevent neural collapse. Additionally, the study found a low-rank bias predicts the existence of intrinsic SGD noise, which offers a new perspective on understanding the noise within deep neural networks. These findings could significantly advance the field of deep learning and contribute to the development of more accurate and efficient models.
Check out the Paper and Reference Article. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Niharika is a Technical consulting intern at Marktechpost. She is a third year undergraduate, currently pursuing her B.Tech from Indian Institute of Technology(IIT), Kharagpur. She is a highly enthusiastic individual with a keen interest in Machine learning, Data science and AI and an avid reader of the latest developments in these fields.


Title: https://www.marktechpost.com/2023/03/11/the-scope-of-foundation-models-in-decision-making-their-challenges-opportunities-and-potentials/
Link: https://www.marktechpost.com/2023/03/11/the-scope-of-foundation-models-in-decision-making-their-challenges-opportunities-and-potentials/
Description: 
Full text:
Foundation models have taken the Artificial Intelligence community by storm. Their recent impact has helped contribute to a wide range of industries such as healthcare, finance, education, entertainment, etc. The popular large language models such as GPT-3, DALLE 2, and BERT are the ones that are known as foundation models and are performing extraordinary tasks and easing lives. GPT-3 can write an excellent essay and generate content given just a short natural language prompt. DALLE 2 can create images in response to a simple textual description. These models are the only reason due to which Artificial Intelligence and Machine Learning are rapidly moving through a paradigm shift.
In a recent research paper, a team of researchers explored the scope of foundation models in decision-making. The team has proposed some conceptual tools and technical background for going in-depth into the problem space and inspecting the new research directions. A foundation model is basically a model which is trained in a way that it can be used for downstream tasks, i.e., it can be used for tasks for which it has not previously been trained. The less popular terms, such as self-supervised and pre-trained models, are interchangeably used for foundation models only. These reusable AI models can be applied to any field or industry task.
The research paper reviews and addresses the latest methods that aid foundation models in practical decision-making. These models are used in various applications in several ways, like prompting, conditional generative modeling, planning, optimal control, and reinforcement learning. The paper mentions relevant background and notations of sequential decision-making. It introduces a few example scenarios where foundation models and decision-making are better considered jointly, such as using human feedback for dialogue tasks, using the internet as an environment for decision-making, and considering the task of video generation as a universal policy.
Foundation models can be presented as generative models of behavior and the environment. The paper discusses how skill discovery can be an example of behavior. On the other hand, foundation models can be generative models of the environment for conducting model-based rollouts. These models can even describe different components of decision-making, such as states (S), behaviors (A), dynamics (T), and task specifiers (R), through generative modeling or representation learning with examples of plug-and-play vision-language models, model-based representation learning and so on.
The paper, in the end, discusses common challenges and issues while applying foundation models to decision-making. One is the dataset gap, as the big datasets used for vision and language tasks can have different structures and manners than interactive datasets. For example, videos in a broad dataset mostly do not have explicit action labels, whereas actions and rewards are significant components of interactive datasets. To overcome the challenge, broad video, and text data can be made more task-specific by post-processing the data, using techniques like hindsight relabeling actions and rewards. In contrast, the decision-making datasets can be made so by blending a variety of task-specific datasets. Thus, this latest research paper explains how the advancing foundation models can be utilized for different decision-making opportunities by overcoming challenges.
Check out the Paper. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Tanya Malhotra is a final year undergrad from the University of Petroleum & Energy Studies, Dehradun, pursuing BTech in Computer Science Engineering with a specialization in Artificial Intelligence and Machine Learning.
She is a Data Science enthusiast with good analytical and critical thinking, along with an ardent interest in acquiring new skills, leading groups, and managing work in an organized manner.


Title: https://www.marktechpost.com/2023/03/11/understanding-explainable-ai-and-interpretable-ai/
Link: https://www.marktechpost.com/2023/03/11/understanding-explainable-ai-and-interpretable-ai/
Description: 
Full text:
As a result of recent technological advances in machine learning (ML), ML models are now being used in a variety of fields to improve performance and eliminate the need for human labor. These disciplines can be as simple as assisting authors and poets in refining their writing style or as complex as protein structure prediction. Furthermore, there is very little tolerance for error as ML models gain popularity in a number of crucial industries, like medical diagnostics, credit card fraud detection, etc. As a result, it becomes necessary for humans to comprehend these algorithms and their workings on a deeper level. After all, for academics to design even more robust models and repair the flaws of present models concerning bias and other concerns, obtaining a greater knowledge of how ML models make predictions is crucial.
This is where Interpretable (IAI) and Explainable (XAI) Artificial Intelligence techniques come into play, and the need to understand their differences become more apparent. Although the distinction between the two is not always clear, even to academics, the terms interpretability and explainability are sometimes used synonymously when referring to ML approaches. It is crucial to distinguish between IAI and XAI models because of their increasing popularity in the ML field in order to assist organizations in selecting the best strategy for their use case.
To put it briefly, interpretable AI models can be easily understood by humans by only looking at their model summaries and parameters without the aid of any additional tools or approaches. In other words, it is safe to say that an IAI model provides its own explanation. On the other hand, explainable AI models are very complicated deep learning models that are too complex for humans to understand without the aid of additional methods. This is why when Explainable AI models can give a clear idea of why a decision was made but not how it arrived at that decision. In the rest of the article, we take a deeper dive into the concepts of interpretability and explainability and understand them with the help of examples.
1. Interpretable Machine Learning
We argue that anything can be interpretable if it is possible to discern its meaning, i.e., its cause and effect can be clearly determined. For instance, if someone consumes too many chocolates straight after dinner, they always have trouble sleeping. Situations of this nature can be interpreted. A model is said to be interpretable in the domain of ML if people can understand it on their own based on its parameters. With interpretable AI models, humans can easily understand how the model arrived at a particular solution, but not if the criteria used to arrive at that result is sensible. Decision trees and linear regression are a couple of examples of interpretable models. Let’s illustrate interpretability better with the help of an example:
Consider a bank that uses a trained decision-tree model to determine whether to approve a loan application. The applicant’s age, monthly income, whether they have any other loans that are pending, and other variables are taken into consideration while making a decision. To understand why a particular decision was made, we can easily traverse down the nodes of the tree, and based on the decision criteria, we can understand why the end result was what it was. For instance, a decision criterion can specify that a loan application won’t be authorized if someone who is not a student has a monthly income of less than $3000. However, we cannot comprehend the rationale behind choosing the decision criteria by using these models. For instance, the model fails to explain why a $3000 minimum income requirement is enforced for a non-student applicant in this scenario.
To produce the supplied output, interpreting different factors, including weights, features, etc., is necessary for organizations that wish to better understand why and how their models generate predictions. But this is possible only when the models are fairly simple. Both the linear regression model and the decision tree have a small number of parameters. As models become more complicated, we can no longer understand them this way.
2. Explainable Machine Learning
Explainable AI models are ones whose internal workings are too complex for humans to comprehend how they affect the final prediction. Black-box models, in which model features are regarded as the input and the eventually produced predictions are the output, are another name for ML algorithms. Humans require additional methods to look into these “black-box” systems in order to comprehend how these models operate. An example of such a model would be a Random Forest Classifier consisting of many Decision Trees. In this model, each tree’s predictions are considered when determining the final prediction. This complexity only increases when neural network-based models such as LogoNet are taken into consideration. With an increase in the complexity of such models, it becomes simply impossible for humans to understand the model by just looking at the model weights.
As mentioned earlier, humans need extra methods to comprehend how sophisticated algorithms generate predictions. Researchers make use of different methods to find connections between the input data and model-generated predictions, which can be useful in understanding how the ML model behaves. Such model-agnostic methods (methods that are independent of the kind of model) include partial dependence plots, SHapley Additive exPlanations (SHAP) dependence plots, and surrogate models. Several approaches that emphasize the importance of different features are also employed. These strategies determine how well each attribute may be utilized to predict the target variable. A higher score means that the feature is more crucial to the model and has a significant impact on prediction.
However, the question that still remains is why there is a need to distinguish between the interpretability and explainability of ML models. It is clear from the arguments mentioned above that some models are easier to interpret than others. In simple terms, one model is more interpretable than another if it is easier for a human to grasp how it makes predictions than the other model. It is also the case that, generally, less complicated models are more interpretable and often have lower accuracy than more complex models involving neural networks. Thus, high interpretability typically comes at the cost of lower accuracy. For instance, employing logistic regression to perform image recognition would yield subpar results. On the other hand, model explainability starts to play a bigger role if a company wants to attain high performance but still needs to understand the behavior of the model.
Thus, businesses must consider whether interpretability is required before starting a new ML project. When datasets are large, and the data is in the form of images or text, neural networks can meet the customer’s objective with high performance. In such cases, When complex methods are needed to maximize performance, data scientists put more emphasis on model explainability than interpretability. Because of this, it’s crucial to comprehend the distinctions between model explainability and interpretability and to know when to favor one over the other.
Don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Khushboo Gupta is a consulting intern at MarktechPost. She is currently pursuing her B.Tech from the Indian Institute of Technology(IIT), Goa. She is passionate about the fields of Machine Learning, Natural Language Processing and Web Development. She enjoys learning more about the technical field by participating in several challenges.


Title: https://www.marktechpost.com/2023/03/12/best-ai-tools-for-web-designers-2023/
Link: https://www.marktechpost.com/2023/03/12/best-ai-tools-for-web-designers-2023/
Description: 
Full text:
The emergence of artificial intelligence forces our civilization to undergo a paradigm change. What was previously created by humans might be done by AI in several sectors in the future. Changes and difficulties are ahead for us shortly, which we must overcome in previously unthinkable ways. Artificial intelligence will need us to rethink how we create jobs and our social models to coexist in the same universe.
The realm of web design has been impacted by this transformation, much like many other industries. In reality, several technologies have been developed recently that apply AI to design and web development.
Numerous businesses have begun to use artificial intelligence to produce solutions that aid web project developers and designers. These new technologies make it easier for professionals to do their jobs and speed up some of the procedures involved in producing digital products.
It has already been shown how artificial intelligence and its machine learning models may inspire a new way of working in design and front-end development, even if some of these tools are still experimental and need refinement.
Colormind
Creating harmonizing color palettes is one of the most boring, repetitive, but increasingly important chores in the web design flow, and Colormind is an AI-powered tool that does it.
Colormind has been around for a while, so the community has extensively evaluated and endorsed it. It picks up color preferences from daily posted images, movies, and works of popular art. It now includes a large database of color schemes created by other people.
Khroma
Khroma is an artificial intelligence-based color tool you can train by selecting 50 of your favorite colors. Based on these selections, the computer creates infinite color palette combinations. They may be shown on themes, such as Text, Poster, Gradient, and Picture, that address common use-case situations. Users may also try color schemes on their original photographs.
Vance AI
Web designers and anybody else who needs polished or altered photographs now have it much simpler thanks to AI image processing and automated photo editing technologies. One of the most well-known companies in this sector is Vance AI.
Vance AI employs Deep Convolutional Neural Networks trained on millions of photos and excels at handling genuine details. Use the platform to streamline processes and provide new opportunities for imaginative picture alterations and manipulations that adhere to brand requirements and customer expectations.
Stable Diffusion
Steady Diffusion is a recently released AI-based technology developed by Stability AI, CompVis LMU, and Runway with EleutherAI and LAION. It promises to provide some amazing outcomes.
Now, depending on a text prompt describing the items to be included or excluded from the output, it creates detailed graphics. In addition, the model can redraw old graphics to include newly specified components.
Stable Diffusion may also produce picture-to-image translations or create new images based on the original image, including information from the text prompt. The fact that it’s free is great. Nonetheless, keep in mind copyright and appropriate output use.
Dall-E
We could not resist but mention Dall-E while discussing deep learning models used to produce digital pictures from natural language descriptions. This powerful AI technology has some legal problems since it is still being determined who owns the images, but it has already transformed how we can make certain types of photos.
So what’s the big deal? Dall-E is an AI device that creates graphics in response to commands. With greater resolutions, it may produce realistic images that mimic desired aesthetics, ideas, and specific features and traits.
Dall-E cannot produce violent, hateful, or pornographic pictures, which is wonderful news. Moreover, it employs cutting-edge methods to stop the development of lifelike facial images of actual people. Thus, famous people are secure. Web designers may use it securely to customize vector stock for their site design ideas.
Midjourney
Recently, Midjourney has gained popularity. You have heard about it once or twice if you are a designer. While the platform is still in its testing phase (it is now accessible in open beta and solicits input for enhancements), the first findings are rather encouraging.
A chatbot named Midjourney converses with people in their language. From verbal descriptions and prompts, site designers may give instructions and orders to build graphics, including online interfaces. It works well for creating ideas and unique material perfect for so-called “custom adverts developed for people.”
ChatGPT Prompts – Curated Collection
ChatGPT has completely changed how web design projects are completed. It uses algorithms to evaluate a statement presented in the conversation and produces a result pertinent to the input. Only straightforward instruction will do.
Even while there is still much space for growth, it has already produced some fantastic outcomes. The results from ChatGPT when requested to perform the roles of a web designer, UI/UX designer, copywriter, or project manager are just a few examples. They have compiled a list of authentic ChatGPT prompts for site designers and outputs that address some frequent problems as evidence.
Designs AI
For people without design expertise and web designers who need to build stunning templates quickly, Designs AI is a powerful AI-based all-in-one solution. The brand may be presented in various important ways, including voice, video, design, and logo. A graphic creator, color matcher, and font pairs are also included.
It is a well-rounded system that offers generators and complies with the most recent norms and specifications. Although it can’t create a mockup without your help, it still has several great AI capabilities that significantly speed up the web design process.
Zyro
The era of Lorem Ipsum is over. AI-powered tools that make human-like readable copies might be useful if you require a text excerpt for your website. Zyro is one among them as well. Despite saving millions of site designers from spending time producing fake text, with this amazing internet tool, the niche continues to grow.
Based on your parameters, our AI-powered content generator creates unique and customized text, including blog headlines, in seconds. It adheres to the top SEO and copywriting techniques since it has been trained on millions of resources.
Uizard
Uizard’s tagline says that no prior design knowledge is necessary.
To build websites with stunning looks, appealing functionality, and an incredible user experience, web designers may use this remarkable AI-powered tool, which needs no design knowledge or expertise. Customers may quickly transform their hand-drawn concepts into workable, polished digital versions with Uizard. This is the newest generation of well-known website builders with drag-and-drop play areas.
More exists. You may show the platform many screenshots of user interfaces you like if you need a drawing. The AI helper will do this automatically, giving you a distinctive UX design style and a firm base on which to build.
From simple landing pages to extensive e-commerce endeavors and online apps, Uizard can design user interfaces for mobile applications and websites. For product managers, UX designers, company founders, consultants, and developers, it covers a variety of situations.
Sketch to Code
Microsoft developed this amazing AI-powered tool, and it is quickly gaining acceptance in the online community. It already has a sizable following that uses its promise to streamline workflow procedures and break down barriers.
Sketch to Code allegedly received training on millions of photos. As a consequence, you should anticipate some refined outcomes. This AI technology does a good job of converting hand-drawn site design components like a textbox or button from paper to their digital counterparts placed on digital sheets. You may quickly convert the wireframe for a web page into a digital layout.
Not just that. This digital edition will include an HTML version, enabling web designers to create UI wireframes and transform them into functional prototypes without specialized knowledge or programming expertise.
Sensei by Adobe
Another well-known brand in our list of the top AI tools for web designers focuses on boosting output and turning a workflow into a fun activity. An artificial intelligence product called Adobe Sensei works with Adobe applications. It was made to make it easier for creatives, such as web designers, to quickly and flawlessly offer the ideal customer experience across various communication channels. AI drives the creative tools in the Adobe Suite, making it considerably simpler and quicker for web designers to get the desired results.
Fronty
Fronty is an online application that uses AI to transform a picture into HTML and CSS code. With this tool, you may easily construct a website by uploading a photo of your layout. Fronty will instantly produce the front-end code (HTML / CSS) after your design has loaded.
You can also quickly change the content and layout of your website online using their UI editor. Fronty can produce clean Code that uses strong SCSS and semantically valid HTML.
Playground by TeleportHQ
An open-source platform called TeleportHQ leverages AI to provide “real-time design-to-code” solutions. The video (above) demonstrating the real-time conversion of a wireframe written on a blackboard into front-end code was initially released by TeleportHQ in 2018. Impressive was the outcome!
Computer vision is taught to detect hand-drawn UI components. A machine learning model created using TensorFlow made the procedure feasible (open source platform for machine learning developed by the Google Brain team). At that time, TeleportHQ, a Romanian startup, continued its study in artificial intelligence for web design.
Zecoda
Zecoda is a tool that, with the aid of artificial intelligence, enables you to rapidly and efficiently convert your Sketch files into front-end code. Zecoda will quickly deliver the HTML, CSS, and JavaScript code using the Vue.js framework when you submit your design file. Without writing any code, you can use Zecoda to turn your design into a responsive website ready for publication.
Zecoda is a 90% artificially intelligent, 10% human-touched instrument. This is so that it combines deep learning’s speed with the assurance of quality and security provided by a web developer evaluation. In actuality, a developer will always review the Code that is offered to you.
Artyline
With the help of the augmented reality (AR) smartphone app Artyline, you can quickly create interactive, high-fidelity prototypes from your hand-drawn designs. Consider being able to exhibit your paper-drawn prototype in augmented reality for a briefing. Artyline makes this feasible.
Also, this iOS software allows you to create real-time usability testing with eye-tracking and click-tracking statistics and creative screens with contemporary UI and design choices.
The software is easy to use: make your drawing, scan it with the app, and in a few seconds, you’ll have your digital sketch in augmented reality. Using computer vision to identify sketch components and augmented reality on your smartphone to create a ready-made mockup, Artyline is boosted by artificial intelligence.
Don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more. If you have any question regarding the above article or if we missed anything, feel free to email us at Asif@marktechpost.com
Prathamesh Ingle is a Mechanical Engineer and works as a Data Analyst. He is also an AI practitioner and certified Data Scientist with an interest in applications of AI. He is enthusiastic about exploring new technologies and advancements with their real-life applications


Title: https://www.marktechpost.com/2023/03/12/memoji-on-steroids-this-ai-model-can-reconstruct-3d-avatars-from-videos/
Link: https://www.marktechpost.com/2023/03/12/memoji-on-steroids-this-ai-model-can-reconstruct-3d-avatars-from-videos/
Description: 
Full text:
We see digital avatars everywhere, from our favorite chat applications to virtual marketing assistants on our favorite e-commerce websites. They are becoming increasingly popular and integrating quickly into our daily lives. You go into your avatar editor, select skin color, eye shape, accessories, etc. and have one ready to mimic you in the digital world.
Constructing a digital avatar face manually and using it as a living emoji can be fun, but it only scratches the surface of what is possible. The true potential of digital avatars lies in the ability to become a clone of our entire body. This type of avatar has become an increasingly popular technology in video games and virtual reality (VR) applications.
Generating high-fidelity 3D avatars require expensive and specialized equipment. Therefore, we only see them used in a limited number of applications, like the professional actors we see in video games.
What if we could simplify this process? Imagine you could generate a high-fidelity 3D full-body avatar by just using some videos captured in the wild. No professional equipment, no complicated sensor setup to capture every tiny detail, just a camera and a simple recording with a smartphone. This breakthrough in avatar technology could revolutionize many applications in VR, robotics, video games, movies, sports, etc.
The time has arrived. We have a tool that can generate high-fidelity 3D avatars from videos captured in the wild. Time to meet Vid2Avatar.
Vid2Avatar learns 3D human avatars from in-the-wild videos. It does not need without need ground truth supervision, priors extracted from large datasets, or any external segmentation modules. You just give it a video of someone, and it will generate a robust 3D avatar for you.
Vid2Avatar has some smart tricks up its sleeves to achieve this. The first thing to do is to separate the human from the background in a scene and model it as a neural field. They solve the tasks of scene separation and surface reconstruction directly in 3D. They model two separate neural fields to learn both the human body and background implicitly. This is normally a challenging task because you need to associate the human body with 3D points without relying on 2D segmentation.
The human body is modeled using a single temporally consistent representation of the human shape and texture in canonical space. This representation is learned from deformed observations using an inverse mapping of a parametric body model. Moreover, Vid2Avatar uses an optimization algorithm to adjust multiple parameters related to the background, human subject, and their poses in order to best fit the available data from a sequence of images or video frames.
To further improve the separation, Vid2Avatar uses a special technique for representing the scene in 3D, where the human body is separated from the background in a way that makes it easier to analyze the motion and appearance of each separately. Also, it uses novel objectives, like focusing on having a clear boundary between the human body and the background, guiding the optimization process toward producing more accurate and detailed reconstructions of the scene.
Overall, a global optimization approach for robust and high-fidelity human body reconstruction is proposed. This method uses videos capture in-the-wild without requiring any further information. Carefully designed components achieve robust modeling, and in the end, we get 3D avatars that could be used in many applications.
Check out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Ekrem Çetinkaya received his B.Sc. in 2018 and M.Sc. in 2019 from Ozyegin University, Istanbul, Türkiye. He wrote his M.Sc. thesis about image denoising using deep convolutional networks. He is currently pursuing a Ph.D. degree at the University of Klagenfurt, Austria, and working as a researcher on the ATHENA project. His research interests include deep learning, computer vision, and multimedia networking.


Title: https://www.marktechpost.com/2023/03/12/microsoft-proposes-mathprompter-a-technique-that-improves-large-language-models-llms-performance-on-mathematical-reasoning-problems/
Link: https://www.marktechpost.com/2023/03/12/microsoft-proposes-mathprompter-a-technique-that-improves-large-language-models-llms-performance-on-mathematical-reasoning-problems/
Description: 
Full text:
LLMs stands for Large Language Models. These are advanced machine learning models that are trained to comprehend massive volumes of text data and generate natural language. Examples of LLMs include GPT-3 (Generative Pre-trained Transformer 3) and BERT (Bidirectional Encoder Representations from Transformers). LLMs are trained on massive amounts of data, often billions of words, to develop a broad understanding of language. They can then be fine-tuned on tasks such as text classification, machine translation, or question-answering, making them highly adaptable to various language-based applications.
LLMs struggle with arithmetic reasoning tasks and frequently produce incorrect responses. Unlike natural language understanding, math problems usually have only one correct answer, making it difficult for LLMs to generate precise solutions. As far as it is known, no LLMs currently indicate their confidence level in their responses, resulting in a lack of trust in these models and limiting their acceptance.
To address this issue, scientists proposed ‘MathPrompter,’ which enhances LLM performance on mathematical problems and increases reliance on forecasts. MathPrompter is an AI-powered tool that helps users solve math problems by generating step-by-step solutions. It uses deep learning algorithms and natural language processing techniques to understand and interpret math problems, then generates a solution explaining each process step.
To generate multiple Algebraic expressions or Python functions to answer the same mathematical issue in various ways and increase the confidence level in the output results, MathPrompter uses the Zero-shot chain-of-thought promoting technique. This differs from previous prompt-based CoT approaches, where the intermediate steps’ accuracy needs to be verified.
AI method known as the zero-shot-CoT (Concept over Text) process can resolve problems involving mathematical inference without being trained beforehand. Instead, they focus on the capacity to think critically about the text and general comprehension of arithmetic ideas.
With these techniques, an artificial intelligence model is given a problem statement in natural language text, creating a symbolic representation of the issue. The model manipulates the symbols using algebraic or geometric operations to produce a solution.
Zero-shot-CoT approaches are beneficial for tackling challenging mathematics problems, such as those that appear in contests or standardized tests. Because they rely on a more symbolic representation of the problem rather than on natural language interpretation, they can also aid in addressing the shortcomings of LLMs in arithmetic reasoning problems.
One of the drawbacks of this research is that even while the scientists run the MathPrompter several times in different ways to improve the quality of the results, it may not always ensure the output is accurate. Even if the prompt outputs are identical, algebraic and Pythonic expressions could still result in inaccurate outcomes.
This issue can be resolved by adding more prompts. Scientists are now looking into a more principled approach to solving this problem.
Check out the Paper. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Niharika is a Technical consulting intern at Marktechpost. She is a third year undergraduate, currently pursuing her B.Tech from Indian Institute of Technology(IIT), Kharagpur. She is a highly enthusiastic individual with a keen interest in Machine learning, Data science and AI and an avid reader of the latest developments in these fields.


Title: https://www.marktechpost.com/2023/03/12/roadmap-of-becoming-a-prompt-engineer-2023/
Link: https://www.marktechpost.com/2023/03/12/roadmap-of-becoming-a-prompt-engineer-2023/
Description: 
Full text:
A prompt is a set of input text or instructions used to guide AI models like ChatGPT, DALLE-2, etc., toward generating desired outputs. In other words, a prompt is a specific text that prompts an AI model to create an outcome that aligns with certain criteria or parameters.
Prompt engineering is the process of creating and refining these prompts to generate the desired result. The goal of prompt engineering is to create accurate and effective prompts. Prompt engineers program in prose and send the plain text commands to the AI model, which does the actual work.
A more technical area of prompt engineering is fine-tuning the input data used to train AI models. It involves carefully selecting and structuring the input data to maximize its usefulness for training the model.
Why is Prompt Engineering important?
Prompt engineering can help improve the accuracy and performance of AI models. The flaws of AI models can be better understood through prompt engineering, which allows developers to identify and address any issue that may arise during the training of the model.
Furthermore, prompt engineering can transform simple inputs into unique outputs, enhancing the model’s overall performance. In cases where data availability is limited, such as in medical imaging, prompt engineering can make the most of the available data by selecting and structuring it to maximize its effectiveness in training the model.
Additionally, prompt engineering is critical in ensuring software applications meet user expectations by responding quickly to user input, leading to a positive user experience. Therefore, prompt engineering is a crucial aspect of AI that helps ensure project timelines are met and software applications are developed and deployed on time, leading to project success.
Prompt Engineering as a career
Prompt engineering is a rapidly growing career path that offers exciting opportunities for those with a deep understanding of natural language processing and a creative mind. As AI and NLP technologies become increasingly pervasive in various industries, the demand for skilled prompt engineers who can design effective prompts and refine their outputs is expected to rise.
This field requires engineers to understand the task or application they are working on, possess a deep understanding of the language model they are using, and be able to craft prompts that provide the necessary context to generate accurate and varied outputs.
As companies increasingly use language models to find user-friendly solutions, the need for transparency and responsibility in this area will increase, making experienced prompt engineers even more valuable. With the rise of AI and ML, prompt engineering is set to become one of the top career choices of the future.
We are on the verge of a new era of AI, and state-of-the-art tools like ChatGPT are taking the lead in advancing the field. The possibilities for further development in AI are endless, and the enthusiasm surrounding it is evident. For those who aspire to be part of the forefront of AI innovation, prompt engineering is the key to joining the wave of progress in the world of AI.
Skills required to become a prompt engineer?
The following skills are necessary to pursue prompt engineering as a career option:
- Bachelor’s degree in Computer Science or a related field.
- Good writing and communication skills.
- Knowledge about AI models – their limitations, how they work, etc.
- It would be helpful to have experience with natural language processing.
Job profile
A prompt engineer is a software professional responsible for designing, developing, testing, debugging, maintaining, and updating software applications. They work closely with software developers to ensure the software is efficient and responsive. The critical aspect of their job is identifying and addressing the flaws in AI systems and developing strategies to maximize their potential. They need to have a deep understanding of the technology to achieve unique results.
Prompt engineering is the magic behind top-notch language model responses. The success of a language model depends on well-written prompts that avoid off-topic, inconsistent, or offensive output. Prompt engineers need to push the limits of AI technology and develop complex strategies to transform simple inputs into high-quality results.
Prompt engineers are responsible for developing exceptional language model responses by understanding the limitations and potential of AI systems and developing unique strategies to deliver high-quality results.
Salary range of a prompt engineer in the USA
The field of prompt engineering is a relatively new and rapidly growing area of machine learning. As such, there is little job data on the salary range for prompt engineers in the USA.
However, recent job postings by AI research companies Anthropic and OpenAI provide insight into the earning potential for prompt engineers. Anthropic recently listed a job for a prompt engineer and prompt librarian, with a base salary range of $250,000 to $335,000.
Similarly, OpenAI is hiring hundreds of engineers to improve the reliability and effectiveness of their machine learning platform, ChatGPT, with salaries for research and engineering roles in applied AI at the firm reaching up to $370,000.
While these job listings offer a glimpse into the earning potential for prompt engineers, it’s important to note that the field is relatively new and evolving, so exact salary ranges may vary based on experience, location, and industry. Overall, prompt engineering is proving to be a highly lucrative field with significant growth potential in the coming years.
Conclusion
In conclusion, prompt engineering is a crucial aspect of AI development that helps ensure accuracy, effectiveness, and user satisfaction. With the increasing demand for AI and NLP technologies across various industries, the career prospects for prompt engineers are promising.
A combination of technical skills, knowledge of AI models and natural language processing, and a creative mindset are necessary to excel in this field. The salary potential for prompt engineers is also lucrative, with top companies offering substantial compensation packages.
As we continue to push the limits of AI technology, prompt engineering will undoubtedly play a crucial role in unlocking its potential for innovative solutions and advancements in various industries.
Don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more. If you have any question regarding the above article or if we missed anything, feel free to email us at Asif@marktechpost.com
I am a Civil Engineering Graduate (2022) from Jamia Millia Islamia, New Delhi, and I have a keen interest in Data Science, especially Neural Networks and their application in various areas.


Title: https://www.marktechpost.com/2023/03/12/this-ai-paper-proposes-cafo-a-cascade-of-foundation-models-that-incorporates-diverse-prior-knowledge-of-various-pre-training-paradigms-for-better-few-shot-learning/
Link: https://www.marktechpost.com/2023/03/12/this-ai-paper-proposes-cafo-a-cascade-of-foundation-models-that-incorporates-diverse-prior-knowledge-of-various-pre-training-paradigms-for-better-few-shot-learning/
Description: 
Full text:
Many datasets, convolutional neural networks, and transformers have achieved remarkable success on various vision tasks. Instead, few-shot learning, where the networks are confined to learn from constrained pictures with annotations, also becomes a research hotspot for various data-deficient and resource-finite scenarios. Numerous earlier publications have suggested using meta-learning, metric learning, and data augmentation to improve a model’s generalization capacity. Recent results demonstrate good zero-shot transfer ability for open-vocabulary visual identification using CLIP pre-trained by large-scale language-image pairings.
It is further extended for few-shot classification by the follow-up CoOp, CLIP-Adapter, and Tip-Adapter, which also achieves improved performance on various downstream datasets. This shows that the network has strong representational capabilities even while the few-shot training material is inadequate, which greatly aids the few-shot learning on downstream domains. With the advent of other self-supervision models than CLIP, may they collaborate and adaptively integrate their prior knowledge to become better few-shot learners? Chinese researchers suggest CaFo, a Cascade of Foundation model, to address this problem by combining the information from several pre-training paradigms with a “Prompt, Produce, then Cache” pipeline.
They combine CLIP, DINO, DALL-E, and GPT3 to give CaFo four forms of previous knowledge, as seen in Figure 1. CLIP is pre-trained to provide paired features for each picture and its corresponding description text in the embedding space. With language-contrastive knowledge and texts with various category meanings, CLIP can categorize the photos successfully. DINO uses contrastive self-supervised learning to match the representations between two transformations of the same picture. DINO is an expert at differentiating between various images using vision-contrastive knowledge. DALL-E is pre-trained using picture-text pairings, much like CLIP, except it learns to anticipate the encoded image tokens based on the provided text tokens. Depending on the supplied text, DALLE might use vision-generative knowledge to generate high-quality synthetic pictures in a zero-shot way.
When given a few handwritten templates as input, the large-scale language corpus-trained GPT-3 automatically creates sentences that seem like human speech and are rich in generative language knowledge. The four models, therefore, have different pre-training objectives and might offer to complement information to aid in few-shot visual identification. They cascade them in three phases, specifically:
1) Quick: Based on a few handwritten templates, they use GPT-3 to generate textual prompts for CLIP. The textual encoder in CLIP receives these instructions with a more sophisticated language understanding.
2) Produce: They use DALL-E, which expands the few-shot training data while requiring no more labor for collection and annotation, to produce additional training pictures for various categories based on the domain-specific texts.
3) Cache: To adaptively incorporate the predictions from CLIP and DINO, they use a caching model. They construct the cache model with two types of keys by the two pre-trained models using Tip-Adapter. They adaptively ensemble the predictions of two cached keys as the output, using zero-shot CLIP as the distribution baseline. CaFo can improve few-shot visual recognition by learning to combine previous knowledge and use their complementing properties by fine-tuning the lightweight cache model via increased training data.
The following summarizes their key contributions:
• For improved few-shot learning, they suggest using CaFo to incorporate past information from diverse pre-training paradigms.
• They conduct thorough experiments on 11 datasets for few-shot classification, where CaFo achieves state-of-the-art without using additional annotated data.
• They collaborate with CLIP, DINO, GPT-3, and DALL-E to use more semantic prompts, enrich the limited few-shot training data, and adaptively ensemble diverse predictions via the cache model.
Check out the Paper and Code. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Aneesh Tickoo is a consulting intern at MarktechPost. He is currently pursuing his undergraduate degree in Data Science and Artificial Intelligence from the Indian Institute of Technology(IIT), Bhilai. He spends most of his time working on projects aimed at harnessing the power of machine learning. His research interest is image processing and is passionate about building solutions around it. He loves to connect with people and collaborate on interesting projects.


Title: https://www.marktechpost.com/2023/03/12/together-releases-the-first-open-source-chatgpt-alternative-called-openchatkit/
Link: https://www.marktechpost.com/2023/03/12/together-releases-the-first-open-source-chatgpt-alternative-called-openchatkit/
Description: 
Full text:
Together is creating the first distributed cloud designed specifically for handling huge foundation models. The company offers an intuitive platform combining data, models, and computing to help AI researchers, developers, and businesses better harness and advance AI.
Together team believes that open-source models for philanthropies have the potential to be more democratic, open, strong, and adaptive. They recently released OpenChatKit 0.15 under the Apache-2.0 license, making the code, model weights, and training datasets freely available to the public. The robust, open-source foundation offered by OpenChatKit enables the development of domain-specific and general-purpose chatbots. Users can submit feedback, and community members can add new datasets using the OpenChatKit tools, all of which add to the increasing corpus of open training data, eventually leading to better LLMs.
The Together team collaborated with LAION and Ontocord to build the dataset used for training. Reasoning, multi-turn discussion, knowledge, and generating answers are all supported by OpenChatKit’s chat model, which has 20 billion parameters and was trained on 43 million instructions.
A useful chatbot must be able to regulate responses, obey directions given in normal language, and keep the conversation in context. The OpenChatKit framework includes a generic chatbot and the components necessary to create specialized bots.
There are four main parts to the set:
- From EleutherAI’s GPT-NeoX-20B, a large language model tuned for a chat with over 43 million instructions on 100% carbon negative compute
- A set of customization recipes to fine-tune the model to achieve high accuracy on user’s tasks is documented and available open-source under the Apache-2.0 license on GitHub.
- A retrieval system that can be expanded so that information from a document repository, API, or another live-updating information source can be added to a bot’s responses at inference time; includes publicly available examples for using Wikipedia and web search APIs.
- A GPT-JT-6B-derived moderation model is accessible on HuggingFace under the Apache-2.0 license; it selects which queries the bot answers.
Potential fields of study and related assignments include:
- The protected rollout of models that can produce bad data without risking user privacy.
- Exploring and comprehending the flaws and biases of models of conversation and language.
- Create works of art and apply them to design and other creative tasks.
- Tools for learning.
- Study of models of conversation or language.
Just like any other language model-based chatbot, GPT-NeoXT-Chat-Base-20B has some restrictions. For instance, the model might not return an accurate or relevant answer when asked something novel, unclear, or outside of its training data. The team invites participation from many groups and individuals to build a more robust and inclusive chatbot.
Check out the Demo, Model and Reference Article. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Tanushree Shenwai is a consulting intern at MarktechPost. She is currently pursuing her B.Tech from the Indian Institute of Technology(IIT), Bhubaneswar. She is a Data Science enthusiast and has a keen interest in the scope of application of artificial intelligence in various fields. She is passionate about exploring the new advancements in technologies and their real-life application.


Title: https://www.marktechpost.com/2023/03/13/data-labeling-and-ai-revolution-2023/
Link: https://www.marktechpost.com/2023/03/13/data-labeling-and-ai-revolution-2023/
Description: 
Full text:
What is Data labeling?
Data labeling is employed for machine learning algorithms to identify and comprehend objects properly. Face recognition, autonomous driving, aerial drones, robotics, etc., are all areas where ML has proven essential. Visual (photographic and cinematic), aural, and text data are now the primary categories used in data gathering and labeling. Two primary factors determine an AI system’s effectiveness:
- First, the standard of the underlying model used in the procedure.
- Two: The Amount and High-Quality of Available Training Data
Data labeling, in its simplest form, teaches the system to recognize vehicles by providing examples of various automobiles so that it may learn the shared characteristics of each and properly identify cars in unlabelled photos.
How does data labeling work?
Machine learning (ML) and deep learning typically require massive volumes of data to provide the groundwork for reliable learning patterns. The data they collect for their training systems must be labeled to get the intended outcome.
Labels used for feature recognition should be descriptive, discriminating, and unique if the resulting algorithm is to be reliable. A well-labeled dataset offers verifiability that the ML model may utilize to check the precision of its predictions and refine its method.
Accuracy and precision are the hallmarks of a top-notch algorithm. An accurate dataset is one in which specific labels may be retrieved directly from the original data. In data science, quality is defined as the degree to which a dataset is true overall.
Key to win
Systems or machinery that can recognize patterns or function autonomously require extensive training in the form of high-quality, copious data. The CDAO, where Martell works, was founded in December 2021 to speed up and broaden the Defense Department’s use of AI and data analytics. After months of consolidating the Joint AI Center, the Defense Digital Service, Advana, and the chief data officer’s position, the office finally began operating at full capacity in June.
For a long time, the Military has been interested in artificial intelligence to make better judgments more rapidly and open up previously inaccessible areas to an investigation that no soldier, sailor, or human would dare to explore.
As of early 2021, the Defense Department was working on more than 685 AI projects, according to a study by the Government Accountability Office. Some of these programs involved important military systems. Last month, the Air Force selected Howard University to lead research on tactical autonomy, including manned-unmanned teaming, as part of a five-year, $90 million contract.
The data-centric method has its drawbacks. In particular, the model-centric strategy is the only choice if the team is strapped for cash and one is trying to avoid human-handled labeling entirely using a pre-existing dataset. Meanwhile, there are two labeling options: doing it in-house, which may be very expensive and time-consuming, or outsourcing it, which can sometimes be a gamble and typically costs a lot. Synthetic labeling is another approach that involves producing fake data for ML, but it is resource-intensive and hence out of reach for many smaller businesses. Therefore, many groups conclude that the data-centric strategy isn’t worth the effort required, whereas, in reality, they need to be more informed.
The data-centric strategy is effective, but only if one is putting in the effort to work with the data. The good news is that data labeling doesn’t have to be expensive or take months, thanks to crowdsourcing techniques. The problem, however, is that more people need to be made aware of such procedures, let alone that they have evolved to become successful. Notwithstanding the drawbacks, over 80% of ML practitioners choose the in-house route, according to the research. And a recent poll shows that these doctors don’t utilize this technique because they prefer it over others; they use it because they don’t know any better.
To sum it up
Access to large volumes of high-quality labeled data is still a major roadblock in advancing artificial intelligence. An increase in the need for properly tagged data is virtually inevitable as the movement with Ng as its leader gathers traction. So, progressive AI professionals are rethinking how they classify their data. Due to the high cost and limited scalability of in-house labeling, they may soon outgrow it and be priced out of using external sources like pre-packaged data, data scraping, or establishing links with data-rich entities. The bottom conclusion is that high-quality input is essential for the real-world success of AI initiatives. And accuracy, that is, correct labeling, is required to improve the data quality and, by extension, the models it powers.
Dhanshree Shenwai is a Computer Science Engineer and has a good experience in FinTech companies covering Financial, Cards & Payments and Banking domain with keen interest in applications of AI. She is enthusiastic about exploring new technologies and advancements in today’s evolving world making everyone's life easy.


Title: https://www.marktechpost.com/2023/03/13/meet-gigagan-a-large-scale-modified-gan-architecture-for-text-to-image-synthesis/
Link: https://www.marktechpost.com/2023/03/13/meet-gigagan-a-large-scale-modified-gan-architecture-for-text-to-image-synthesis/
Description: 
Full text:
The introduction of popular language models like ChatGPT and DALL-E has been a massive topic of interest for the past few months, especially in the Artificial Intelligence community. These models can perform tasks ranging from answering questions and generating content to producing good-quality images. They do so by using some advanced deep-learning methodologies. For the unaware, DALL-E, developed by OpenAI, is a text-to-image generation model that creates high-quality images with the help of the fed textual description as input. Trained on massive datasets of texts and images, DALL-E and other text-to-image generation models develop a visual representation of the given text or the prompt. Apart from this, Stable diffusion even allows the generation of a new image from an existing image.
These LLMs completely rely on an iterative interface, making them useful for stable training with basic objectives but computationally expensive and less efficient. Compared to these models, Generative Adversarial Networks (GANs) are more efficient as generating images in GANs takes place only through a single pass. GANs are basically deep learning architectures consisting of a generator network to create samples and discriminator data to evaluate the samples if they are real or fake. The goal of GANs is to simply produce new data that imitates some known data distribution. But scaling GANs has been established with certain instabilities in the training procedure. A recent paper has explored whether and how GANs can be scaled up with stable training.
A team of researchers has developed GigaGAN, which is a new GAN architecture that can far exceed the limitations of the previously existing StyleGAN architecture. GigaGAN is a one billion parameter GAN and showed stable and scalable training on large-scale datasets such as LAION2B-en. GigaGAN is extremely fast and can produce a 512px image in just 0.13 seconds and 4096px at 3.7s. It can also produce high-resolution images, such as 16-megapixel images, in just 3.66 seconds. The two main components of GigaGAN’s architecture does the following –
- GigaGAN generator – It includes a text encoding branch, style mapping network, and a multi-scale synthesis network which is augmented by stable attention and adaptive kernel selection.
- GigaGAN discriminator – It includes two branches for processing the image as well as the text conditioning. The text branch processes the text like the generator, and the image branch receives an image pyramid making independent predictions for each image scale.
GigaGAN even supports a number of latent space editings applications, such as latent interpolation, style mixing, and vector arithmetic operations. Compared to Stable Diffusion v1.5, DALL·E 2, and Parti-750M, GigaGAN has a lower Fréchet inception distance (FID), a metric used to evaluate the quality of images created by a generative model by calculating the distance between feature vectors. Lower scores show that the two groups of images are more similar.
With a disentangled, continuous, and controllable latent space, GigaGAN is a viable option for text-to-image synthesis and offers significant advantages over other generative models.
Check out the Paper and Github. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Tanya Malhotra is a final year undergrad from the University of Petroleum & Energy Studies, Dehradun, pursuing BTech in Computer Science Engineering with a specialization in Artificial Intelligence and Machine Learning.
She is a Data Science enthusiast with good analytical and critical thinking, along with an ardent interest in acquiring new skills, leading groups, and managing work in an organized manner.


Title: https://www.marktechpost.com/2023/03/13/meet-magnushammer-a-transformer-based-approach-to-premise-selection/
Link: https://www.marktechpost.com/2023/03/13/meet-magnushammer-a-transformer-based-approach-to-premise-selection/
Description: 
Full text:
Artificial intelligence’s main focus has been on automating mathematical reasoning. More recently, machine learning has greatly benefited both informal and formal theorem proving. The latter method, which they use in this research, enables proof assistants to interact with machine learning models to verify proofs produced by such models automatically. Mathematics is hierarchical because it builds upon and bootstraps from an existing body of knowledge. As a result, proving a mathematical assertion is seen as a creative process requiring, among other things, intuition, insights, and a wise choice of tactics.
These skills can assist in selecting pertinent facts that, when applied at a certain stage, develop the case and finally point to the desired result. Premise selection is the term used to describe this procedure in automated reasoning systems. Premise selection has been addressed by several tools, including a family of devices known as “hammers” that include Automatic Theorem Provers into interactive proof helpers. One such tool, Sledgehammer, rose to popularity with Isabelle, where it was used to produce a sizable chunk of the Archive of Formal Proofs, Isabelle’s proof corpus.
Although hammers have been implemented into other proof assistants, not all proof assistants now support them. This is because hammers implementation is difficult owing to the variety of evidence object structures and the intricate translation procedures needed across various logics. So, there is a critical need for an efficient premise selection tool that can operate across all proof helpers with little need for customization. In this work, researchers from GoogleAI present Magnushammer, a general-purpose, data-driven transformer-based premise selection tool. They show that it can conduct premise selection efficiently and with little domain-specific expertise.
Magnushammer has two retrieval phases, each trained via contrastive learning. At the SELECT stage, given a proof state, they select the 1024 premises from the theorem that are most pertinent to the proof (as determined by the cosine similarity of their embeddings) (database up to 433K). In the second step, RERANK, they re-rank the retrieved premises using more precise but costly processing. Using a transformer architecture, they allowed the proof state tokens to directly attend to the retrieved premise tokens, producing a relevance score. Magnushammer surpasses Sledgehammer’s 38.3% proof rate by a wide margin, scoring a 59.5% on the PISA benchmark.
They show that given any compute budget, the proof rate of Magnushammer significantly outperforms that of Sledgehammer, as illustrated in Figure 1. A neural-symbolic model called Thor has a Sledgehammer component that they replace with a Magnushammer component, increasing the state-of-the-art proof rate from 57.0% to 71.0%. The Isabelle theorem prover and its human-proof libraries were mined for a dataset of premise selection to get these findings. The collection includes 433K distinct premises among 4.4M examples of premise selection instances. This is the largest premise selection dataset of its sort that they are aware of.
Their contributions can be sumarised as follows:
• As a general, data-driven strategy for premise selection, they suggest using transformers trained contrastively. Magnushammer, the approach they developed, greatly outperforms Sledgehammer, the most widely used symbolic premise selection tool, with a 59.5% proof rate on the PISA benchmark.
• To their knowledge, they extracted and made the largest premise selection dataset available. It has 433K distinct premises and 4.4M premise selection instances. They anticipate that this dataset will be useful for advancing present and future research in the field.
• They examine the scalability of Magnushammer concerning model size, dataset size, and computing budget for inference time. Their analysis suggests that adding more computer power might lead to even greater gains.
Check out the Paper. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Aneesh Tickoo is a consulting intern at MarktechPost. He is currently pursuing his undergraduate degree in Data Science and Artificial Intelligence from the Indian Institute of Technology(IIT), Bhilai. He spends most of his time working on projects aimed at harnessing the power of machine learning. His research interest is image processing and is passionate about building solutions around it. He loves to connect with people and collaborate on interesting projects.


Title: https://www.marktechpost.com/2023/03/13/researchers-from-tsinghua-university-introduce-a-novel-machine-learning-algorithm-under-the-meta-learning-paradigm/
Link: https://www.marktechpost.com/2023/03/13/researchers-from-tsinghua-university-introduce-a-novel-machine-learning-algorithm-under-the-meta-learning-paradigm/
Description: 
Full text:
Recent achievements in supervised tasks of deep learning can be attributed to the availability of large amounts of labeled training data. Yet it takes a lot of effort and money to collect accurate labels. In many practical contexts, only a small fraction of the training data have labels attached. Semi-supervised learning (SSL) aims to boost model performance using labeled and unlabeled input. Many effective SSL approaches, when applied to deep learning, undertake unsupervised consistency regularisation to use unlabeled data.
State-of-the-art consistency-based algorithms typically introduce several configurable hyper-parameters, even though they attain excellent performance. For optimal algorithm performance, it is common practice to tune these hyper-parameters to optimal values. Unfortunately, hyper-parameter searching is often unreliable in many real-world SSL scenarios, such as medical image processing, hyper-spectral image classification, network traffic recognition, and document recognition. This is because the annotated data are scarce, leading to high variance when cross-validation is adopted. Having algorithm performance sensitive to hyper-parameter values makes this issue even more pressing. Moreover, the computational cost may become unmanageable for cutting-edge deep learning algorithms as the search space grows exponentially concerning the number of hyper-parameters.
Researchers from Tsinghua University introduced a meta-learning-based SSL algorithm called Meta-Semi to leverage the labeled data more. Meta-Semi achieves outstanding performance in many scenarios by adjusting just one more hyper-parameter.
The team was inspired by the realization that the network may be trained successfully using the appropriately “pseudo-labeled” unannotated examples. Specifically, during the online training phase, they produce pseudo-soft labels for the unlabeled data based on the network predictions. Next, they remove the samples with unreliable or incorrect pseudo labels and use the remaining data to train the model. This work shows that the distribution of correctly “pseudo-labeled” data should be comparable to that of the labeled data. If the network is trained with the former, the final loss on the latter should also be minimized.
They defined the meta-reweighting objective to minimize the final loss on the labeled data by selecting the most appropriate weights (weights throughout the paper always refer to the coefficients used to reweight each unlabeled sample rather than referring to the parameters of neural networks). The researchers encountered computing difficulties when tackling this problem using optimization algorithms.
For this reason, they suggest an approximation formulation from which a closed-form solution can be derived. Theoretically, they demonstrate that each training iteration only needs a single meta gradient step to achieve the approximate solutions.
In conclusion, they suggest a dynamic weighting approach to reweight previously pseudo-labeled samples with 0-1 weights. The results show that this approach eventually reaches the stationary point of the supervised loss function. In popular image classification benchmarks (CIFAR-10, CIFAR-100, SVHN, and STL-10), the proposed technique has been shown to perform better than state-of-the-art deep networks. For the difficult CIFAR-100 and STL-10 SSL tasks, Meta-Semi gets much higher performance than state-of-the-art SSL algorithms like ICT and MixMatch and obtains somewhat better performance than them on CIFAR-10. Moreover, Meta-Semi is a useful addition to consistency-based approaches; incorporating consistency regularisation into the algorithm further boosts performance.
According to the researchers, Meta-Semi requires a little more time to train is a drawback. They plan to look into this issue in the future.
Check out the Paper and Reference Article. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Tanushree Shenwai is a consulting intern at MarktechPost. She is currently pursuing her B.Tech from the Indian Institute of Technology(IIT), Bhubaneswar. She is a Data Science enthusiast and has a keen interest in the scope of application of artificial intelligence in various fields. She is passionate about exploring the new advancements in technologies and their real-life application.


Title: https://www.marktechpost.com/2023/03/13/this-ai-paper-expounds-on-the-nature-of-human-creativity-involved-in-text-to-image-art-with-a-specific-focus-on-the-practice-of-prompt-engineering/
Link: https://www.marktechpost.com/2023/03/13/this-ai-paper-expounds-on-the-nature-of-human-creativity-involved-in-text-to-image-art-with-a-specific-focus-on-the-practice-of-prompt-engineering/
Description: 
Full text:
Text-to-image generation systems have become increasingly popular for creating digital art. These systems allow anyone to create high-quality digital images by simply inputting natural language prompts. However, the question arises as to whether this process is truly creative. The traditional definition of creativity as a product-centered view may not fully capture the human creativity involved in text-to-image generation. A more comprehensive view, such as Rhodes’ conceptual model of the “4 P” of creativity, is needed to evaluate the full extent of human creativity in this context. Online communities also play an important role in the creative ecosystem around text-to-image generation.
Researchers have examined text-to-image art’s creativity and human creativity’s role through empirical studies, theoretical analyses, and critical reviews. Studies have shown that text-to-image art may be perceived as less creative than human-generated art but still shows signs of creativity. Theoretical analyses have explored whether creativity can be automated, while critical reviews have looked at the broader social and cultural implications of text-to-image art. Whether text-to-image art is truly creative and the role of human creativity in the process remains an active area of research and debate.
Recently a researcher from the University of JyväskyläIs in Finland published an article to try to answer the question: Is text-to-image art truly creative, and what role does human creativity play in the process?
The main contribution of this paper is to explore and expound on the nature of human creativity involved in the text-to-image generation, specifically in the sub-culture of text-to-image art. The paper argues that human creativity in text-to-image synthesis lies not in the end product (i.e., the digital image), but arises from the interaction of humans with the AI and the resulting practices that evolve from this interaction (e.g., “prompt engineering” and curation).
To achieve this contribution, the paper uses Rhodes’ four P framework to explain the nature of human creativity involved in text-to-image generation, with a special focus on the iterative and interactive practice of prompt engineering and the online community of practitioners of this novel creative practice. The paper also highlights image-level and portfolio-level curation as two important creative practices involved in the creative process of text-to-image generation.
Furthermore, the paper emphasizes the growing importance of communities in the emerging ecosystem of text-to-image generation as a catalyst for creativity and learning and outlines five different roles taken by members of the AI art community. The paper also discusses the practical challenges of evaluating the creativity of images synthesized with text-to-image generation systems and provides opportunities for future research in the field of Human-Computer Interaction (HCI) and the broader implications of text-based co-creation with AI-based systems.
In conclusion, the article confirms that text-to-image art contributes to the digital creative economy by selling NFTs, but raises questions about the level of human creativity involved. The standard product-based definition of creativity may not fully capture the unique factors contributing to creative expression. Challenges also arise when assessing the creativity of text-to-image art.
Check out the Paper. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Mahmoud is a PhD researcher in machine learning. He also holds a
bachelor's degree in physical science and a master's degree in
telecommunications and networking systems. His current areas of
research concern computer vision, stock market prediction and deep
learning. He produced several scientific articles about person re-
identification and the study of the robustness and stability of deep
networks.


Title: https://www.marktechpost.com/2023/03/13/top-ai-based-startups-in-estonia-2023/
Link: https://www.marktechpost.com/2023/03/13/top-ai-based-startups-in-estonia-2023/
Description: 
Full text:
AI is the hot new trend in the startup world. Every new startup is attempting to develop products and find solutions utilizing AI. Estonia, a small country in Northern Europe, is quickly becoming a hotspot for tech startups. The following list includes some of the best AI-based Estonian startups:
Veriff offers an AI-powered identity verification technology that assists companies in real-time customer identity verification to help decrease fraud and satisfy KYC regulations. To verify that the individual is who they say they are, their software uses machine learning algorithms to examine ID documents and facial biometrics. Companies, including Grello, Holo, TF Bank, and Go Urban, employ Veriff’s technology.
Lingvist is a language learning platform that utilizes machine learning and natural language processing (NLP) to tailor the learning process for each user. The platform offers courses in several languages, including French, Russian, German, and Spanish, and it adapts to the learner’s progress by giving them feedback and activities specifically designed for them. These exercises include flashcards and reading, listening, grammar, and speaking tasks.
Excalitrade is a financial markets broker that helps clients to trade without stress by providing automated trading solutions and trading signals. Users can access various markets, including equities, cryptocurrencies, indices, and CFDs, which they may trade using signals driven by AI to make daily profits. Excalitrade provides its users with a number of benefits, such as affordable rates, excellent security, round-the-clock customer service, and an intuitive user interface.
Ready Player me offers AI-powered 3D avatar modeling tools for the metaverse. It creates a cross-platform tool for making avatars for video games and virtual reality programs. Users can create their own avatars and use a single, consistent identity to explore virtual worlds. It serves as the metaverse’s passport. It allows for the integration of applications and websites from third parties.
MeetFrank is a web-based tool for recruiting. Job searchers can use the platform to build a profile that promotes their qualifications, experience, and interests. The platform’s employers publish relevant job openings, and the AI system matches job searchers with those possibilities. Applicants who sign up anonymously using the app will receive suitable job ads based on their qualifications and career goals.
Yanu offers an AI-powered bartending robot that engages in conversations with customers. It is a tiny, mobile-enabled bar where customers may place orders via an interactive screen or mobile app, pay with a card or a phone, and get their drinks from an integrated robotic arm. It can serve 100–150 drinks each hour. Yanu’s creators aim to change the hospitality sector by offering a quick, affordable, and contactless way to serve drinks.
Understanding what consumers think of them may be very challenging for most businesses. Feelingstream is a customer chat platform that assists in gathering consumer feedback, analyzing customer trends, and providing insights that can benefit a number of sectors, including insurance, finance, logistics, telecoms, and utilities. Its conversation analytics tool can automate manual activities, convert speech to text in many languages, and provide real-time customer insights to boost productivity, sales, and customer service standards.
Employees operate in teams, but assessments and recruiting typically concentrate on individuals. Wisnio is a hiring platform that uses AI to comprehend team needs and identify the most suitable individuals for teams. It evaluates current teams by examining the members’ strengths, skills, and motivators. Then, it maps the team’s core values, finds gaps in the members’ important competencies, finds the appropriate candidates for the team, and selects them for the interview.
Cleveron creates cutting-edge robots and parcel lockers to automate omnichannel initiatives and provide a smooth consumer experience. It has developed the necessary hardware and software to offer assistance and maintenance to customers worldwide. Cleveron’s simple solutions make parcel handover swift and convenient by delivering packages in a matter of seconds. Leading merchants and logistics firms worldwide employ these parcel robots and APMs.
Cookie3 performs the function of Google Analytics for Web3. All NFTs, smart contracts and tokens now in use across a number of chains are collected, processed, and interpreted by Cookie3 with the goal of better understanding individual behavior. It focuses on analyzing consumer profiles using the financial history of the users. It intends to rule this market and is a pioneer in the area of behavioral analytics on the blockchain.
Restaurants may build and maintain any food safety protocol with FoodDocs’ AI-based food safety software. The platform develops, tracks, and automatically produces real-time dashboards and monitoring sheets for food safety. It also monitors crucial control components to obtain automated temperature sensor and barcode reader data. The company provides a subscription-based service.
Raison is a cutting-edge platform for handling investments and personal finances online. The platform invests in blockchain-related items so that customers can examine their financial situation and make purchases using virtual cards. By doing so, the business streamlines the complex and time-consuming financial administration process. Anyone may begin their road toward being a late venture investor and create a highly diversified portfolio of the greatest tech firms with the help of the Raison app marketplace.
Hala enables enterprise software to be used with natural language to complete tasks. The system uses a chatbot, which can retrieve and visualize data, interpret and carry out commands in plain language, and configure and modify various aspects of the corporate software. With several Digital Skills and a strong API, the Hala AI platform team collaborates with partners and developers internationally to create Digital Skills and integrations that streamline any job, automate tedious processes, and free up more time for work that adds value.
Eurora Solutions offer AI-based cross-border compliance services. It manages cross-border taxes, descriptions of commodities, and electronic declarations using cutting-edge machine-learning technology. It provides solutions for recordkeeping, VAT payments, and more. It also offers cutting-edge customs clearance and fiscal representation services, which together provide safe and quick access to the EU market.
Alvin is a SaaS data observability platform providing data teams with a unified view of their complete data infrastructure. Their product offers a comprehensive perspective of how data moves and how it is used throughout a company’s data environment by automatically mapping all transformation processes from source via databases and tables to where the data is consumed (e.g., dashboards, ML models, notebooks).
Don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more. If we missed anything, please feel free to contact us at Asif@marktechpost.com
Consultant Intern: Currently in her third year of B.Tech from Indian Institute of Technology(IIT), Goa. She is an ML enthusiast and has a keen interest in Data Science. She is a very good learner and tries to be well versed with the latest developments in Artificial Intelligence.


Title: https://www.marktechpost.com/2023/03/13/ut-austin-researchers-propose-wice-a-new-dataset-for-fact-verification-built-on-real-claims-in-wikipedia-with-fine-grained-annotations/
Link: https://www.marktechpost.com/2023/03/13/ut-austin-researchers-propose-wice-a-new-dataset-for-fact-verification-built-on-real-claims-in-wikipedia-with-fine-grained-annotations/
Description: 
Full text:
Natural language inference and textual entailment are enduring issues in NLP that can take many shapes. There are some significant gaps when current entailment systems are applied to this job. Using NLI “as a tool for the evaluation of domain-general methods to semantic representation” is the declared goal of the SNLI dataset. This, however, is different from how NLI is currently applied. NLI has been used to comprehend knowledge-grounded discourse, validate replies from QA systems, and assess the accuracy of produced summaries. These applications are more closely related to factual consistency or attribution: is it the case that a hypothesis is accurate given the details in a document’s premise?
First, many NLI datasets, including VitaminC and WANLI, which both concentrate on single-sentence evidence, target temporary premises. Existing frameworks for document-level entailment are based on local entailment scores, either through combining these scores or retrieval-based methods. There are a few outliers, like DocNLI, but it contains many artificially generated bad data. This draws attention to the second flaw: the scarcity of negative instances that are environmentally sound. Contradictory situations are created in a way that produces misleading correlations, such as single-word correlations, syntactic heuristics, or a disregard for the input.
Finally, in addition to the conventional three-class “entailed,” “neutral,” and “contradicted” set, fine-grained labeling of whether elements of a claim are supported or not would be more relevant. Existing datasets do not have these fine-grained annotations. As demonstrated in Figure 1, stated in Wikipedia and the associated articles it refers to, they identify entailment, a list of supporting sentences in the referenced article, and tokens in the claim that are not supported by evidence. They demonstrate that these statements require difficult retrieval and verification issues, such as multi-sentence reasoning. Researchers from UT Austin have compiled WICE (Wikipedia Citation Entailment), a dataset for validating actual claims in Wikipedia solving the above discussed problems.
Figure 1 illustrates how the real-world Wikipedia assertions in WICE are frequently more complicated than the theories employed in most prior NLI datasets. They offer CLAIM-SPLIT, a method of dissecting hypotheses utilizing few-shot prompting with GPT-3, to assist in creating their dataset and give fine-grained annotation, as seen in Figure 2. This decomposition is similar to other frameworks built from OpenIE or Pyramid. However, it does it without the need for annotated data and with more flexibility, thanks to GPT-3. They streamline their annotation procedure and the ultimate entailment prediction work for automated models by acting at the sub-claim level.
On their dataset, they test various systems, such as short-paragraph entailment models that have already been “stretched” to create document-level entailment judgments from short-paragraph judgments. They discover that these models perform poorly at the claim level but better at the level of their sub-claims when used with their dataset, indicating that proposition-level splitting might be a helpful step in the pipeline for an attribution system. Although existing systems perform below the human level on this dataset and only sometimes return good evidence, they demonstrate that chunk-level input processing to assess these sub-claims is a solid starting point for future systems.
They provide WICE, a novel dataset for fact-checking based on actual claims in Wikipedia with fine-grained annotations, as one of their main contributions. These demonstrate how difficult it is to decide entailment with the proper facts still. They suggest CLAIM-SPLIT, a strategy for breaking down big claims into smaller, independent sub-claims at both the data collection and inference time. The dataset and code can be found on GitHub.
Check out the Paper and Github. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 15k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more.
Aneesh Tickoo is a consulting intern at MarktechPost. He is currently pursuing his undergraduate degree in Data Science and Artificial Intelligence from the Indian Institute of Technology(IIT), Bhilai. He spends most of his time working on projects aimed at harnessing the power of machine learning. His research interest is image processing and is passionate about building solutions around it. He loves to connect with people and collaborate on interesting projects.

